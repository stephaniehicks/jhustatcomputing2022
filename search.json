[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Location: In person and Online for Fall 2022\nCourse time: Tuesdays and Thursdays from 1:30-2:50pm (Eastern Daylight Time zone)\nCourse location:\n140.776.01 is in person in W2008\n140.776.41 will have recorded lectures posted online after class.\nAssignments: Three projects\n\n\n\n\nTo add the course to your 1st term registration: You can sign up for either the in person (140.776.01) or online only (140.776.41) course.\nAll lectures will be recorded and posted on CoursePlus. You can watch all lecture material and complete the assignments asynchronously.\nPlease course instructor if interested in auditing.\n\n\n\n\n\nStephanie C. Hicks (https://www.stephaniehicks.com)\n\nOffice Location: E3545, Wolfe Street building\nEmail: shicks19@jhu.edu\n\n\nInstructor office hours are announced on CoursePlus. If there are conflicts and/or need to cancel office hours, announcements will be made on CoursePlus.\n\n\n\n\nPhyllis Wei (ywei43@jhu.edu)\nJoe Sartini (jsartin1@jhu.edu)\n\nTA office hours are announced on CoursePlus.\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nWe strongly encourage you to use CoursePlus to ask questions first, before joining office hours. The reason for this is so that other students in the class (who likely have similar questions) can also benefit from the questions and answers asked by your colleagues.\nYou are welcome to join office hours to get more group interactive feedback.\nIf you are not able to make the office hours, appointments can be made by email with either the instructor or the TAs."
  },
  {
    "objectID": "syllabus.html#important-links",
    "href": "syllabus.html#important-links",
    "title": "Syllabus",
    "section": "Important Links",
    "text": "Important Links\n\nCourse website: https://stephaniehicks.com/jhustatcomputing2022\nGitHub repository with all course material: https://github.com/stephaniehicks/jhustatcomputing2022"
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nUpon successfully completing this course, students will be able to:\n\nInstall and configure software necessary for a statistical programming environment\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite and debug code in base R and the tidyverse (and integrate code from Python modules)\nBuild basic data visualizations using R and the tidyverse\nDiscuss best practices for coding and reproducible research, basics of data ethics, basics of working with special data types, and basics of storing data"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nIn Fall 2022, we will have in person lectures that will be recorded enabling an entirely online section for the course too. Mandatory attendance is not required."
  },
  {
    "objectID": "syllabus.html#textbook-and-other-course-material",
    "href": "syllabus.html#textbook-and-other-course-material",
    "title": "Syllabus",
    "section": "Textbook and Other Course Material",
    "text": "Textbook and Other Course Material\nThere is no required textbook. We will make use of several freely available textbooks and other materials. All course materials will be provided. We will use the R software for data analysis, which is freely available for download."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus",
    "section": "Software",
    "text": "Software\nWe will make heavy use of R in this course, so you should have R installed. You can obtain R from the Comprehensive R Archive Network. There are versions available for Mac, Windows, and Unix/Linux. This software is required for this course.\nIt is important that you have the latest version of R installed. For this course we will be using R version 4.2.1. You can determine what version of R you have by starting up R and typing into the console R.version.string and hitting the return/enter key. If you do not have the proper version of R installed, go to CRAN and download and install the latest version.\nWe will also make use of the RStudio interactive development environment (IDE). RStudio requires that R be installed, and so is an “add-on” to R. You can obtain the RStudio Desktop for free from the RStudio web site. In particular, we will make heavy use of it when developing R packages. It is also essential that you have the latest release of RStudio. You can determine the version of RStudio by looking at menu item Help > About RStudio. You should be using RStudio version 1.4.1106 or higher."
  },
  {
    "objectID": "syllabus.html#projects",
    "href": "syllabus.html#projects",
    "title": "Syllabus",
    "section": "Projects",
    "text": "Projects\nThere will be 4 assignments, due every 2–3 weeks. Projects will be submitted electronically via the Drop Box on the CoursePlus web site (unless otherwise specified).\nThe project assignments will be due on\n\nProject 0: September 8, 1:29pm (entirely optional and not graded but hopefully useful and fun)\nProject 1: September 16, 11:59pm\nProject 2: September 30, 11:59pm\nProject 3: October 21, 11:59pm\n\n\nProject collaboration\nPlease feel free to study together and talk to one another about project assignments. The mutual instruction that students give each other is among the most valuable that can be achieved.\nHowever, it is expected that project assignments will be implemented and written up independently unless otherwise specified. Specifically, please do not share analytic code or output. Please do not collaborate on write-up and interpretation. Please do not access or use solutions from any source before your project assignment is submitted for grading."
  },
  {
    "objectID": "syllabus.html#discussion-forum",
    "href": "syllabus.html#discussion-forum",
    "title": "Syllabus",
    "section": "Discussion Forum",
    "text": "Discussion Forum\nThe course will make use of the CoursePlus Discussion Forum in order to ask and answer questions regarding any of the course materials. The Instructor and the Teaching Assistants will monitor the discussion boards and answer questions when appropriate."
  },
  {
    "objectID": "syllabus.html#exams",
    "href": "syllabus.html#exams",
    "title": "Syllabus",
    "section": "Exams",
    "text": "Exams\nThere are no exams in this course."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nGrades in the course will be based on Projects 0–3 with a percentage of the final grade being apportioned to each assignment. Each of Projects 1–3 counts approximately equally in the final grade. Grades for the projects and the final grade will be issued via the CoursePlus grade book."
  },
  {
    "objectID": "syllabus.html#academic-ethics-and-student-conduct-code",
    "href": "syllabus.html#academic-ethics-and-student-conduct-code",
    "title": "Syllabus",
    "section": "Academic Ethics and Student Conduct Code",
    "text": "Academic Ethics and Student Conduct Code\nStudents enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations.\nStudents should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal.\nThe faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions."
  },
  {
    "objectID": "syllabus.html#disability-support-service",
    "href": "syllabus.html#disability-support-service",
    "title": "Syllabus",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with Student Disability Service (SDS). It is the responsibility of the student to register for accommodations with SDS. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. Once you are f a student in your class has approved accommodations you will receive formal notification and the student will be encouraged to reach out. If you have questions about requesting accommodations, please contact BSPH.dss@jhu.edu."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis is a quantitative course. We will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking are useful. Previous experience with writing computer programs in general and R in particular is also helpful, but not necessary. If you have no programming experience, expect to spend extra time getting yourself familiar with R. As long as you are willing to invest the time to learn the programming and you do not mind thinking quantitatively, you should be able to take the course, independent of your background.\nFormal requirement for the course is Biostatistics 140.621. Knowledge of material from 140.621 is assumed. If you didn’t take this course, please contact me to get permission to enroll.\n\nGetting set up\nYou must install R and RStudio on your computer in order to complete this course. These are two different applications that must be installed separately before they can be used together:\n\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\n\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\n\ninstall.packages(\"tidyverse\")\n\nin the R console.\n\nHow to Download R for Windows\nGo to https://cran.r-project.org and\n\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.2.1 for Windows”\n\n\n\n\n\n\n\nWarning\n\n\n\nThe version in the video is not the latest version. Please download the latest version.\n\n\n\n\n\nVideo Demo for Downloading R for Windows\n\n\n\n\nHow to Download R for the Mac\nGoto https://cran.r-project.org and\n\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.2.1.pkg”\n\n\n\n\n\n\n\nWarning\n\n\n\nThe version in the video is not the latest version. Please download the latest version.\n\n\n\n\n\nVideo Demo for Downloading R for the Mac\n\n\n\n\nHow to Download RStudio\nGoto https://rstudio.com and\n\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe video shows how to download RStudio for the Mac but you should download RStudio for whatever computing setup you have\n\n\n\n\n\nVideo Demo for Downloading RStudio"
  },
  {
    "objectID": "syllabus.html#general-disclaimers",
    "href": "syllabus.html#general-disclaimers",
    "title": "Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\n\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Statistical Computing!",
    "section": "",
    "text": "Welcome to Statistical Computing at Johns Hopkins Bloomberg School of Public Health!"
  },
  {
    "objectID": "index.html#what-is-this-course",
    "href": "index.html#what-is-this-course",
    "title": "Welcome to Statistical Computing!",
    "section": "What is this course?",
    "text": "What is this course?\nThis course covers the basics of practical issues in programming and other computer skills required for the research and application of statistical methods. Includes programming in R and the tidyverse, data ethics, best practices for coding and reproducible research, introduction to data visualizations, best practices for working with special data types (dates/times, text data, etc), best practices for storing data, basics of debugging, organizing and commenting code, basics of leveraging Python from R. Topics in statistical data analysis provide working examples."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Statistical Computing!",
    "section": "Getting started",
    "text": "Getting started\nI suggest that you start by looking over the Syllabus and Schedule under General Information. After that, start with the Lectures content in the given order."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to Statistical Computing!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Stephanie Hicks.\nThe following individuals have contributed to improving the course or materials have been adapted from their courses: Roger D. Peng, Andreas Handel, Naim Rashid, Michael Love.\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Big Book of R: https://www.bigbookofr.com\nList of resources to learn R (but also Python, SQL, Javascript): https://github.com/delabj/datacamp_alternatives/blob/master/index.md\nlearnr4free. Resources (books, videos, interactive websites, papers) to learn R. Some of the resources are beginner-friendly and start with the installation process: https://www.learnr4free.com/en\nData Science with R by Danielle Navarro: https://robust-tools.djnavarro.net"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "project 0\n\n\nprojects\n\n\n\n\nInformation for Project 0 (entirely optional, but hopefully useful and fun!)\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2022-08-30-project-0/index.html",
    "href": "projects/2022-08-30-project-0/index.html",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "Part 1\nThis part of the project is to ensure that you have successfully set up your computing environment. Please email (use the Subject line: 140.776 Setup) the Course Instructor (Dr. Stephanie Hicks) at shicks19@jhu.edu the following information:\n\nSetting up your computing environment\n\nYour name, JHED ID (if applicable).\nThe type of computer/operating system you are using (Windows, Mac, Unix/Linux, other)\nThe version of R that you have installed on your computer. To do this, start up R and run the following in the R console and include the output in your email.\n\n\nprint(R.version.string)\n\n\n\n\nPrinting the R version string\n\n\n\nThe version of RStudio that you have installed on your computer. To do this start up RStudio and in the R console window, run the following and again include the output in your email:\n\n\nprint(RStudio.Version()$version)\n\n\nIf you have a GitHub username, please include this in your email. If you do not have a GitHub username, read https://happygitwithr.com, sign up for GitHub, and include your new username in your email to me.\nTo make sure git is installed on your system, use the ‘Terminal’ (e.g. it’s next to the R Console within RStudio) (or whatever you use), run the following and include the output in your email:\n\nFor example, this is mine:\n\ngit --version\n\ngit version 2.32.1 (Apple Git-133)\n\n\nIf you have any trouble with any of the steps above, try to first post on the discussion board on CoursePlus. The TAs and I will be checking it frequently, but other students may also be helpful in their replies. You can also use other resources to get help (Google, R4DS, colleagues/friends/relatives with R/Markdown experience, etc.). Still, try to do as much as possible yourself. We will use all the bits you are learning here repeatedly during this course.\n\n\n\nPart 2\nThis part of the project is to help you introduce yourself (and your interests!) to others in this course. You will create a new GitHub repository and build a small website about yourself.\n\n1. Create a GitHub repo for your website\nCreate a new GitHub repository titled biostat776-intro-<firstname>-<lastname> (where you replace <firstname> with your first name and <lastname> with your last name) in your own personal GitHub account (e.g. https://github.com/<yourgithubusername>/biostat776-intro-<firstname>-<lastname>).\nFor example, you can find an example that I created for myself at\n\ngithub repo: https://github.com/stephaniehicks/biostat776-intro-stephanie-hicks\n\n\n\n2. Build a website using R Markdown\nUsing one of the many ways we discussed in class (e.g. a simple R Markdown website, blogdown, distill, etc), create a new project in RStudio with the appropriate files. For example, you might include the following information:\n\nWrite a short summary introducing yourself. Structure the webpage with headings, subheadings, etc. Talk a bit about yourself, your background, training, research interests. Let me/us know what kind of statistics, programming, data analysis experience you already have. I am also curious to know what you most hope to learn in this course.\nFive fun facts about yourself\nA web page linking to something you think is really cool/interesting/inspiring/etc. You could also describe briefly what it is and why you like it.\n\nIf you want, feel free to get creative and include other things. You can play with RMarkdown if you wish to, e.g., you can try to include some table or a video, etc.\n\n\n3. Include a README.md file\nYour project repository should include a README.md file (if it was not included already).\nEdit the repository README.md file. Typically it will only contain the name of your repository with a # sign in front. The # represents a level 1 heading in Markdown. Change the headline and call it “Introducing myself” (or something like that). Underneath write something like “This website contains a short introduction of Your Name.”\nMake sure the 2 files (README.md and especially index.Rmd / index.html) look the way you want. Make changes until everything works.\n\n\n4. Deploy your website\nDepending on how you want to deploy your website, the following may or may not be relevant to you. In general, you want to make sure you have initialized your project to use git (i.e. you can type git init to initialize the repository to use git. Add and commit your changes. Push your changes and deploy your website.\nFollowing steps 2-4, here is my example website:\n\nwebsite: https://www.stephaniehicks.com/biostat776-intro-stephanie-hicks\n\n\n\n5. Share your website\n\nGo to the Discussion Board in CoursePlus and write a short post with a link (URL) to your website (and URL to the corresponding GitHub repository) that you created.\nAs you read the introductions from other folks in the class, feel free to comment/reply using Discussion board.\n\nIn class on Sept 8, I will show as many websites as I can from Courseplus!"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html",
    "href": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html",
    "title": "Introduction to R and RStudio!",
    "section": "",
    "text": "There are only two kinds of languages: the ones people complain about and the ones nobody uses. —Bjarne Stroustrup"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html#rtistry",
    "href": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html#rtistry",
    "title": "Introduction to R and RStudio!",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Water Colours’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2022-09-01-reproducible-research/index.html",
    "href": "posts/2022-09-01-reproducible-research/index.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "[Link to Claerbout and Karrenbach (1992) article]\n\nPre-lecture materials\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://ropensci.github.io/reproducibility-guide/sections/introduction\nhttps://rdpeng.github.io/Biostat776\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\n\n\n\n\nIntroduction\nThis lecture will be about reproducible reporting, and I want to take the opportunity to cover some basic concepts and ideas that are related to reproducible reporting, just in case you have not heard about it or don not know what it is.\nBefore we get to reproducibility, we need to cover a little background with respect to how science works (even if you are not a scientist, this is important).\nThe ultimate standard in strengthening scientific evidence is replication. Assume you claim that X causes Y, or that Vitamin C improves disease. The goal of replication is to have independent people to do independent things with different data, different methods, and different laboratories and see if they get the same result.\nThere is a sense that if a relationship in nature is truly there, then it should be robust to having different people discover it in different ways. Replication is particularly important in areas where findings can have big policy impacts or can influence regulatory types of decisions.\n\nWhat is wrong with replication?\nThere is really nothing wrong with it. This is what science has been doing for a long time, through hundreds of years. And there is nothing wrong with it today.\nBut the problem is that it is becoming more and more challenging to do replication or to replicate other studies.\nHere are some reasons:\n\nOften times studies are much larger and more costly than previously. If you want to do ten versions of the same study, you need ten times as much money and there is not as much money around as there used to be.\nSometimes it is difficult to replicate a study because if the original study took 20 years to complete, it is difficult to wait around another 20 years for replication.\nSome studies are just plain unique, such as studying the impact of a massive earthquake in a very specific location and time. If you are looking at a unique situation in time or a unique population, you cannot readily replicate that situation.\n\nThere are a lot of good reasons why you cannot replicate a study. If you cannot replicate a study, is the alternative just to do nothing (?? 😱), just let that study stand by itself?\nThe idea behind a reproducible reporting is to create a kind of minimum standard (or a middle ground) where we will not be replicating a study, but maybe we can do something in between. What can we do that’s in between the gold standard and doing nothing?\nThat is where reproducibility comes in. That’s how we can kind of bridge the gap between replication and nothing.\nIn non-research settings, often full replication is not even the point. Often the goal is to preserve something to the point where anybody in an organization can repeat what you did (for example, after you leave the organization).\n\nIn this case, reproducibility is key to maintaining the history of a project and making sure that every step along the way is clear.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nReplication, whereby scientific questions are examined and verified independently by different scientists, is the gold standard for scientific validity.\nReplication can be difficult and often there are no resources to independently replicate a study.\nReproducibility, whereby data and code are re-analyzed by independent scientists to obtain the same results of the original investigator, is a reasonable minimum standard when replication is not possible.\n\n\n\n\n\n\nReproducibility to the Rescue\nLet’s first define reproducibility. The basic idea is that you need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you have run, and come to the same findings that you found.\nWhat reproducible reporting is about is a validation of the data analysis (not the original question itself). Because you are not collecting independent data using independent methods, it is a little bit more difficult to validate the scientific question itself. But if you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nIn this way, you can at least have confidence that you can reproduce the analysis.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\n\nhttps://www.science.org/toc/science/334/6060\n\nOther journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\n\nhttps://jasa-acs.github.io/repro-guide\n\n\nWhy does this matter?\nHere is an example. In 2012, a feature on the TV show 60 minutes looked at a major incident at Duke University where many results involving a promising cancer test were found to be not reproducible. This led to a number of studies and clinical trials having to be stopped, followed by an investigation which is still ongoing.\n\n\n\n\n[Source on YouTube]\n\n\nTypes of reproducibility\nWhat are the different kinds of reproducible research? Enabling reproducibility can be complicated, but by separating out some of the levels and degrees of reproducibility the problem can become more manageable because we can focus our efforts on what best suits our specific scientific domain. Victoria Stodden (2014), a prominent scholar on this topic, has identified some useful distinctions in reproducible research:\n\nComputational reproducibility: when detailed information is provided about code, software, hardware and implementation details.\nEmpirical reproducibility: when detailed information is provided about non-computational empirical scientific experiments and observations. In practice this is enabled by making data freely available, as well as details of how the data was collected.\nStatistical reproducibility: when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. This mostly relates to pre-registration of study design to prevent p-value hacking and other manipulations.\n\n[Source]\n\n\nElements of computational reproducibility\nWhat do we need for computational reproducibility? There are a variety of ways to talk about this, but one basic definition that we hae come up with is that there are four things that are required to make results reproducible:\n\nAnalytic data. The data that were used for the analysis that was presented should be available for others to access. This is different from the raw data because very often in a data analysis the raw data are not all used for the analysis, but rather some subset is used. It may be interesting to see the raw data but impractical to actually have it. Analytic data is key to examining the data analysis.\nAnalytic code. The analytic code is the code that was applied to the analytic data to produce the key results. This may be preprocessing code, regression modeling code, or really any other code used to produce the results from the analytic data.\nDocumentation. Documentation of that code and the data is very important.\nDistribution. Finally, there needs to be some standard means of distribution, so all this data in the code is easily accessible.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nReproducible reporting is about is a validation of the data analysis\nThere are multiple types of reproducibility\nThere are four elements to computational reproducibility\n\n\n\n\n\n\n“X” to “Computational X”\nWhat is driving this need for a “reproducibility middle ground” between replication and doing nothing?\nFor starters, there are a lot of new technologies on the scene and in many different fields of study including, biology, chemistry and environmental science. These technologies allow us to collect data at a much higher throughput so we end up with these very complex and very high dimensional data sets.\nThese datasets can be collected almost instantaneously compared to even just ten years ago—the technology has allowed us to create huge data sets at essentially the touch of a button. Furthermore, we the computing power to take existing (already huge) databases and merge them into even bigger and bigger databases. Finally, the massive increase in computing power has allowed us to implement more sophisticated and complex analysis routines.\nThe analyses themselves, the models that we fit and the algorithms that we run, are much much more complicated than they used to be. Having a basic understanding of these algorithms is difficult, even for a sophisticated person, and it is almost impossible to describe these algorithms with words alone.\nUnderstanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used.\nThe bottom line with all these different trends is that for every field “X”, there is now “Computational X”. There’s computational biology, computational astronomy—whatever it is you want, there is a computational version of it.\n\nExample: machine learning in the life sciences\nOne example of an area where reproducibility is important comes from research that I have conducted in the area of machine learning in the life sciences.\nIn the above article, computational reproducibility is not throught of as a binary property, but rather it is on a sliding scale that reflects the time needed to reproduce. Published works fall somewhere on this scale, which is bookended by ‘forever’, for a completely irreproducible work, and ‘zero’, for a work where one can automatically repeat the entire analysis with a single keystroke.\nAs in many cases it is difficult to impose a single standard that divides work into ‘reproducible’ and ‘irreproducible’. Therefore, instead a menu is proposed of three standards with varying degrees of rigor for computational reproducibility:\n\nBronze standard. The authors make the data, models and code used in the analysis publicly available. The bronze standard is the minimal standard for reproducibility. Without data, models and code, it is not possible to reproduce a work.\nSilver standard. In addition to meeting the bronze standard: (1) the dependencies of the analysis can be downloaded and installed in a single command; (2) key details for reproducing the work are documented, including the order in which to run the analysis scripts, the operating system used and system resource requirements; and (3) all random components in the analysis are set to be deterministic. The silver standard is a midway point between minimal availability and full automation. Works that meet this standard will take much less time to reproduce than ones only meeting the bronze standard.\nGold standard. The work meets the silver standard, and the authors make the analysis reproducible with a single command. The gold standard for reproducibility is full automation. When a work meets this standard, it will take little to no effort for a scientist to reproduce it.\n\n\n\n\nThe Data Science Pipeline\nThe basic issue is when you read a description of a data analysis, such as in an article or a technical report, for the most part, what you get is the report and nothing else.\nOf course, everyone knows that behind the scenes there’s a lot that went into this article and that is what I call the data science pipeline.\n\n\n\nThe Data Science Pipeline\n\n\nIn this pipeline, there are two “actors”: the author of the report/article and the reader.\n\nOn the left side, the author is going from left to right along this pipeline.\nOn the right side, the reader is going from right to left.\n\nIf you are the reader, you read the article, and you may want to know more about what happened e.g.\n\nWhere are the data?\nWhat methods were used here?\n\nThe basic idea behind computational reproducibility is to focus on the elements in the blue box: the analytic data and the computational results. With computational reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”.\n\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience.\n\nPublishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely).\nResources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\n\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together.\n\nReaders may not have the same computational resources that the original authors did.\nIf the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\n\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing.\n\nIn practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized.\nThere are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\n\n\n\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat is the difference between replication and reproducible?\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?"
  },
  {
    "objectID": "posts/2022-09-01-reference-management/index.html",
    "href": "posts/2022-09-01-reference-management/index.html",
    "title": "Reference management",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nAuthoring in R Markdown from RStudio\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nBibliography from R Markdown Cookbook\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html"
  },
  {
    "objectID": "posts/2022-09-01-reference-management/index.html#rtistry",
    "href": "posts/2022-09-01-reference-management/index.html#rtistry",
    "title": "Reference management",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n[Add here.]"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html",
    "title": "Reading and Writing data",
    "section": "",
    "text": "[Source]"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#advantages",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#advantages",
    "title": "Reading and Writing data",
    "section": "Advantages",
    "text": "Advantages\nThe advantage of the read_csv() function is perhaps better understood from an historical perspective.\n\nR’s built in read.csv() function similarly reads CSV files, but the read_csv() function in readr builds on that by removing some of the quirks and “gotchas” of read.csv() as well as dramatically optimizing the speed with which it can read data into R.\nThe read_csv() function also adds some nice user-oriented features like a progress meter and a compact method for specifying column types."
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#example",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#example",
    "title": "Reading and Writing data",
    "section": "Example",
    "text": "Example\nA typical call to read_csv() will look as follows.\n\nlibrary(readr)\nteams <- read_csv(here(\"data\", \"team_standings.csv\"))\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Team\ndbl (1): Standing\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nteams\n\n# A tibble: 32 × 2\n   Standing Team       \n      <dbl> <chr>      \n 1        1 Spain      \n 2        2 Netherlands\n 3        3 Germany    \n 4        4 Uruguay    \n 5        5 Argentina  \n 6        6 Brazil     \n 7        7 Ghana      \n 8        8 Paraguay   \n 9        9 Japan      \n10       10 Chile      \n# … with 22 more rows\n\n\nBy default, read_csv() will open a CSV file and read it in line-by-line. Similar to read.table(), you can tell the function to skip lines or which lines are comments:\n\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2)\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n\nAlternatively, you can use the comment argument:\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\")\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n\nIt will also (by default), read in the first few rows of the table in order to figure out the type of each column (i.e. integer, character, etc.). From the read_csv() help page:\n\nIf ‘NULL’, all column types will be imputed from the first 1000 rows on the input. This is convenient (and fast), but not robust. If the imputation fails, you’ll need to supply the correct types yourself.\n\nYou can specify the type of each column with the col_types argument.\n\n\n\n\n\n\nNote\n\n\n\nIn general, it is a good idea to specify the column types explicitly.\nThis rules out any possible guessing errors on the part of read_csv().\nAlso, specifying the column types explicitly provides a useful safety check in case anything about the dataset should change without you knowing about it.\n\n\nHere is an example of how to specify the column types explicitly:\n\nteams <- read_csv(here(\"data\", \"team_standings.csv\"), \n                  col_types = \"cc\")\n\nNote that the col_types argument accepts a compact representation. Here \"cc\" indicates that the first column is character and the second column is character (there are only two columns). Using the col_types argument is useful because often it is not easy to automatically figure out the type of a column by looking at a few rows (especially if a column has many missing values).\n\n\n\n\n\n\nNote\n\n\n\nThe read_csv() function will also read compressed files automatically.\nThere is no need to decompress the file first or use the gzfile connection function.\n\n\nThe following call reads a gzip-compressed CSV file containing download logs from the RStudio CRAN mirror.\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 n_max = 10)\n\nRows: 10 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): r_version, r_arch, r_os, package, version, country\ndbl  (2): size, ip_id\ndate (1): date\ntime (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that the warnings indicate that read_csv() may have had some difficulty identifying the type of each column. This can be solved by using the col_types argument.\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 col_types = \"ccicccccci\", \n                 n_max = 10)\nlogs\n\n# A tibble: 10 × 10\n   date       time       size r_ver…¹ r_arch r_os  package version country ip_id\n   <chr>      <chr>     <int> <chr>   <chr>  <chr> <chr>   <chr>   <chr>   <int>\n 1 2016-07-19 22:00:00 1.89e6 3.3.0   x86_64 ming… data.t… 1.9.6   US          1\n 2 2016-07-19 22:00:05 4.54e4 3.3.1   x86_64 ming… assert… 0.1     US          2\n 3 2016-07-19 22:00:03 1.43e7 3.3.1   x86_64 ming… stringi 1.1.1   DE          3\n 4 2016-07-19 22:00:05 1.89e6 3.3.1   x86_64 ming… data.t… 1.9.6   US          4\n 5 2016-07-19 22:00:06 3.90e5 3.3.1   x86_64 ming… foreach 1.4.3   US          4\n 6 2016-07-19 22:00:08 4.88e4 3.3.1   x86_64 linu… tree    1.0-37  CO          5\n 7 2016-07-19 22:00:12 5.25e2 3.3.1   x86_64 darw… surviv… 2.39-5  US          6\n 8 2016-07-19 22:00:08 3.23e6 3.3.1   x86_64 ming… Rcpp    0.12.5  US          2\n 9 2016-07-19 22:00:09 5.56e5 3.3.1   x86_64 ming… tibble  1.1     US          2\n10 2016-07-19 22:00:10 1.52e5 3.3.1   x86_64 ming… magrit… 1.5     US          2\n# … with abbreviated variable name ¹​r_version\n\n\nYou can specify the column type in a more detailed fashion by using the various col_*() functions.\nFor example, in the log data above, the first column is actually a date, so it might make more sense to read it in as a Date object.\nIf we wanted to just read in that first column, we could do\n\nlogdates <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                     col_types = cols_only(date = col_date()),\n                     n_max = 10)\nlogdates\n\n# A tibble: 10 × 1\n   date      \n   <date>    \n 1 2016-07-19\n 2 2016-07-19\n 3 2016-07-19\n 4 2016-07-19\n 5 2016-07-19\n 6 2016-07-19\n 7 2016-07-19\n 8 2016-07-19\n 9 2016-07-19\n10 2016-07-19\n\n\nNow the date column is stored as a Date object which can be used for relevant date-related computations (for example, see the lubridate package).\n\n\n\n\n\n\nNote\n\n\n\nThe read_csv() function has a progress option that defaults to TRUE.\nThis options provides a nice progress meter while the CSV file is being read.\nHowever, if you are using read_csv() in a function, or perhaps embedding it in a loop, it is probably best to set progress = FALSE."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome! I am very excited to have you in our one-term (i.e. half a semester) course on Statistical Computing course number (140.776) offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health.\nThis course is designed for ScM and PhD students at Johns Hopkins Bloomberg School of Public Health. I am pretty flexible about permitting outside students, but I want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works.\nThis class is not designed to teach the theoretical aspects of statistical or computational methods, but rather the goal is to help with the practical issues related to setting up a statistical computing environment for data analyses, developing high-quality R packages, conducting reproducible data analyses, best practices for data visualization and writing code, and creating websites for personal or project use."
  },
  {
    "objectID": "posts/welcome/index.html#disability-support-service",
    "href": "posts/welcome/index.html#disability-support-service",
    "title": "Welcome!",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with Student Disability Service (SDS). It is the responsibility of the student to register for accommodations with SDS. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. Once you are f a student in your class has approved accommodations you will receive formal notification and the student will be encouraged to reach out. If you have questions about requesting accommodations, please contact BSPH.dss@jhu.edu."
  },
  {
    "objectID": "posts/welcome/index.html#previous-versions-of-the-class",
    "href": "posts/welcome/index.html#previous-versions-of-the-class",
    "title": "Welcome!",
    "section": "Previous versions of the class",
    "text": "Previous versions of the class\n\nhttps://www.stephaniehicks.com/jhustatcomputing2021\nhttps://rdpeng.github.io/Biostat776"
  },
  {
    "objectID": "posts/welcome/index.html#typos-and-corrections",
    "href": "posts/welcome/index.html#typos-and-corrections",
    "title": "Welcome!",
    "section": "Typos and corrections",
    "text": "Typos and corrections\nFeel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/stephaniehicks/jhustatcomputing2022. You will have the thanks of your grateful instructor!"
  },
  {
    "objectID": "posts/welcome/index.html#rtistry",
    "href": "posts/welcome/index.html#rtistry",
    "title": "Welcome!",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Unboxing’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-gitgithub/index.html",
    "href": "posts/2022-08-30-introduction-to-gitgithub/index.html",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nHappy Git with R from Jenny Bryan\nChapter on git and GitHub in dsbook from Rafael Irizarry\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-gitgithub/index.html#rtistry",
    "href": "posts/2022-08-30-introduction-to-gitgithub/index.html#rtistry",
    "title": "Introduction to git/GitHub",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Flametree’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html",
    "href": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html",
    "title": "Managing data frames with the Tidyverse",
    "section": "",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the advantages of a tibble and data.frame data objects in R\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\n\n\n\n\n\nData Frames\nThe data frame (or data.frame) is a key data structure in statistics and in R. The basic structure of a data frame is that there is one observation per row and each column represents a variable, a measure, feature, or characteristic of that observation. R has an internal implementation of data frames that is likely the one you will use most often. However, there are packages on CRAN that implement data frames via things like relational databases that allow you to operate on very very large data frames (but we will not discuss them here).\nGiven the importance of managing data frames, it is important that we have good tools for dealing with them. For example, operations like filtering rows, re-ordering rows, and selecting columns, can often be tedious operations in R whose syntax is not very intuitive. The dplyr package is designed to mitigate a lot of these problems and to provide a highly optimized set of routines specifically for dealing with data frames.\n\nTibbles\nAnother type of data structure that we need to discuss is called the tibble! It’s best to think of tibbles as an updated and stylish version of the data.frame. And, tibbles are what tidyverse packages work with most seamlessly. Now, that does not mean tidyverse packages require tibbles. In fact, they still work with data.frames, but the more you work with tidyverse and tidyverse-adjacent packages, the more you will see the advantages of using tibbles.\nBefore we go any further, tibbles are data frames, but they have some new bells and whistles to make your life easier.\n\nHow tibbles differ from data.frame\nThere are a number of differences between tibbles and data.frames. To see a full vignette about tibbles and how they differ from data.frame, you will want to execute vignette(\"tibble\") and read through that vignette. However, we will summarize some of the most important points here:\n\nInput type remains unchanged - data.frame is notorious for treating strings as factors; this will not happen with tibbles\nVariable names remain unchanged - In base R, creating data.frames will remove spaces from names, converting them to periods or add “x” before numeric column names. Creating tibbles will not change variable (column) names.\nThere are no row.names() for a tibble - Tidy data requires that variables be stored in a consistent way, removing the need for row names.\nTibbles print first ten rows and columns that fit on one screen - Printing a tibble to screen will never print the entire huge data frame out. By default, it just shows what fits to your screen.\n\n\n\nCreating a tibble\nThe tibble package is part of the tidyverse and can thus be loaded in (once installed) using:\n\nlibrary(tidyverse)\n\n\n\nas_tibble()\nSince many packages use the historical data.frame from base R, you will often find yourself in the situation that you have a data.frame and want to convert that data.frame to a tibble. To do so, the as_tibble() function is exactly what you are looking for.\nFor the example, in this lesson we will be using a dataset containing air pollution and temperature data for the city of Chicago in the U.S. The dataset is available in this repository.\nYou can load the data into R using the readRDS() function.\n\nlibrary(here)\n\nhere() starts at /Users/stephaniehicks/Documents/github/teaching/jhustatcomputing2022\n\nchicago <- readRDS(here(\"data\", \"chicago.rds\"))\n\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\n\ndim(chicago)\n\n[1] 6940    8\n\nstr(chicago)\n\n'data.frame':   6940 obs. of  8 variables:\n $ city      : chr  \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num  31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num  31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date, format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num  NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num  34 NA 34.2 47 NA ...\n $ o3tmean2  : num  4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num  20 23.2 23.8 30.4 30.3 ...\n\n\nWe see this data structure is a data.frame with 6940 observations and 8 variables. To convert this data.frame to a tibble you would use the following:\n\nstr(as_tibble(chicago))\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\n\nNote in the above example and as mentioned earlier, that tibbles, by default, only print the first ten rows to screen. If you were to print chicago to screen, all 6940 rows would be displayed. When working with large data.frames, this default behavior can be incredibly frustrating. Using tibbles removes this frustration because of the default settings for tibble printing.\nAdditionally, you will note that the type of the variable is printed for each variable in the tibble. This helpful feature is another added bonus of tibbles relative to data.frame.\nIf you do want to see more rows from the tibble, there are a few options! First, the View() function in RStudio is incredibly helpful. The input to this function is the data.frame or tibble you would like to see. Specifically, View(chicago) would provide you, the viewer, with a scrollable view (in a new tab) of the complete dataset.\nA second option is the fact that print() enables you to specify how many rows and columns you would like to display. Here, we again display the chicago data.frame as a tibble but specify that we’d only like to see 5 rows. The width = Inf argument specifies that we would like to see all the possible columns. Here, there are only 8, but for larger datasets, this can be helpful to specify.\n\nas_tibble(chicago) %>% \n  print(n = 5, width = Inf)\n\n# A tibble: 6,940 × 8\n  city   tmpd  dptp date       pm25tmean2 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>      <dbl>    <dbl>     <dbl>\n1 chic   31.5  31.5 1987-01-01         NA       34       4.25      20.0\n2 chic   33    29.9 1987-01-02         NA       NA       3.30      23.2\n3 chic   33    27.4 1987-01-03         NA       34.2     3.33      23.8\n4 chic   29    28.6 1987-01-04         NA       47       4.38      30.4\n5 chic   32    28.9 1987-01-05         NA       NA       4.75      30.3\n# … with 6,935 more rows\n\n\n\n\ntibble()\nAlternatively, you can create a tibble on the fly by using tibble() and specifying the information you’d like stored in each column. Note that if you provide a single value, this value will be repeated across all rows of the tibble. This is referred to as “recycling inputs of length 1.”\nIn the example here, we see that the column c will contain the value ‘1’ across all rows.\n\ntibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n# A tibble: 5 × 4\n      a     b     c     z\n  <int> <int> <dbl> <dbl>\n1     1     6     1    50\n2     2     7     1    82\n3     3     8     1   122\n4     4     9     1   170\n5     5    10     1   226\n\n\nThe tibble() function allows you to quickly generate tibbles and even allows you to reference columns within the tibble you’re creating, as seen in column z of the example above.\nWe also noted previously that tibbles can have column names that are not allowed in data.frame. In this example, we see that to utilize a nontraditional variable name, you surround the column name with backticks. Note that to refer to such columns in other tidyverse packages, you’ll continue to use backticks surrounding the variable name.\n\ntibble(\n  `two words` = 1:5,\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\n\n# A tibble: 5 × 3\n  `two words` `12`    `:)` \n        <int> <chr>   <chr>\n1           1 numeric smile\n2           2 numeric smile\n3           3 numeric smile\n4           4 numeric smile\n5           5 numeric smile\n\n\n\n\nSubsetting\nSubsetting tibbles also differs slightly from how subsetting occurs with data.frame. When it comes to tibbles, [[ can subset by name or position; $ only subsets by name. For example:\n\ndf <- tibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n# Extract by name using $ or [[]]\ndf$z\n\n[1]  50  82 122 170 226\n\ndf[[\"z\"]]\n\n[1]  50  82 122 170 226\n\n# Extract by position requires [[]]\ndf[[4]]\n\n[1]  50  82 122 170 226\n\n\nHaving now discussed tibbles, which are the type of object most tidyverse and tidyverse-adjacent packages work best with, we now know the goal. In many cases, tibbles are ultimately what we want to work with in R. However, data are stored in many different formats outside of R. We will spend the rest of this lesson discussing wrangling functions that work either a data.frame or tibble.\n\n\n\n\nThe dplyr Package\nThe dplyr package was developed by RStudio and is an optimized and distilled version of the older plyr package for data manipulation. The dplyr package does not provide any “new” functionality to R per se, in the sense that everything dplyr does could already be done with base R, but it greatly simplifies existing functionality in R.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulation and for operating on data frames. With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist. Another useful contribution is that the dplyr functions are very fast, as many key operations are coded in C++.\n\n\n\nArtwork by Allison Horst on the dplyr package\n\n\n[Source: Artwork by Allison Horst]\n\ndplyr grammar\nSome of the key “verbs” provided by the dplyr package are\n\nselect(): return a subset of the columns of a data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame based on logical conditions\narrange(): reorder rows of a data frame\nrename(): rename variables in a data frame\nmutate(): add new variables/columns or transform existing variables\nsummarise() / summarize(): generate summary statistics of different variables in the data frame, possibly within strata\n%>%: the “pipe” operator is used to connect multiple verb actions together into a pipeline\n\nThe dplyr package as a number of its own data types that it takes advantage of. For example, there is a handy print method that prevents you from printing a lot of data to the console. Most of the time, these additional data types are transparent to the user and do not need to be worried about.\n\n\ndplyr functions\nAll of the functions that we will discuss in this Chapter will have a few common characteristics. In particular,\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame specified in the first argument, and you can refer to columns in the data frame directly (without using the $ operator, just use the column names).\nThe return result of a function is a new data frame.\nData frames must be properly formatted and annotated for this to all be useful. In particular, the data must be tidy. In short, there should be one observation per row, and each column should represent a feature or characteristic of that observation.\n\n\n\n\nArtwork by Allison Horst on tidy data\n\n\n[Source: Artwork by Allison Horst]\n\n\ndplyr installation\nThe dplyr package can be installed from CRAN or from GitHub using the devtools package and the install_github() function. The GitHub repository will usually contain the latest updates to the package and the development version.\nTo install from CRAN, just run\n\ninstall.packages(\"dplyr\")\n\nThe dplyr package is also installed when you install the tidyverse meta-package.\nAfter installing the package it is important that you load it into your R session with the library() function.\n\nlibrary(dplyr)\n\nYou may get some warnings when the package is loaded because there are functions in the dplyr package that have the same name as functions in other packages. For now you can ignore the warnings.\n\n\nselect()\nWe will continue to use the chicago dataset containing air pollution and temperature data.\n\nchicago <- as_tibble(chicago)\nstr(chicago)\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\n\nThe select() function can be used to select columns of a data frame that you want to focus on. Often you will have a large data frame containing “all” of the data, but any given analysis might only use a subset of variables or observations. The select() function allows you to get the few columns you might need.\nSuppose we wanted to take the first 3 columns only. There are a few ways to do this. We could for example use numerical indices. But we can also use the names directly.\n\nnames(chicago)[1:3]\n\n[1] \"city\" \"tmpd\" \"dptp\"\n\nsubset <- select(chicago, city:dptp)\nhead(subset)\n\n# A tibble: 6 × 3\n  city   tmpd  dptp\n  <chr> <dbl> <dbl>\n1 chic   31.5  31.5\n2 chic   33    29.9\n3 chic   33    27.4\n4 chic   29    28.6\n5 chic   32    28.9\n6 chic   40    35.1\n\n\nNote that the : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\nYou can also omit variables using the select() function by using the negative sign. With select() you can do\n\nselect(chicago, -(city:dptp))\n\nwhich indicates that we should include every variable except the variables city through dptp. The equivalent code in base R would be\n\ni <- match(\"city\", names(chicago))\nj <- match(\"dptp\", names(chicago))\nhead(chicago[, -(i:j)])\n\nNot super intuitive, right?\nThe select() function also allows a special syntax that allows you to specify variable names based on patterns. So, for example, if you wanted to keep every variable that ends with a “2”, we could do\n\nsubset <- select(chicago, ends_with(\"2\"))\nstr(subset)\n\ntibble [6,940 × 4] (S3: tbl_df/tbl/data.frame)\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\n\nOr if we wanted to keep every variable that starts with a “d”, we could do\n\nsubset <- select(chicago, starts_with(\"d\"))\nstr(subset)\n\ntibble [6,940 × 2] (S3: tbl_df/tbl/data.frame)\n $ dptp: num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date: Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n\n\nYou can also use more general regular expressions if necessary. See the help page (?select) for more details.\n\n\nfilter()\nThe filter() function is used to extract subsets of rows from a data frame. This function is similar to the existing subset() function in R but is quite a bit faster in my experience.\n\n\n\nArtwork by Allison Horst on filter() function\n\n\n[Source: Artwork by Allison Horst]\nSuppose we wanted to extract the rows of the chicago data frame where the levels of PM2.5 are greater than 30 (which is a reasonably high level), we could do\n\nchic.f <- filter(chicago, pm25tmean2 > 30)\nstr(chic.f)\n\ntibble [194 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:194] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:194] 23 28 55 59 57 57 75 61 73 78 ...\n $ dptp      : num [1:194] 21.9 25.8 51.3 53.7 52 56 65.8 59 60.3 67.1 ...\n $ date      : Date[1:194], format: \"1998-01-17\" \"1998-01-23\" ...\n $ pm25tmean2: num [1:194] 38.1 34 39.4 35.4 33.3 ...\n $ pm10tmean2: num [1:194] 32.5 38.7 34 28.5 35 ...\n $ o3tmean2  : num [1:194] 3.18 1.75 10.79 14.3 20.66 ...\n $ no2tmean2 : num [1:194] 25.3 29.4 25.3 31.4 26.8 ...\n\n\nYou can see that there are now only 194 rows in the data frame and the distribution of the pm25tmean2 values is.\n\nsummary(chic.f$pm25tmean2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.05   32.12   35.04   36.63   39.53   61.50 \n\n\nWe can place an arbitrarily complex logical sequence inside of filter(), so we could for example extract the rows where PM2.5 is greater than 30 and temperature is greater than 80 degrees Fahrenheit.\n\nchic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)\nselect(chic.f, date, tmpd, pm25tmean2)\n\n# A tibble: 17 × 3\n   date        tmpd pm25tmean2\n   <date>     <dbl>      <dbl>\n 1 1998-08-23    81       39.6\n 2 1998-09-06    81       31.5\n 3 2001-07-20    82       32.3\n 4 2001-08-01    84       43.7\n 5 2001-08-08    85       38.8\n 6 2001-08-09    84       38.2\n 7 2002-06-20    82       33  \n 8 2002-06-23    82       42.5\n 9 2002-07-08    81       33.1\n10 2002-07-18    82       38.8\n11 2003-06-25    82       33.9\n12 2003-07-04    84       32.9\n13 2005-06-24    86       31.9\n14 2005-06-27    82       51.5\n15 2005-06-28    85       31.2\n16 2005-07-17    84       32.7\n17 2005-08-03    84       37.9\n\n\nNow there are only 17 observations where both of those conditions are met.\nOther logical operators you should be aware of include:\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nEquals\ncity == chic\n\n\n!=\nDoes not equal\ncity != chic\n\n\n>\nGreater than\ntmpd > 32.0\n\n\n>=\nGreater than or equal to\ntmpd >- 32.0\n\n\n<\nLess than\ntmpd < 32.0\n\n\n<=\nLess than or equal to\ntmpd <= 32.0\n\n\n%in%\nIncluded in\ncity %in% c(\"chic\", \"bmore\")\n\n\nis.na()\nIs a missing value\nis.na(pm10tmean2)\n\n\n\nIf you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement. A common use of this is to identify observations with non-missing data (e.g., !(is.na(pm10tmean2))).\n\n\narrange()\nThe arrange() function is used to reorder rows of a data frame according to one of the variables/columns. Reordering rows of a data frame (while preserving corresponding order of other columns) is normally a pain to do in R. The arrange() function simplifies the process quite a bit.\nHere we can order the rows of the data frame by date, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\nchicago <- arrange(chicago, date)\n\nWe can now check the first few rows\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 1987-01-01         NA\n2 1987-01-02         NA\n3 1987-01-03         NA\n\n\nand the last few rows.\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 2005-12-29       7.45\n2 2005-12-30      15.1 \n3 2005-12-31      15   \n\n\nColumns can be arranged in descending order too by useing the special desc() operator.\n\nchicago <- arrange(chicago, desc(date))\n\nLooking at the first three and last three rows shows the dates in descending order.\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 2005-12-31      15   \n2 2005-12-30      15.1 \n3 2005-12-29       7.45\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 1987-01-03         NA\n2 1987-01-02         NA\n3 1987-01-01         NA\n\n\n\n\nrename()\nRenaming a variable in a data frame in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nHere you can see the names of the first five variables in the chicago data frame.\n\nhead(chicago[, 1:5], 3)\n\n# A tibble: 3 × 5\n  city   tmpd  dptp date       pm25tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>\n1 chic     35  30.1 2005-12-31      15   \n2 chic     36  31   2005-12-30      15.1 \n3 chic     35  29.4 2005-12-29       7.45\n\n\nThe dptp column is supposed to represent the dew point temperature adn the pm25tmean2 column provides the PM2.5 data. However, these names are pretty obscure or awkward and probably be renamed to something more sensible.\n\nchicago <- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nhead(chicago[, 1:5], 3)\n\n# A tibble: 3 × 5\n  city   tmpd dewpoint date        pm25\n  <chr> <dbl>    <dbl> <date>     <dbl>\n1 chic     35     30.1 2005-12-31 15   \n2 chic     36     31   2005-12-30 15.1 \n3 chic     35     29.4 2005-12-29  7.45\n\n\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\n\nQuestion: How would you do the equivalent in base R without dplyr?\n\n\n\nmutate()\nThe mutate() function exists to compute transformations of variables in a data frame. Often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that.\n\n\n\nArtwork by Allison Horst on mutate() function\n\n\n[Source: Artwork by Allison Horst]\nFor example, with air pollution data, we often want to detrend the data by subtracting the mean from the data. That way we can look at whether a given day’s air pollution level is higher than or less than average (as opposed to looking at its absolute level).\nHere we create a pm25detrend variable that subtracts the mean from the pm25 variable.\n\nchicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\nhead(chicago)\n\n# A tibble: 6 × 9\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2 pm25detr…¹\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>      <dbl>\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2      -1.23\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8      -1.17\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0      -8.78\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3       1.52\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5       7.33\n6 chic     35     29.6 2005-12-26  8.4         8.5    14.0       16.8      -7.83\n# … with abbreviated variable name ¹​pm25detrend\n\n\nThere is also the related transmute() function, which does the same thing as mutate() but then drops all non-transformed variables.\nHere, we de-trend the PM10 and ozone (O3) variables.\n\nhead(transmute(chicago, \n               pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE),\n               o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n\n# A tibble: 6 × 2\n  pm10detrend o3detrend\n        <dbl>     <dbl>\n1      -10.4     -16.9 \n2      -14.7     -16.4 \n3      -10.4     -12.6 \n4       -6.40    -16.2 \n5       -6.90    -15.0 \n6      -25.4      -5.39\n\n\nNote that there are only two columns in the transmuted data frame.\n\n\ngroup_by()\nThe group_by() function is used to generate summary statistics from the data frame within strata defined by a variable. For example, in this air pollution dataset, you might want to know what the average annual level of PM2.5 is. So the stratum is the year, and that is something we can derive from the date variable.\nIn conjunction with the group_by() function we often use the summarize() function (or summarise() for some parts of the world).\nThe general operation here is a combination of splitting a data frame into separate pieces defined by a variable or group of variables (group_by()), and then applying a summary function across those subsets (summarize()).\nFirst, we can create a year variable using as.POSIXlt().\n\nchicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)\n\nNow we can create a separate data frame that splits the original data frame by year.\n\nyears <- group_by(chicago, year)\n\nFinally, we compute summary statistics for each year in the data frame with the summarize() function.\n\nsummarize(years, pm25 = mean(pm25, na.rm = TRUE), \n          o3 = max(o3tmean2, na.rm = TRUE), \n          no2 = median(no2tmean2, na.rm = TRUE))\n\n# A tibble: 19 × 4\n    year  pm25    o3   no2\n   <dbl> <dbl> <dbl> <dbl>\n 1  1987 NaN    63.0  23.5\n 2  1988 NaN    61.7  24.5\n 3  1989 NaN    59.7  26.1\n 4  1990 NaN    52.2  22.6\n 5  1991 NaN    63.1  21.4\n 6  1992 NaN    50.8  24.8\n 7  1993 NaN    44.3  25.8\n 8  1994 NaN    52.2  28.5\n 9  1995 NaN    66.6  27.3\n10  1996 NaN    58.4  26.4\n11  1997 NaN    56.5  25.5\n12  1998  18.3  50.7  24.6\n13  1999  18.5  57.5  24.7\n14  2000  16.9  55.8  23.5\n15  2001  16.9  51.8  25.1\n16  2002  15.3  54.9  22.7\n17  2003  15.2  56.2  24.6\n18  2004  14.6  44.5  23.4\n19  2005  16.2  58.8  22.6\n\n\nsummarize() returns a data frame with year as the first column, and then the annual averages of pm25, o3, and no2.\nIn a slightly more complicated example, we might want to know what are the average levels of ozone (o3) and nitrogen dioxide (no2) within quintiles of pm25. A slicker way to do this would be through a regression model, but we can actually do this quickly with group_by() and summarize().\nFirst, we can create a categorical variable of pm25 divided into quantiles\n\nqq <- quantile(chicago$pm25, seq(0, 1, 0.2), na.rm = TRUE)\nchicago <- mutate(chicago, pm25.quint = cut(pm25, qq))\n\nNow we can group the data frame by the pm25.quint variable.\n\nquint <- group_by(chicago, pm25.quint)\n\nFinally, we can compute the mean of o3 and no2 within quintiles of pm25.\n\nsummarize(quint, o3 = mean(o3tmean2, na.rm = TRUE), \n          no2 = mean(no2tmean2, na.rm = TRUE))\n\n# A tibble: 6 × 3\n  pm25.quint     o3   no2\n  <fct>       <dbl> <dbl>\n1 (1.7,8.7]    21.7  18.0\n2 (8.7,12.4]   20.4  22.1\n3 (12.4,16.7]  20.7  24.4\n4 (16.7,22.6]  19.9  27.3\n5 (22.6,61.5]  20.3  29.6\n6 <NA>         18.8  25.8\n\n\nFrom the table, it seems there is not a strong relationship between pm25 and o3, but there appears to be a positive correlation between pm25 and no2. More sophisticated statistical modeling can help to provide precise answers to these questions, but a simple application of dplyr functions can often get you most of the way there.\n\n\n%>%\nThe pipeline operator %>% is very handy for stringing together multiple dplyr functions in a sequence of operations. Notice above that every time we wanted to apply more than one function, the sequence gets buried in a sequence of nested function calls that is difficult to read, i.e.\n\nthird(second(first(x)))\n\nThis nesting is not a natural way to think about a sequence of operations. The %>% operator allows you to string operations in a left-to-right fashion, i.e.\n\nfirst(x) %>% second %>% third\n\nTake the example that we just did in the last section where we computed the mean of o3 and no2 within quintiles of pm25. There we had to\n\ncreate a new variable pm25.quint\nsplit the data frame by that new variable\ncompute the mean of o3 and no2 in the sub-groups defined by pm25.quint\n\nThat can be done with the following sequence in a single R expression.\n\nmutate(chicago, pm25.quint = cut(pm25, qq)) %>%    \n        group_by(pm25.quint) %>% \n        summarize(o3 = mean(o3tmean2, na.rm = TRUE), \n                  no2 = mean(no2tmean2, na.rm = TRUE))\n\n# A tibble: 6 × 3\n  pm25.quint     o3   no2\n  <fct>       <dbl> <dbl>\n1 (1.7,8.7]    21.7  18.0\n2 (8.7,12.4]   20.4  22.1\n3 (12.4,16.7]  20.7  24.4\n4 (16.7,22.6]  19.9  27.3\n5 (22.6,61.5]  20.3  29.6\n6 <NA>         18.8  25.8\n\n\nThis way we do not have to create a set of temporary variables along the way or create a massive nested sequence of function calls.\nNotice in the above code that I pass the chicago data frame to the first call to mutate(), but then afterwards I do not have to pass the first argument to group_by() or summarize(). Once you travel down the pipeline with %>%, the first argument is taken to be the output of the previous element in the pipeline.\nAnother example might be computing the average pollutant level by month. This could be useful to see if there are any seasonal trends in the data.\n\nmutate(chicago, month = as.POSIXlt(date)$mon + 1) %>% \n        group_by(month) %>% \n        summarize(pm25 = mean(pm25, na.rm = TRUE), \n                  o3 = max(o3tmean2, na.rm = TRUE), \n                  no2 = median(no2tmean2, na.rm = TRUE))\n\n# A tibble: 12 × 4\n   month  pm25    o3   no2\n   <dbl> <dbl> <dbl> <dbl>\n 1     1  17.8  28.2  25.4\n 2     2  20.4  37.4  26.8\n 3     3  17.4  39.0  26.8\n 4     4  13.9  47.9  25.0\n 5     5  14.1  52.8  24.2\n 6     6  15.9  66.6  25.0\n 7     7  16.6  59.5  22.4\n 8     8  16.9  54.0  23.0\n 9     9  15.9  57.5  24.5\n10    10  14.2  47.1  24.2\n11    11  15.2  29.5  23.6\n12    12  17.5  27.7  24.5\n\n\nHere, we can see that o3 tends to be low in the winter months and high in the summer while no2 is higher in the winter and lower in the summer.\n\n\nslice_*()\nThe slice_sample() function of the dplyr package will allow you to see a sample of random rows in random order. The number of rows to show is specified by the n argument. This can be useful if you don’t want to print the entire tibble, but you want to get a greater sense of the values. This is a good option for data analysis reports, where printing the entire tibble would not be appropriate if the tibble is quite large.\n\nslice_sample(chicago, n = 10)\n\n# A tibble: 10 × 11\n   city   tmpd dewpoint date        pm25 pm10tme…¹ o3tme…² no2tm…³ pm25d…⁴  year\n   <chr> <dbl>    <dbl> <date>     <dbl>     <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 chic   39       15.8 2002-03-23  12.1      39     26.8     17.9   -4.13  2002\n 2 chic   21       10.8 2002-02-27   9.2      26     18.3     23.7   -7.03  2002\n 3 chic   45       32.6 2000-03-14  26.1      45.5   14.0     25.3    9.87  2000\n 4 chic   76       66.2 1999-06-26  24.9      37.5   35.6     27.1    8.67  1999\n 5 chic   67       46.1 2001-05-09  10.7      59     36.9     29.3   -5.53  2001\n 6 chic   44       29.5 1992-04-28  NA        55     18.1     41.6   NA     1992\n 7 chic   67       59.9 2003-09-02  12.5      21     31.8     21.9   -3.73  2003\n 8 chic   30       21.8 2004-12-03  21        28      6.92    27.3    4.77  2004\n 9 chic   81.5     68.8 1991-09-15  NA        34.3   22.2     16.9   NA     1991\n10 chic   58       34.6 1988-10-10  NA        39     20.8     17.3   NA     1988\n# … with 1 more variable: pm25.quint <fct>, and abbreviated variable names\n#   ¹​pm10tmean2, ²​o3tmean2, ³​no2tmean2, ⁴​pm25detrend\n\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\nslice_head(chicago, n = 5)\n\n# A tibble: 5 × 11\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tme…¹ no2tm…² pm25d…³  year\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n1 chic     35     30.1 2005-12-31 15          23.5    2.53    13.2   -1.23  2005\n2 chic     36     31   2005-12-30 15.1        19.2    3.03    22.8   -1.17  2005\n3 chic     35     29.4 2005-12-29  7.45       23.5    6.79    20.0   -8.78  2005\n4 chic     37     34.5 2005-12-28 17.8        27.5    3.26    19.3    1.52  2005\n5 chic     40     33.6 2005-12-27 23.6        27      4.47    23.5    7.33  2005\n# … with 1 more variable: pm25.quint <fct>, and abbreviated variable names\n#   ¹​o3tmean2, ²​no2tmean2, ³​pm25detrend\n\n\nThis will show the last 5 rows.\n\nslice_tail(chicago, n = 5)\n\n# A tibble: 5 × 11\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tme…¹ no2tm…² pm25d…³  year\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n1 chic   32       28.9 1987-01-05    NA       NA      4.75    30.3      NA  1987\n2 chic   29       28.6 1987-01-04    NA       47      4.38    30.4      NA  1987\n3 chic   33       27.4 1987-01-03    NA       34.2    3.33    23.8      NA  1987\n4 chic   33       29.9 1987-01-02    NA       NA      3.30    23.2      NA  1987\n5 chic   31.5     31.5 1987-01-01    NA       34      4.25    20.0      NA  1987\n# … with 1 more variable: pm25.quint <fct>, and abbreviated variable names\n#   ¹​o3tmean2, ²​no2tmean2, ³​pm25detrend\n\n\n\n\n\nSummary\nThe dplyr pacfkage provides a concise set of operations for managing data frames. With these functions we can do a number of complex operations in just a few lines of code. In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().\nOnce you learn the dplyr grammar there are a few additional benefits\n\ndplyr can work with other data frame “back ends” such as SQL databases. There is an SQL interface for relational databases via the DBI package\ndplyr can be integrated with the data.table package for large fast tables\n\nThe dplyr package is handy way to both simplify and speed up your data frame management code. It is rare that you get such a combination at the same time!\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nHow can you tell if an object is a tibble?\nWhat option controls how many additional column names are printed at the footer of a tibble?\nUsing the trees dataset in base R (this dataset stores the girth, height, and volume for Black Cherry Trees) and using the pipe operator: (i) convert the data.frame to a tibble, (ii) filter for rows with a tree height of greater than 70, and (iii) order rows by Volume (smallest to largest).\n\n\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nhttps://r4ds.had.co.nz/tibbles.html\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio"
  },
  {
    "objectID": "posts/2022-09-01-literate-programming/index.html",
    "href": "posts/2022-09-01-literate-programming/index.html",
    "title": "Literate Statistical Programming",
    "section": "",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to define literate programming\nRecognize differences between available tools to for literate programming\nKnow how to efficiently work within RStudio for efficient literate programming\nCreate a R Markdown document\n\n\n\n\n\nIntroduction\nOne basic idea to make writing reproducible reports easier is what’s known as literate statistical programming (or sometimes called literate statistical practice). This comes from the idea of literate programming in the area of writing computer programs.\nThe idea is to think of a report or a publication as a stream of text and code.\n\nThe text is readable by people and the code is readable by computers.\nThe analysis is described in a series of text and code chunks.\nEach kind of code chunk will do something like load some data or compute some results.\nEach text chunk will relay something in a human readable language.\n\nThere might also be presentation code that formats tables and figures and there’s article text that explains what’s going on around all this code. This stream of text and code is a literate statistical program or a literate statistical analysis.\n\nWeaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways.\nLiterate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents”, or in other words, machine readable code.\nThe basic idea behind literate programming in order to generate the different kinds of output you might need, you only need a single source document—you can weave and tangle to get the rest.\nIn order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable).\n\n\nSweave\nOne of the original literate programming systems in R that was designed to do this was called Sweave. Sweave enables users to combine R code with a documentation program called LaTeX.\nSweave files ends a .Rnw and have R code weaved through the document:\n<<plot1, height=4, width=5, eval=FALSE>>=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file, Sweave will process the file, executing the R chunks and replacing them with output as appropriate before creating the PDF document.\nIt was originally developed by Fritz Leisch, who is a core member of R, and the code base is still maintained by R Core. The Sweave system comes with any installation of R.\nThere are many limitations to the original Sweave system.\n\nOne of the limitations is that it is focused primarily on LaTeX, which is not a documentation language that many people are familiar with.\nTherefore, it can be difficult to learn this type of markup language if you’re not already in a field that uses it regularly.\nSweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\n\nInstead, folks have moved towards using something called knitr, which offers everything Sweave does, plus it extends it further.\n\nWith Sweave, additional tools are required for advanced operations, whereas knitr supports more internally. We’ll discuss knitr below.\n\n\n\nrmarkdown\nAnother choice for literate programming is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md.. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\n\n\n\n\n\nTip\n\n\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\n\nhttps://bookdown.org/yihui/rmarkdown\n\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\n\nBecause this is lecture is built in a .qmd file (which is very similar to a .Rmd file), let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\n\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\n\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio).\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nmarkdown (R package). markdown is an R package which converts .md files into HTML. It is no longer recommended for use has been surpassed by rmarkdown (discussed below).\nR Markdown (markup language). R Markdown is an extension of the markdown syntax. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the R Markdown Quick Tour for more:\n\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\n\n\n\n\n\nArtwork by Allison Horst on RMarkdown\n\n\n\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr.\n\nThe knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them.\nknitr still uses R as its programming language, but it allows you to mix other programming languages in.\nYou can also use a variety of documentation languages now, such as LaTeX, markdown and HTML.\nknitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\n\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\n\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\n\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\n\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\n\n\n\nConverting a Rmd file to many outputs using knitr and pandoc\n\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]\n\n\n\nCreate and Knit Your First R Markdown Document\n\n\nWhen creating your first R Markdown document, in RStudio you can\n\nGo to File > New File > R Markdown…\nFeel free to edit the Title\nMake sure to select “Default Output Format” to be HTML\nClick “OK”. RStudio creates the R Markdown document and places some boilerplate text in there just so you can see how things are setup.\nClick the “Knit” button (or go to File > Knit Document) to make sure you can create the HTML output\n\nIf you successfully knit your first R Markdown document, then congratulations!\n\n\n\n\n\nMission accomplished!\n\n\n\n\n\n\nWebsites and Books in R Markdown\nNow that you are on the road to using R Markdown documents, it is important to know about other wonderful things you do with these documents. For example, let’s say you have multiple .Rmd documents that you want to put together into a website, blog, book, etc.\nThere are primarily two ways to build multiple .Rmd documents together:\n\nblogdown for building websites\nbookdown for authoring books\n\nIn this section, we briefly introduce both packages, but it’s worth mentioning that the rmarkdown package also has a built-in site generator to build websites.\n\nblogdown\n\n\n\n\n\nblogdown logo\n\n\n\n\n[Source]\nThe blogdown R package is built on top of R Markdown, supports multi-page HTML output to write a blog post or a general page in an Rmd document, or a plain Markdown document.\n\nThese source documents (e.g. .Rmd or .md) are built into a static website (i.e. a bunch of static HTML files, images and CSS files).\nUsing this folder of files, it is very easy to publish it to any web server as a website.\nAlso, it is easy to maintain because it is only a single folder.\n\n\n\n\n\n\n\nTip\n\n\n\nFor example, my personal website was built in blogdown:\n\nhttps://www.stephaniehicks.com\n\nOther really great examples can be found here:\n\nhttps://awesome-blogdown.com\n\n\n\nOther advantages include the content likely being reproducible, easier to maintain, and easy to convert pages to e.g. PDF or other formats in the future if you do not want to convert to HTML files.\nBecause it is based on the Markdown syntax, it is easy to write technical documents, including math equations, insert figures or tables with captions, cross-reference with figure or table numbers, add citations, and present theorems or proofs.\nHere’s a video you can watch of someone making a blogdown website.\n\n\n\n\n[Source on YouTube]\n\n\nbookdown\n\n\n\n\n\nbook logo\n\n\n\n\n[Source]\nSimilar to blogdown, the bookdown R package is built on top of R Markdown, but also offers features like multi-page HTML output, numbering and cross-referencing figures/tables/sections/equations, inserting parts/appendices, and imported the GitBook style (https://www.gitbook.com) to create elegant and appealing HTML book pages. Share\n\n\n\n\n\n\nTip\n\n\n\nFor example, the previous version of this course was built in bookdown:\n\nhttps://rdpeng.github.io/Biostat776\n\nAnother example is the Tidyverse Skills for Data Science book that the JHU Data Science Lab wrote. The github repo that contains all the .Rmd files can be found here.\n\nhttps://jhudatascience.org/tidyversecourse\nhttps://github.com/jhudsl/tidyversecourse\n\n\n\nNote: Even though the word “book” is in “bookdown”, this package is not only for books. It really can be anything that consists of multiple .Rmd documents meant to be read in a linear sequence such as course dissertation/thesis, handouts, study notes, a software manual, a thesis, or even a diary.\n\nhttps://bookdown.org/yihui/rmarkdown/basics-examples.html#examples-books\n\n\n\ndistill\nThere is another great way to build blogs or websites using the distill for R Markdown.\n\nhttps://rstudio.github.io/distill\n\nDistill for R Markdown combines the technical authoring features of the Distill web framework (optimized for scientific and technical communication) with R Markdown, enabling a fully reproducible workflow based on literate programming (Knuth 1984).\nDistill articles include:\n\nReader-friendly typography that adapts well to mobile devices.\nFeatures essential to technical writing like LaTeX math, citations, and footnotes.\nFlexible figure layout options (e.g. displaying figures at a larger width than the article text).\nAttractively rendered tables with optional support for pagination.\nSupport for a wide variety of diagramming tools for illustrating concepts. The ability to incorporate JavaScript and D3-based interactive visualizations.\nA variety of ways to publish articles, including support for publishing sets of articles as a Distill website or as a Distill blog.\n\nThe course website from last year was built in Distill for R Markdown:\n\nWebsite: https://stephaniehicks.com/jhustatcomputing2021\nGithub: https://github.com/stephaniehicks/jhustatcomputing2021\n\nSome other cool things about distill is the use of footnotes and asides.\nFor example 1. The number of the footnote will be automatically generated.\nYou can also optionally include notes in the gutter of the article (immediately to the right of the article text). To do this use the aside tag.\n\nThis content will appear in the gutter of the article.\n\nYou can also include figures in the gutter. Just enclose the code chunk which generates the figure in an aside tag\n\n\n\nTips and tricks in R Markdown in RStudio\nHere are shortcuts and tips on efficiently using RStudio to improve how you write code.\n\nRun code\nIf you want to run a code chunk:\ncommand + Enter on Mac\nCtrl + Enter on Windows\n\n\nInsert a comment in R and R Markdown\nTo insert a comment:\ncommand + Shift + C on Mac\nCtrl + Shift + C on Windows\nThis shortcut can be used both for:\n\nR code when you want to comment your code. It will add a # at the beginning of the line\nfor text in R Markdown. It will add <!-- and --> around the text\n\nNote that if you want to comment more than one line, select all the lines you want to comment then use the shortcut. If you want to uncomment a comment, apply the same shortcut.\n\n\nKnit a R Markdown document\nYou can knit R Markdown documents by using this shortcut:\ncommand + Shift + K on Mac\nCtrl + Shift + K on Windows\n\n\nCode snippets\nCode snippets is usually a few characters long and is used as a shortcut to insert a common piece of code. You simply type a few characters then press Tab and it will complete your code with a larger code. Tab is then used again to navigate through the code where customization is required. For instance, if you type fun then press Tab, it will auto-complete the code with the required code to create a function:\nname <- function(variables) {\n  \n}\nPressing Tab again will jump through the placeholders for you to edit it. So you can first edit the name of the function, then the variables and finally the code inside the function (try by yourself!).\nThere are many code snippets by default in RStudio. Here are the code snippets I use most often:\n\nlib to call library()\n\n\nlibrary(package)\n\n\nmat to create a matrix\n\n\nmatrix(data, nrow = rows, ncol = cols)\n\n\nif, el, and ei to create conditional expressions such as if() {}, else {} and else if () {}\n\n\nif (condition) {\n  \n}\n\nelse {\n  \n}\n\nelse if (condition) {\n  \n}\n\n\nfun to create a function\n\n\nname <- function(variables) {\n  \n}\n\n\nfor to create for loops\n\n\nfor (variable in vector) {\n  \n}\n\n\nts to insert a comment with the current date and time (useful if you have very long code and share it with others so they see when it has been edited)\n\n\n# Tue Jan 21 20:20:14 2020 ------------------------------\n\nYou can see all default code snippets and add yours by clicking on Tools > Global Options… > Code (left sidebar) > Edit Snippets…\n\n\nOrdered list in R Markdown\nIn R Markdown, when creating an ordered list such as this one:\n\nItem 1\nItem 2\nItem 3\n\nInstead of bothering with the numbers and typing\n1. Item 1\n2. Item 2\n3. Item 3\nyou can simply type\n1. Item 1\n1. Item 2\n1. Item 3\nfor the exact same result (try it yourself or check the code of this article!). This way you do not need to bother which number is next when creating a new item.\nTo go even further, any numeric will actually render the same result as long as the first item is the number you want to start from. For example, you could type:\n1. Item 1\n7. Item 2\n3. Item 3\nwhich renders\n\nItem 1\nItem 2\nItem 3\n\nHowever, I suggest always using the number you want to start from for all items because if you move one item at the top, the list will start with this new number. For instance, if we move 7. Item 2 from the previous list at the top, the list becomes:\n7. Item 2\n1. Item 1\n3. Item 3\nwhich incorrectly renders\n\nItem 2\nItem 1\nItem 3\n\n\n\nNew code chunk in R Markdown\nWhen editing R Markdown documents, you will need to insert a new R code chunk many times. The following shortcuts will make your life easier:\ncommand + option + I on Mac (or command + alt + I depending on your keyboard)\nCtrl + ALT + I on Windows\n\n\nReformat code\nA clear and readable code is always easier and faster to read (and look more professional when sharing it to collaborators). To automatically apply the most common coding guidelines such as white spaces, indents, etc., use:\ncmd + Shift + A on Mac\nCtrl + Shift + A on Windows\nSo for example the following code which does not respect the guidelines (and which is not easy to read):\n1+1\n  for(i in 1:10){if(!i%%2){next}\nprint(i)\n }\nbecomes much more neat and readable:\n1 + 1\nfor (i in 1:10) {\n  if (!i %% 2) {\n    next\n  }\n  print(i)\n}\n\n\nRStudio addins\nRStudio addins are extensions which provide a simple mechanism for executing advanced R functions from within RStudio. In simpler words, when executing an addin (by clicking a button in the Addins menu), the corresponding code is executed without you having to write the code. RStudio addins have the advantage that they allow you to execute complex and advanced code much more easily than if you would have to write it yourself.\n\n\n\n\n\n\nTip\n\n\n\nFor more information about RStudio addins, check out:\n\nhttps://rstudio.github.io/rstudioaddins\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown\n\n\n\n\n\nOthers\nSimilar to many other programs, you can also use:\n\ncommand + Shift + N on Mac and Ctrl + Shift + N on Windows to open a new R Script\ncommand + S on Mac and Ctrl + S on Windows to save your current script or R Markdown document\n\nCheck out Tools –> Keyboard Shortcuts Help to see a long list of these shortcuts.\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\nQuestions\n\nWhat is literate programming?\nWhat was the first literate statistical programming tool to weave together a statistical language (R) with a markup language (LaTeX)?\nWhat is knitr and how is different than other literate statistical programming tools?\nWhere can you find a list of other commands that help make your code writing more efficient in RStudio?\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nRMarkdown Tips and Tricks by Indrajeet Patil\nhttps://bookdown.org/yihui/rmarkdown\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\nFootnotes\n\n\nThis will become a hover-able footnote↩︎"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all to clone the repo and navigate within RStudio.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\n\nModule 1\n\nStatistical and computational tools for scientific and reproducible research\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nAug 30\n👋 Course introduction [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n👩‍💻 Introduction to R and RStudio [html] [Qmd]\n\n\n\n\n\n\n🐙 Introduction to git/GitHub [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nSept 1\n🔬 Reproducible Research [html] [Qmd]\n\n\n\n\n\n\n👓 Literate programming [html] [Qmd]\n\n\n\n\n\n\n🆒 Reference management [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nData analysis in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\nSept 6\n👀 Reading and writing data [html] [Qmd]\n🌴 Project 1\n\n\n\n\n\n✂️ Managing data frames with Tidyverse [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nSept 8\n😻 Tidy data and the Tidyverse\n🍂 Project 0 due\n\n\n\n\n\n🤝 Joining data in R: Basics\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData visualizations R\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\nSept 13\n📊 Plotting systems in R\n\n\n\n\n\n\n📊 The ggplot2 plotting system: qplot()\n\n\n\n\n\n\n\n\n\n\n\n\nSept 15\n📊 The ggplot2 plotting system: ggplot()\n🌴 Project 2\n\n\n\n\n\n\n\n\n\n\n\nSept 16\n\n🍂 Project 1 due\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nNuts and bolts of R\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\nSept 20\n🔩 Best practices for project-oriented workflows in R\n\n\n\n\n\n\n\n\n\n\n\n\nSept 22\n🔩 R Nuts and Bolts\n\n\n\n\n\n\n🔩 Control structures in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nSept 27\n🔩 Functions in R\n\n\n\n\n\n\n🔩 Loop functions\n\n\n\n\n\n\n\n\n\n\n\n\nSept 29\n🐛 Debugging code in R\n\n\n\n\n\n\n🐛 Error handling code in R\n\n\n\n\n\nSept 30\n\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nSpecial data types in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\nOct 4\n📆 Working with dates and times\n🌴 Project 3\n\n\n\n\n\n\n\n\n\n\n\nOct 6\n✨ Regular expressions\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\nOct 11\n🐱 Working with factors\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13\n📆 Working with text data and sentiment analysis\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nBest practices for working with data and other languages\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\nOct 18\n☁️ Best practices for storing data\n\n\n\n\n\n\n☁️ Best practices for ethical data analysis\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20\n🐍 Leveraging Python within R\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21\n\n🍂 Project 3 due"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "module 1\n\n\nweek 2\n\n\nR\n\n\nprogramming\n\n\nreadr\n\n\nhere\n\n\ntidyverse\n\n\n\n\nHow to get data in and out of R using relative paths\n\n\n\n\n\n\nSep 6, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nR\n\n\nprogramming\n\n\ndplyr\n\n\nhere\n\n\ntibble\n\n\ntidyverse\n\n\n\n\nAn introduction to data frames in R and the managing them with the dplyr R package\n\n\n\n\n\n\nSep 6, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR\n\n\nreproducibility\n\n\n\n\nIntroduction to reproducible research covering some basic concepts and ideas that are related to reproducible reporting\n\n\n\n\n\n\nSep 1, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR Markdown\n\n\nprogramming\n\n\n\n\nHow to use citations and incorporate references from a bibliography in R Markdown.\n\n\n\n\n\n\nSep 1, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR Markdown\n\n\nprogramming\n\n\n\n\nIntroduction to literate statistical programming tools including R Markdown\n\n\n\n\n\n\nSep 1, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR\n\n\nprogramming\n\n\nRStudio\n\n\n\n\nLet’s dig into the R programming language and the RStudio integrated developer environment\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncourse-admin\n\n\nmodule 1\n\n\nweek 1\n\n\n\n\nOverview course information for BSPH Biostatistics 140.776 in Fall 2022\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nprogramming\n\n\nversion control\n\n\ngit\n\n\nGitHub\n\n\n\n\nVersion control is a game changer; or how I learned to love git/GitHub\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\nNo matching items"
  }
]