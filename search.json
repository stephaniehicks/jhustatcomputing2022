[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Location: In person and Online for Fall 2022\nCourse time: Tuesdays and Thursdays from 1:30-2:50pm (Eastern Daylight Time zone)\nCourse location:\n140.776.01 is in person in W2008\n140.776.41 will have recorded lectures posted online after class.\nAssignments: Three projects\n\n\n\n\nTo add the course to your 1st term registration: You can sign up for either the in person (140.776.01) or online only (140.776.41) course.\nAll lectures will be recorded and posted on CoursePlus. You can watch all lecture material and complete the assignments asynchronously.\nPlease course instructor if interested in auditing.\n\n\n\n\n\nStephanie C. Hicks (https://www.stephaniehicks.com)\n\nOffice Location: E3545, Wolfe Street building\nEmail: shicks19@jhu.edu\n\n\nInstructor office hours are announced on CoursePlus. If there are conflicts and/or need to cancel office hours, announcements will be made on CoursePlus.\n\n\n\n\nPhyllis Wei (ywei43@jhu.edu)\nJoe Sartini (jsartin1@jhu.edu)\n\nTA office hours are announced on CoursePlus.\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nWe strongly encourage you to use CoursePlus to ask questions first, before joining office hours. The reason for this is so that other students in the class (who likely have similar questions) can also benefit from the questions and answers asked by your colleagues.\nYou are welcome to join office hours to get more group interactive feedback.\nIf you are not able to make the office hours, appointments can be made by email with either the instructor or the TAs."
  },
  {
    "objectID": "syllabus.html#important-links",
    "href": "syllabus.html#important-links",
    "title": "Syllabus",
    "section": "Important Links",
    "text": "Important Links\n\nCourse website: https://stephaniehicks.com/jhustatcomputing2022\nGitHub repository with all course material: https://github.com/stephaniehicks/jhustatcomputing2022"
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nUpon successfully completing this course, students will be able to:\n\nInstall and configure software necessary for a statistical programming environment\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite and debug code in base R and the tidyverse (and integrate code from Python modules)\nBuild basic data visualizations using R and the tidyverse\nDiscuss best practices for coding and reproducible research, basics of data ethics, basics of working with special data types, and basics of storing data"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nIn Fall 2022, we will have in person lectures that will be recorded enabling an entirely online section for the course too. Mandatory attendance is not required."
  },
  {
    "objectID": "syllabus.html#textbook-and-other-course-material",
    "href": "syllabus.html#textbook-and-other-course-material",
    "title": "Syllabus",
    "section": "Textbook and Other Course Material",
    "text": "Textbook and Other Course Material\nThere is no required textbook. We will make use of several freely available textbooks and other materials. All course materials will be provided. We will use the R software for data analysis, which is freely available for download."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus",
    "section": "Software",
    "text": "Software\nWe will make heavy use of R in this course, so you should have R installed. You can obtain R from the Comprehensive R Archive Network. There are versions available for Mac, Windows, and Unix/Linux. This software is required for this course.\nIt is important that you have the latest version of R installed. For this course we will be using R version 4.2.1. You can determine what version of R you have by starting up R and typing into the console R.version.string and hitting the return/enter key. If you do not have the proper version of R installed, go to CRAN and download and install the latest version.\nWe will also make use of the RStudio interactive development environment (IDE). RStudio requires that R be installed, and so is an “add-on” to R. You can obtain the RStudio Desktop for free from the RStudio web site. In particular, we will make heavy use of it when developing R packages. It is also essential that you have the latest release of RStudio. You can determine the version of RStudio by looking at menu item Help > About RStudio. You should be using RStudio version 1.4.1106 or higher."
  },
  {
    "objectID": "syllabus.html#projects",
    "href": "syllabus.html#projects",
    "title": "Syllabus",
    "section": "Projects",
    "text": "Projects\nThere will be 4 assignments, due every 2–3 weeks. Projects will be submitted electronically via the Drop Box on the CoursePlus web site (unless otherwise specified).\nThe project assignments will be due on\n\nProject 0: September 8, 1:29pm (entirely optional and not graded but hopefully useful and fun)\nProject 1: September 16, 11:59pm\nProject 2: September 30, 11:59pm\nProject 3: October 21, 11:59pm\n\n\nProject collaboration\nPlease feel free to study together and talk to one another about project assignments. The mutual instruction that students give each other is among the most valuable that can be achieved.\nHowever, it is expected that project assignments will be implemented and written up independently unless otherwise specified. Specifically, please do not share analytic code or output. Please do not collaborate on write-up and interpretation. Please do not access or use solutions from any source before your project assignment is submitted for grading."
  },
  {
    "objectID": "syllabus.html#discussion-forum",
    "href": "syllabus.html#discussion-forum",
    "title": "Syllabus",
    "section": "Discussion Forum",
    "text": "Discussion Forum\nThe course will make use of the CoursePlus Discussion Forum in order to ask and answer questions regarding any of the course materials. The Instructor and the Teaching Assistants will monitor the discussion boards and answer questions when appropriate."
  },
  {
    "objectID": "syllabus.html#exams",
    "href": "syllabus.html#exams",
    "title": "Syllabus",
    "section": "Exams",
    "text": "Exams\nThere are no exams in this course."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nGrades in the course will be based on Projects 0–3 with a percentage of the final grade being apportioned to each assignment. Each of Projects 1–3 counts approximately equally in the final grade. Grades for the projects and the final grade will be issued via the CoursePlus grade book."
  },
  {
    "objectID": "syllabus.html#policy-for-submitted-projects-late",
    "href": "syllabus.html#policy-for-submitted-projects-late",
    "title": "Syllabus",
    "section": "Policy for submitted projects late",
    "text": "Policy for submitted projects late\n\nProject 1\nFor submitted solutions that had prior approval to have a two day extension and submitted within the two days, these projects will be graded with no late day penalty.\nFor submitted solutions without prior approval to have an extension, these projects will be graded, but have a late day penalty of 5% off the 100% starting point for each day (24 hours from the original date and time the assignment was due) that it is past due from the original due date.\n\nFor example, we will deduct 5% for the assignment that is <24 hours late, 10% points for the assignment that is 24-48 hours late, and 15% points for the assignment that is 48-72 hours late.\nIf you had approval from the instructor to submit two days late, but submitted the assignment after the extension due date and time, the penalty will apply starting with the extension date if submitted late. For example, we will deduct 5% for the assignment that is <24 hours late from the extension due date, etc.\n\n\n\nProject 2 and Project 3\nStarting with Project 2 and 3, the instructor and TAs will not accept any more email late day policy requests.\nA new policy will be implemented for late submissions. This is the new policy:\n\nEach student will be given two free “late days” for the rest of the course.\nA late day extends the individual project deadline by 24 hours without penalty.\nThe late days can be applied to just one project (e.g. two late days for Project 2), or they can be split across the two projects (one late day for Project 2 and one late day for Project 3). This is entirely left up to the discretion of the student.\nLate days are intended to give you flexibility: you can use them for any reason no questions asked.\nYou do not get any bonus points for not using your late days.\n\nAlthough the each student is only given a total of two late days, we will be accepting homework from students that pass this limit.\n\nWe will be deducting 5% for each extra late day. For example, if you have already used all of your late days for the term, we will deduct 5% for the assignment that is <24 hours late, 10% points for the assignment that is 24-48 hours late, and 15% points for the assignment that is 48-72 hours late.\nWe will not grade assignments that are more than 3 days past the original due date.\n\n\n\nRegrading Policy\nIt is very important to us that all assignments are properly graded. If you believe there is an error in your assignment grading, please send an email to one of the instructors within 7 days of receiving the grade. No re-grade requests will be accepted orally, and no regrade requests will be accepted more than 7 days after you receive the grade for the assignment."
  },
  {
    "objectID": "syllabus.html#academic-ethics-and-student-conduct-code",
    "href": "syllabus.html#academic-ethics-and-student-conduct-code",
    "title": "Syllabus",
    "section": "Academic Ethics and Student Conduct Code",
    "text": "Academic Ethics and Student Conduct Code\nStudents enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations.\nStudents should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal.\nThe faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions."
  },
  {
    "objectID": "syllabus.html#disability-support-service",
    "href": "syllabus.html#disability-support-service",
    "title": "Syllabus",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with Student Disability Service (SDS). It is the responsibility of the student to register for accommodations with SDS. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. Once you are f a student in your class has approved accommodations you will receive formal notification and the student will be encouraged to reach out. If you have questions about requesting accommodations, please contact BSPH.dss@jhu.edu."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis is a quantitative course. We will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking are useful. Previous experience with writing computer programs in general and R in particular is also helpful, but not necessary. If you have no programming experience, expect to spend extra time getting yourself familiar with R. As long as you are willing to invest the time to learn the programming and you do not mind thinking quantitatively, you should be able to take the course, independent of your background.\nFormal requirement for the course is Biostatistics 140.621. Knowledge of material from 140.621 is assumed. If you didn’t take this course, please contact me to get permission to enroll.\n\nGetting set up\nYou must install R and RStudio on your computer in order to complete this course. These are two different applications that must be installed separately before they can be used together:\n\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\n\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\n\ninstall.packages(\"tidyverse\")\n\nin the R console.\n\nHow to Download R for Windows\nGo to https://cran.r-project.org and\n\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.2.1 for Windows”\n\n\n\n\n\n\n\nWarning\n\n\n\nThe version in the video is not the latest version. Please download the latest version.\n\n\n\n\n\nVideo Demo for Downloading R for Windows\n\n\n\n\nHow to Download R for the Mac\nGoto https://cran.r-project.org and\n\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.2.1.pkg”\n\n\n\n\n\n\n\nWarning\n\n\n\nThe version in the video is not the latest version. Please download the latest version.\n\n\n\n\n\nVideo Demo for Downloading R for the Mac\n\n\n\n\nHow to Download RStudio\nGoto https://rstudio.com and\n\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe video shows how to download RStudio for the Mac but you should download RStudio for whatever computing setup you have\n\n\n\n\n\nVideo Demo for Downloading RStudio"
  },
  {
    "objectID": "syllabus.html#general-disclaimers",
    "href": "syllabus.html#general-disclaimers",
    "title": "Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\n\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Statistical Computing!",
    "section": "",
    "text": "Welcome to Statistical Computing at Johns Hopkins Bloomberg School of Public Health!"
  },
  {
    "objectID": "index.html#what-is-this-course",
    "href": "index.html#what-is-this-course",
    "title": "Welcome to Statistical Computing!",
    "section": "What is this course?",
    "text": "What is this course?\nThis course covers the basics of practical issues in programming and other computer skills required for the research and application of statistical methods. Includes programming in R and the tidyverse, data ethics, best practices for coding and reproducible research, introduction to data visualizations, best practices for working with special data types (dates/times, text data, etc), best practices for storing data, basics of debugging, organizing and commenting code, basics of leveraging Python from R. Topics in statistical data analysis provide working examples."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Statistical Computing!",
    "section": "Getting started",
    "text": "Getting started\nI suggest that you start by looking over the Syllabus and Schedule under General Information. After that, start with the Lectures content in the given order."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to Statistical Computing!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Stephanie Hicks.\nThe following individuals have contributed to improving the course or materials have been adapted from their courses: Roger D. Peng, Andreas Handel, Naim Rashid, Michael Love.\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Big Book of R: https://www.bigbookofr.com\nList of resources to learn R (but also Python, SQL, Javascript): https://github.com/delabj/datacamp_alternatives/blob/master/index.md\nlearnr4free. Resources (books, videos, interactive websites, papers) to learn R. Some of the resources are beginner-friendly and start with the installation process: https://www.learnr4free.com/en\nData Science with R by Danielle Navarro: https://robust-tools.djnavarro.net"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "project 2\n\n\nprojects\n\n\n\n\nExploring temperature and rainfall in Australia\n\n\n\n\n\n\nSep 15, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nproject 1\n\n\nprojects\n\n\n\n\nFinding great chocolate bars!\n\n\n\n\n\n\nSep 6, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nproject 0\n\n\nprojects\n\n\n\n\nInformation for Project 0 (entirely optional, but hopefully useful and fun!)\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2022-08-30-project-0/index.html",
    "href": "projects/2022-08-30-project-0/index.html",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "Part 1\nThis part of the project is to ensure that you have successfully set up your computing environment. Please email (use the Subject line: 140.776 Setup) the Course Instructor (Dr. Stephanie Hicks) at shicks19@jhu.edu the following information:\n\nSetting up your computing environment\n\nYour name, JHED ID (if applicable).\nThe type of computer/operating system you are using (Windows, Mac, Unix/Linux, other)\nThe version of R that you have installed on your computer. To do this, start up R and run the following in the R console and include the output in your email.\n\n\nprint(R.version.string)\n\n\n\n\nPrinting the R version string\n\n\n\nThe version of RStudio that you have installed on your computer. To do this start up RStudio and in the R console window, run the following and again include the output in your email:\n\n\nprint(RStudio.Version()$version)\n\n\nIf you have a GitHub username, please include this in your email. If you do not have a GitHub username, read https://happygitwithr.com, sign up for GitHub, and include your new username in your email to me.\nTo make sure git is installed on your system, use the ‘Terminal’ (e.g. it’s next to the R Console within RStudio) (or whatever you use), run the following and include the output in your email:\n\nFor example, this is mine:\n\ngit --version\n\ngit version 2.32.1 (Apple Git-133)\n\n\nIf you have any trouble with any of the steps above, try to first post on the discussion board on CoursePlus. The TAs and I will be checking it frequently, but other students may also be helpful in their replies. You can also use other resources to get help (Google, R4DS, colleagues/friends/relatives with R/Markdown experience, etc.). Still, try to do as much as possible yourself. We will use all the bits you are learning here repeatedly during this course.\n\n\n\nPart 2\nThis part of the project is to help you introduce yourself (and your interests!) to others in this course. You will create a new GitHub repository and build a small website about yourself.\n\n1. Create a GitHub repo for your website\nCreate a new GitHub repository titled biostat776-intro-<firstname>-<lastname> (where you replace <firstname> with your first name and <lastname> with your last name) in your own personal GitHub account (e.g. https://github.com/<yourgithubusername>/biostat776-intro-<firstname>-<lastname>).\nFor example, you can find an example that I created for myself at\n\ngithub repo: https://github.com/stephaniehicks/biostat776-intro-stephanie-hicks\n\n\n\n2. Build a website using R Markdown\nUsing one of the many ways we discussed in class (e.g. a simple R Markdown website, blogdown, distill, etc), create a new project in RStudio with the appropriate files. For example, you might include the following information:\n\nWrite a short summary introducing yourself. Structure the webpage with headings, subheadings, etc. Talk a bit about yourself, your background, training, research interests. Let me/us know what kind of statistics, programming, data analysis experience you already have. I am also curious to know what you most hope to learn in this course.\nFive fun facts about yourself\nA web page linking to something you think is really cool/interesting/inspiring/etc. You could also describe briefly what it is and why you like it.\n\nIf you want, feel free to get creative and include other things. You can play with RMarkdown if you wish to, e.g., you can try to include some table or a video, etc.\n\n\n3. Include a README.md file\nYour project repository should include a README.md file (if it was not included already).\nEdit the repository README.md file. Typically it will only contain the name of your repository with a # sign in front. The # represents a level 1 heading in Markdown. Change the headline and call it “Introducing myself” (or something like that). Underneath write something like “This website contains a short introduction of Your Name.”\nMake sure the 2 files (README.md and especially index.Rmd / index.html) look the way you want. Make changes until everything works.\n\n\n4. Deploy your website\nDepending on how you want to deploy your website, the following may or may not be relevant to you. In general, you want to make sure you have initialized your project to use git (i.e. you can type git init to initialize the repository to use git. Add and commit your changes. Push your changes and deploy your website.\nFollowing steps 2-4, here is my example website:\n\nwebsite: https://www.stephaniehicks.com/biostat776-intro-stephanie-hicks\n\n\n\n5. Share your website\n\nGo to the Discussion Board in CoursePlus and write a short post with a link (URL) to your website (and URL to the corresponding GitHub repository) that you created.\nAs you read the introductions from other folks in the class, feel free to comment/reply using Discussion board.\n\nIn class on Sept 8, I will show as many websites as I can from Courseplus!"
  },
  {
    "objectID": "projects/2022-09-06-project-1/index.html",
    "href": "projects/2022-09-06-project-1/index.html",
    "title": "Project 1",
    "section": "",
    "text": "Part 1: Explore data\nIn this part, use functions from dplyr and ggplot2 to answer the following questions.\n\nMake a histogram of the rating scores to visualize the overall distribution of scores. Change the number of bins from the default to 10, 15, 20, and 25. Pick on the one that you think looks the best. Explain what the difference is when you change the number of bins and explain why you picked the one you did.\n\n\n# Add your solution here and describe your answer afterwards\n\nThe ratings are discrete values making the histogram look strange. When you make the bin size smaller, it aggregates the ratings together in larger groups removing that effect. I picked 15, but there really is no wrong answer. Just looking for an answer here.\n\nConsider the countries where the beans originated from. How many reviews come from each country of bean origin?\n\n\n# Add your solution here\n\n\nWhat is average rating scores from reviews of chocolate bars that have Ecuador as country_of_bean_origin in this dataset? For this same set of reviews, also calculate (1) the total number of reviews and (2) the standard deviation of the rating scores. Your answer should be a new data frame with these three summary statistics in three columns. Label the name of these columns mean, sd, and total.\n\n\n# Add your solution here\n\n\nWhich country makes the best chocolate (or has the highest ratings on average) with beans from Ecuador?\n\n\n# Add your solution here\n\n\nCalculate the average rating across all country of origins for beans. Which top 3 countries have the highest ratings on average?\n\n\n# Add your solution here\n\n\nFollowing up on the previous problem, now remove any countries of bean origins that have less than 10 chocolate bar reviews. Now, which top 3 countries have the highest ratings on average?\n\n\n# Add your solution here\n\n\nFor this last part, let’s explore the relationship between percent chocolate and ratings.\n\nUse the functions in dplyr, tidyr, and lubridate to perform the following steps to the chocolate dataset:\n\nIdentify the countries of bean origin with at least 50 reviews. Remove reviews from countries are not in this list.\nUsing the variable describing the chocolate percentage for each review, create a new column that groups chocolate percentages into one of four groups: (i) <60%, (ii) >=60 to <70%, (iii) >=70 to <90%, and (iii) >=90% (Hint check out the substr() function in base R and the case_when() function from dplyr – see example below).\nUsing the new column described in #2, re-order the factor levels (if needed) to be starting with the smallest percentage group and increasing to the largest percentage group (Hint check out the fct_relevel() function from forcats).\nFor each country, make a set of four side-by-side boxplots plotting the groups on the x-axis and the ratings on the y-axis. These plots should be faceted by country.\n\nOn average, which category of chocolate percentage is most highly rated? Do these countries mostly agree or are there disagreements?\nHint: You may find the case_when() function useful in this part, which can be used to map values from one variable to different values in a new variable (when used in a mutate() call).\n\n## Generate some random numbers\ndat <- tibble(x = rnorm(100))\nslice(dat, 1:3)\n\n# A tibble: 3 × 1\n       x\n   <dbl>\n1  0.598\n2  0.450\n3 -0.524\n\n## Create a new column that indicates whether the value of 'x' is positive or negative\ndat %>%\n        mutate(is_positive = case_when(\n                x >= 0 ~ \"Yes\",\n                x < 0 ~ \"No\"\n        ))\n\n# A tibble: 100 × 2\n         x is_positive\n     <dbl> <chr>      \n 1  0.598  Yes        \n 2  0.450  Yes        \n 3 -0.524  No         \n 4  0.900  Yes        \n 5 -1.83   No         \n 6 -1.04   No         \n 7  0.199  Yes        \n 8  1.03   Yes        \n 9 -0.354  No         \n10  0.0216 Yes        \n# … with 90 more rows\n\n\n\n# Add your solution here\n\n\n\nPart 2: Join two datasets together\nThe goal of this part of the assignment is to join two datasets together. gapminder is a R package that contains an excerpt from the Gapminder data.\n\nTasks\n\nUse this dataset it to create a new column called continent in our chocolate dataset that contains the continent name for each review where the country of bean origin is.\nOnly keep reviews that have reviews from countries of bean origin with at least 10 reviews.\nAlso, remove the country of bean origin named \"Blend\".\nMake a set of violin plots with ratings on the y-axis and continents on the x-axis.\n\nHint:\n\nCheck to see if there are any NAs in the new column. If there are any NAs, add the continent name for each row.\n\n\n# Add your solution here\n\n\n\n\nPart 3: Convert wide data into long data\nThe goal of this part of the assignment is to take a dataset that is either messy or simply not tidy and to make them tidy datasets. The objective is to gain some familiarity with the functions in the dplyr, tidyr packages. You may find it helpful to review the section on spreading and gathering data.\n\nTasks\nWe are going to create a set of features for us to plot over time. Use the functions in dplyr and tidyr to perform the following steps to the chocolate dataset:\n\nCreate a new set of columns titled beans, sugar, cocoa_butter, vanilla, letchin, and salt that contain a 1 or 0 representing whether or not that review for the chocolate bar contained that ingredient (1) or not (0).\nCreate a new set of columns titled char_cocoa, char_sweet, char_nutty, char_creamy, char_roasty, char_earthy that contain a 1 or 0 representing whether or not that the most memorable characteristic for the chocolate bar had that word (1) or not (0). For example, if the word “sweet” appears in the most_memorable_characteristics, then record a 1, otherwise a 0 for that review in the char_sweet column (Hint: check out str_detect() from the stringr package).\nFor each year (i.e. review_date), calculate the mean value in each new column you created across all reviews for that year. (Hint: If all has gone well thus far, you should have a dataset with 16 rows and 13 columns).\nConvert this wide dataset into a long dataset with a new feature and mean_score column.\n\nIt should look something like this:\nreview_date     feature   mean_score\n<dbl>           <chr>     <dbl>\n2006    beans   0.967741935     \n2006    sugar   0.967741935     \n2006    cocoa_butter    0.903225806     \n2006    vanilla 0.693548387     \n2006    letchin 0.693548387     \n2006    salt    0.000000000     \n2006    char_cocoa  0.209677419     \n2006    char_sweet  0.161290323     \n2006    char_nutty  0.032258065     \n2006    char_creamy 0.241935484 \n\n\nNotes\n\nYou may need to use functions outside these packages to obtain this result.\nDo not worry about the ordering of the rows or columns. Depending on whether you use gather() or pivot_longer(), the order of your output may differ from what is printed above. As long as the result is a tidy data set, that is sufficient.\n\n\n# Add your solution here\n\n\n\n\nPart 4: Data visualization\nIn this part of the project, we will continue to work with our now tidy song dataset from the previous part.\n\nTasks\nUse the functions in ggplot2 package to make a scatter plot of the mean_scores (y-axis) over time (x-axis). One plot for each mean_score. For full credit, your plot should include:\n\nAn overall title for the plot and a subtitle summarizing key trends that you found. Also include a caption in the figure with your name.\nBoth the observed points for the mean_score, but also a smoothed non-linear pattern of the trend\nAll plots should be shown in the one figure\nThere should be an informative x-axis and y-axis label\n\nConsider playing around with the theme() function to make the figure shine, including playing with background colors, font, etc.\n\n\nNotes\n\nYou may need to use functions outside these packages to obtain this result.\nDon’t worry about the ordering of the rows or columns. Depending on whether you use gather() or pivot_longer(), the order of your output may differ from what is printed above. As long as the result is a tidy data set, that is sufficient.\n\n\n# Add your solution here\n\n\n\n\nPart 5: Make the worst plot you can!\nThis sounds a bit crazy I know, but I want this to try and be FUN! Instead of trying to make a “good” plot, I want you to explore your creative side and make a really awful data visualization in every way. :)\n\nTasks\nUsing the chocolate dataset (or any of the modified versions you made throughout this assignment or anything else you wish you build upon it):\n\nMake the absolute worst plot that you can. You need to customize it in at least 7 ways to make it awful.\nIn your document, write 1 - 2 sentences about each different customization you added (using bullets – i.e. there should be at least 7 bullet points each with 1-2 sentences), and how it could be useful for you when you want to make an awesome data visualization.\n\n\n# Add your solution here\n\n\n\n\nPart 6: Make my plot a better plot!\nThe goal is to take my sad looking plot and make it better! If you’d like an example, here is a tweet I came across of someone who gave a talk about how to zhoosh up your ggplots.\n\nchocolate %>%\n  ggplot(aes(x = as.factor(review_date), \n             y = rating, \n             fill = review_date)) +\n  geom_violin()\n\n\n\n\n\nTasks\n\nYou need to customize it in at least 7 ways to make it better.\nIn your document, write 1 - 2 sentences about each different customization you added (using bullets – i.e. there should be at least 7 bullet points each with 1-2 sentences), describing how you improved it.\n\n\n# Add your solution here"
  },
  {
    "objectID": "projects/2022-09-15-project-2/index.html",
    "href": "projects/2022-09-15-project-2/index.html",
    "title": "Project 2",
    "section": "",
    "text": "Part 1: Fun with functions\nIn this part, we are going to practice creating functions.\n\nPart 1A: Exponential transformation\nThe exponential of a number can be written as an infinite series expansion of the form\n\\[\n\\exp(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\n\\]\nOf course, we cannot compute an infinite series by the end of this term and so we must truncate it at a certain point in the series. The truncated sum of terms represents an approximation to the true exponential, but the approximation may be usable.\nWrite a function that computes the exponential of a number using the truncated series expansion. The function should take two arguments:\n\nx: the number to be exponentiated\nk: the number of terms to be used in the series expansion beyond the constant 1. The value of k is always \\(\\geq 1\\).\n\nFor example, if \\(k = 1\\), then the Exp function should return the number \\(1 + x\\). If \\(k = 2\\), then you should return the number \\(1 + x + x^2/2!\\).\nInclude at least one example of output using your function.\n\n\n\n\n\n\nNote\n\n\n\n\nYou can assume that the input value x will always be a single number.\nYou can assume that the value k will always be an integer \\(\\geq 1\\).\nDo not use the exp() function in R.\nThe factorial() function can be used to compute factorials.\n\n\n\n\nExp <- function(x, k) {\n        # Add your solution here\n}\n\n\n\nPart 1B: Sample mean and sample standard deviation\nNext, write two functions called sample_mean() and sample_sd() that takes as input a vector of data of length \\(N\\) and calculates the sample average and sample standard deviation for the set of \\(N\\) observations.\n\\[\n\\bar{x} = \\frac{1}{N} \\sum_{i=1}^n x_i\n\\]\n\\[\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\]\nInclude at least one example of output using your functions.\n\n\n\n\n\n\nNote\n\n\n\n\nYou can assume that the input value x will always be a vector of numbers of length N.\nDo not use the mean() and sd() functions in R.\n\n\n\n\nsample_mean <- function(x) {\n        # Add your solution here\n}\n\nsample_sd <- function(x) {\n        # Add your solution here\n}\n\n\n\nPart 1C: Confidence intervals\nNext, write a function called calculate_CI() that:\n\nThere should be two inputs to the calculate_CI(). First, it should take as input a vector of data of length \\(N\\). Second, the function should also have a conf (\\(=1-\\alpha\\)) argument that allows the confidence interval to be adapted for different \\(\\alpha\\).\nCalculates a confidence interval (CI) (e.g. a 95% CI) for the estimate of the mean in the population. If you are not familiar with confidence intervals, it is an interval that contains the population parameter with probability \\(1-\\alpha\\) taking on this form\n\n\\[\n\\bar{x} \\pm t_{\\alpha/2, N-1} s_{\\bar{x}}\n\\]\nwhere \\(t_{\\alpha/2, N-1}\\) is the value needed to generate an area of \\(\\alpha / 2\\) in each tail of the \\(t\\)-distribution with \\(N-1\\) degrees of freedom and \\(s_{\\bar{x}} = \\frac{s}{\\sqrt{N}}\\) is the standard error of the mean. For example, if we pick a 95% confidence interval and \\(N\\)=50, then you can calculate \\(t_{\\alpha/2, N-1}\\) as\n\nalpha <- 1 - 0.95\ndegrees_freedom = 50 - 1\nt_score = qt(p=alpha/2, df=degrees_freedom, lower.tail=FALSE)\n\n\nReturns a named vector of length 2, where the first value is the lower_bound, the second value is the upper_bound.\n\n\ncalculate_CI <- function(x, conf = 0.95) {\n        # Add your solution here\n}\n\nInclude example of output from your function showing the output when using two different levels of conf.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to check if your function output matches an existing function in R, consider a vector \\(x\\) of length \\(N\\) and see if the following two code chunks match.\n\ncalculate_CI(x, conf = 0.95)\n\n\ndat = data.frame(x=x)\nfit <- lm(x ~ 1, dat)\n\n# Calculate a 95% confidence interval\nconfint(fit, level=0.95)\n\n\n\n\n\n\nPart 2: Wrangling data\nIn this part, we will practice our wrangling skills with the tidyverse that we learned about in module 1.\n\nData\nThe two datasets for this part of the assignment comes from TidyTuesday. Specifically, we will use the following data from January 2020, which I have provided for you below:\n\ntuesdata <- tidytuesdayR::tt_load('2020-01-07')\nrainfall <- tuesdata$rainfall\ntemperature <- tuesdata$temperature\n\nHowever, to avoid re-downloading data, we will check to see if those files already exist using an if() statement:\n\nlibrary(here)\nif(!file.exists(here(\"data\",\"tuesdata_rainfall.RDS\"))){\n  tuesdata <- tidytuesdayR::tt_load('2020-01-07')\n  rainfall <- tuesdata$rainfall\n  temperature <- tuesdata$temperature\n  \n  # save the files to RDS objects\n  saveRDS(tuesdata$rainfall, file = here(\"data\",\"tuesdata_rainfall.RDS\"))\n  saveRDS(tuesdata$temperature, file = here(\"data\",\"tuesdata_temperature.RDS\"))\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code will only run if it cannot find the path to the tuesdata_rainfall.RDS on your computer. Then, we can just read in these files every time we knit the R Markdown, instead of re-downloading them every time.\n\n\nLet’s load the datasets\n\nrainfall <- readRDS(here(\"data\",\"tuesdata_rainfall.RDS\"))\ntemperature <- readRDS(here(\"data\",\"tuesdata_temperature.RDS\"))\n\nNow we can look at the data with glimpse()\n\nlibrary(tidyverse)\n\nglimpse(rainfall)\n\nRows: 179,273\nColumns: 11\n$ station_code <chr> \"009151\", \"009151\", \"009151\", \"009151\", \"009151\", \"009151…\n$ city_name    <chr> \"Perth\", \"Perth\", \"Perth\", \"Perth\", \"Perth\", \"Perth\", \"Pe…\n$ year         <dbl> 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 196…\n$ month        <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01…\n$ day          <chr> \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10…\n$ rainfall     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ period       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ quality      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ lat          <dbl> -31.96, -31.96, -31.96, -31.96, -31.96, -31.96, -31.96, -…\n$ long         <dbl> 115.79, 115.79, 115.79, 115.79, 115.79, 115.79, 115.79, 1…\n$ station_name <chr> \"Subiaco Wastewater Treatment Plant\", \"Subiaco Wastewater…\n\nglimpse(temperature)\n\nRows: 528,278\nColumns: 5\n$ city_name   <chr> \"PERTH\", \"PERTH\", \"PERTH\", \"PERTH\", \"PERTH\", \"PERTH\", \"PER…\n$ date        <date> 1910-01-01, 1910-01-02, 1910-01-03, 1910-01-04, 1910-01-0…\n$ temperature <dbl> 26.7, 27.0, 27.5, 24.0, 24.8, 24.4, 25.3, 28.0, 32.6, 35.9…\n$ temp_type   <chr> \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"m…\n$ site_name   <chr> \"PERTH AIRPORT\", \"PERTH AIRPORT\", \"PERTH AIRPORT\", \"PERTH …\n\n\nIf we look at the TidyTuesday github repo from 2020, we see this dataset contains temperature and rainfall data from Australia.\n\n[Source: Geoscience Australia]\nHere is a data dictionary for what all the column names mean:\n\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-07/readme.md#data-dictionary\n\n\n\nTasks\nUsing the rainfall and temperature data, perform the following steps and create a new data frame called df:\n\nStart with rainfall dataset and drop any rows with NAs.\nCreate a new column titled date that combines the columns year, month, day into one column separated by “-”. (e.g. “2020-01-01”). This column should not be a character, but should be recognized as a date. (Hint: check out the ymd() function in lubridate R package). You will also want to add a column that just keeps the year.\nUsing the city_name column, convert the city names (character strings) to all upper case.\nJoin this wrangled rainfall dataset with the temperature dataset such that it includes only observations that are in both data frames. (Hint: there are two keys that you will need to join the two datasets together). (Hint: If all has gone well thus far, you should have a dataset with 83,964 rows and 13 columns).\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou may need to use functions outside these packages to obtain this result, in particular you may find the functions drop_na() from tidyr and str_to_upper() function from stringr useful.\n\n\n\n\n# Add your solution here\n\n\n\n\nPart 3: Data visualization\nIn this part, we will practice our ggplot2 plotting skills within the tidyverse starting with our wrangled df data from Part 2. For full credit in this part (and for all plots that you make), your plots should include:\n\nAn overall title for the plot and a subtitle summarizing key trends that you found. Also include a caption in the figure.\nThere should be an informative x-axis and y-axis label.\n\nConsider playing around with the theme() function to make the figure shine, including playing with background colors, font, etc.\n\nPart 3A: Plotting temperature data over time\nUse the functions in ggplot2 package to make a line plot of the max and min temperature (y-axis) over time (x-axis) for each city in our wrangled data from Part 2. You should only consider years 2014 and onwards. For full credit, your plot should include:\n\nFor a given city, the min and max temperature should both appear on the plot, but they should be two different colors.\nUse a facet function to facet by city_name to show all cities in one figure.\n\n\n# Add your solution here\n\n\n\nPart 3B: Plotting rainfall over time\nHere we want to explore the distribution of rainfall (log scale) with histograms for a given city (indicated by the city_name column) for a given year (indicated by the year column) so we can make some exploratory plots of the data.\n\n\n\n\n\n\nNote\n\n\n\nYou are again using the wrangled data from Part 2.\n\n\nThe following code plots the data from one city (city_name == \"PERTH\") in a given year (year == 2000).\n\ndf %>% \n  filter(city_name == \"PERTH\", year == 2000) %>% \n  ggplot(aes(log(rainfall))) + \n    geom_histogram()\n\nWhile this code is useful, it only provides us information on one city in one year. We could cut and paste this code to look at other cities/years, but that can be error prone and just plain messy.\nThe aim here is to design and implement a function that can be re-used to visualize all of the data in this dataset.\n\nThere are 2 aspects that may vary in the dataset: The city_name and the year. Note that not all combinations of city_name and year have measurements.\nYour function should take as input two arguments city_name and year.\nGiven the input from the user, your function should return a single histogram for that input. Furthermore, the data should be readable on that plot so that it is in fact useful. It should be possible visualize the entire dataset with your function (through repeated calls to your function).\nIf the user enters an input that does not exist in the dataset, your function should catch that and report an error (via the stop() function).\n\nFor this section,\n\nWrite a short description of how you chose to design your function and why.\nPresent the code for your function in the R markdown document.\nInclude at least one example of output from your function.\n\n\n# Add your solution here\n\n\n\n\nPart 4: Apply functions and plot\n\nPart 4A: Tasks\nIn this part, we will apply the functions we wrote in Part 1 to our rainfall data starting with our wrangled df data from Part 2.\n\nFirst, filter for only years including 2014 and onwards.\nFor a given city and for a given year, calculate the sample mean (using your function sample_mean()), the sample standard deviation (using your function sample_sd()), and a 95% confidence interval for the average rainfall (using your function calculate_CI()). Specifically, you should add two columns in this summarized dataset: a column titled lower_bound and a column titled upper_bound containing the lower and upper bounds for you CI that you calculated (using your function calculate_CI()).\nCall this summarized dataset rain_df.\n\n\n# Add your solution here\n\n\n\nPart 4B: Tasks\nUsing the rain_df, plots the estimates of mean rainfall and the 95% confidence intervals on the same plot. There should be a separate faceted plot for each city. Think about using ggplot() with both geom_point() (and geom_line() to connect the points) for the means and geom_errorbar() for the lower and upper bounds of the confidence interval.\n\n# Add your solution here"
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” —John Tukey"
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#the-basics-qplot",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#the-basics-qplot",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "The Basics: qplot()",
    "text": "The Basics: qplot()\nThe qplot() function in ggplot2 is meant to get you going quickly.\nIt works much like the plot() function in base graphics system. It looks for variables to plot within a data frame, similar to lattice, or in the parent environment.\nIn general, it is good to get used to putting your data in a data frame and then passing it to qplot().\n\n\n\n\n\n\nPro tip\n\n\n\nThe qplot() function is somewhat discouraged in ggplot2 now and new users are encouraged to use the more general ggplot() function (more details in the next lesson).\nHowever, the qplot() function is still useful and may be easier to use if transitioning from the base plotting system or a different statistical package.\n\n\nPlots are made up of\n\naesthetics (e.g. size, shape, color)\ngeoms (e.g. points, lines)\n\nFactors play an important role for indicating subsets of the data (if they are to have different properties) so they should be labeled properly.\nThe qplot() hides much of what goes on underneath, which is okay for most operations, ggplot() is the core function and is very flexible for doing things qplot() cannot do."
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#before-you-start-label-your-data",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#before-you-start-label-your-data",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "Before you start: label your data",
    "text": "Before you start: label your data\nOne thing that is always true, but is particularly useful when using ggplot2, is that you should always use informative and descriptive labels on your data.\nMore generally, your data should have appropriate metadata so that you can quickly look at a dataset and know\n\nwhat are variables?\nwhat do the values of each variable mean?\n\n\n\n\n\n\n\nPro tip\n\n\n\n\nEach column of a data frame should have a meaningful (but concise) variable name that accurately reflects the data stored in that column\nNon-numeric or categorical variables should be coded as factor variables and have meaningful labels for each level of the factor.\n\nMight be common to code a binary variable as a “0” or a “1”, but the problem is that from quickly looking at the data, it’s impossible to know whether which level of that variable is represented by a “0” or a “1”.\nMuch better to simply label each observation as what they are.\nIf a variable represents temperature categories, it might be better to use “cold”, “mild”, and “hot” rather than “1”, “2”, and “3”.\n\n\n\n\nWhile it is sometimes a pain to make sure all of your data are properly labeled, this investment in time can pay dividends down the road when you’re trying to figure out what you were plotting.\nIn other words, including the proper metadata can make your exploratory plots essentially self-documenting."
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#ggplot2-hello-world",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#ggplot2-hello-world",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "ggplot2 “Hello, world!”",
    "text": "ggplot2 “Hello, world!”\nThis example dataset comes with the ggplot2 package and contains data on the fuel economy of 38 popular car models from 1999 to 2008.\n\nlibrary(tidyverse) # this loads the ggplot2 R package\n# library(ggplot2) # an alternative way to just load the ggplot2 R package\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\nYou can see from the glimpse() (part of the dplyr package) output that all of the categorical variables (like “manufacturer” or “class”) are **appropriately coded with meaningful label*s**.\nThis will come in handy when qplot() has to label different aspects of a plot.\nAlso note that all of the columns/variables have meaningful names (if sometimes abbreviated), rather than names like “X1”, and “X2”, etc.\n\n\n\n\n\n\nExample\n\n\n\nWe can make a quick scatterplot using qplot() of the engine displacement (displ) and the highway miles per gallon (hwy).\n\nqplot(x = displ, y = hwy, data = mpg)\n\n\n\n\nPlot of engine displacement and highway mileage using the mtcars dataset\n\n\n\n\n\n\nIt has a very similar feeling to plot() in base R.\n\n\n\n\n\n\nNote\n\n\n\nIn the call to qplot() you must specify the data argument so that qplot() knows where to look up the variables.\nYou must also specify x and y, but hopefully that part is obvious."
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#modifying-aesthetics",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#modifying-aesthetics",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "Modifying aesthetics",
    "text": "Modifying aesthetics\nWe can introduce a third variable into the plot by modifying the color of the points based on the value of that third variable.\nColor (or colour) is one type of aesthetic and using the ggplot2 language:\n\n“the color of each point can be mapped to a variable”\n\nThis sounds technical, but let’s give an example.\n\n\n\n\n\n\nExample\n\n\n\nWe map the color argument to the drv variable, which indicates whether a car is front wheel drive, rear wheel drive, or 4-wheel drive.\n\nqplot(displ, hwy, data = mpg, color = drv)\n\n\n\n\nEngine displacement and highway mileage by drive class\n\n\n\n\n\n\nNow we can see that the front wheel drive cars tend to have lower displacement relative to the 4-wheel or rear wheel drive cars.\nAlso, it’s clear that the 4-wheel drive cars have the lowest highway gas mileage.\n\n\n\n\n\n\nNote\n\n\n\nThe x argument and y argument are aesthetics too, and they got mapped to the displ and hwy variables, respectively.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn the above plot, I did not specify the x and y variable. What happens when you run these two code chunks. What’s the difference?\n\nqplot(displ, hwy, data = mpg, color = drv)\n\n\nqplot(x = displ, y = hwy, data = mpg, color = drv)\n\n\nqplot(hwy, displ, data = mpg, color = drv)\n\n\nqplot(y = hwy, x = displ, data = mpg, color = drv)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nLet’s try mapping colors in another dataset, namely the palmerpenguins dataset. These data contain observations for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\n\n\n\n\nPalmer penguins\n\n\n\n\n[Source: Artwork by Allison Horst]\n\nlibrary(palmerpenguins)\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIf we wanted to count the number of penguins for each of the three species, we can use the count() function in dplyr:\n\npenguins %>% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\nFor example, we see there are a total of 152 Adelie penguins in the palmerpenguins dataset.\n\n\n\n\n\n\nQuestion\n\n\n\nIf we wanted to use qplot() to map flipper_length_mm and bill_length_mm to the x and y coordinates, what would we do?\n\n# try it yourself\n\nNow try mapping color to the species variable on top of the code you just wrote:\n\n# try it yourself"
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#adding-a-geom",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#adding-a-geom",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "Adding a geom",
    "text": "Adding a geom\nSometimes it is nice to add a smoother to a scatterplot to highlight any trends.\nTrends can be difficult to see if the data are very noisy or there are many data points obscuring the view.\nA smoother is a type of “geom” that you can add along with your data points.\n\n\n\n\n\n\nExample\n\n\n\n\nqplot(displ, hwy, data = mpg, geom = c(\"point\", \"smooth\"))\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nEngine displacement and highway mileage w/smoother\n\n\n\n\n\n\nHere it seems that engine displacement and highway mileage have a nonlinear U-shaped relationship, but from the previous plot we know that this is largely due to confounding by the drive class of the car.\n\n\n\n\n\n\nNote\n\n\n\nPreviously, we did not have to specify geom = \"point\" because that was done automatically.\nBut if you want the smoother overlaid with the points, then you need to specify both explicitly.\n\n\nLook at what happens if we do not include the point geom.\n\nqplot(displ, hwy, data = mpg, geom = c(\"smooth\"))\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nEngine displacement and highway mileage w/smoother\n\n\n\n\nSometimes that is the plot you want to show, but in this case it might make more sense to show the data along with the smoother.\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s add a smoother to our palmerpenguins dataset example.\nUsing the code we previously wrote mapping variables to points and color, add a “point” and “smooth” geom:\n\n# try it yourself"
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#histograms-and-boxplots",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#histograms-and-boxplots",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "Histograms and boxplots",
    "text": "Histograms and boxplots\nThe qplot() function can be used to be used to plot 1-dimensional data too.\nBy specifying a single variable, qplot() will by default make a histogram.\n\n\n\n\n\n\nExample\n\n\n\nWe can make a histogram of the highway mileage data and stratify on the drive class. So technically this is three histograms on top of each other.\n\nqplot(hwy, data = mpg, fill = drv, binwidth = 2)\n\n\n\n\nHistogram of highway mileage by drive class\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nNotice, I used fill here to map color to the drv variable. Why is this? What happens when you use color instead?\n\n# try it yourself\n\n\n\nHaving the different colors for each drive class is nice, but the three histograms can be a bit difficult to separate out.\nSide-by-side boxplots are one solution to this problem.\n\nqplot(drv, hwy, data = mpg, geom = \"boxplot\")\n\n\n\n\nBoxplots of highway mileage by drive class\n\n\n\n\nAnother solution is to plot the histograms in separate panels using facets."
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#facets",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#facets",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "Facets",
    "text": "Facets\nFacets are a way to create multiple panels of plots based on the levels of categorical variable.\nHere, we want to see a histogram of the highway mileages and the categorical variable is the drive class variable. We can do that using the facets argument to qplot().\n\n\n\n\n\n\nNote\n\n\n\nThe facets argument expects a formula type of input, with a ~ separating the left hand side variable and the right hand side variable.\n\nThe left hand side variable indicates how the rows of the panels should be divided\nThe right hand side variable indicates how the columns of the panels should be divided\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nHere, we just want three rows of histograms (and just one column), one for each drive class, so we specify drv on the left hand side and . on the right hand side indicating that there’s no variable there (it’s empty).\n\nqplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)\n\n\n\n\nHistogram of highway mileage by drive class\n\n\n\n\n\n\nWe could also look at more data using facets, so instead of histograms we could look at scatter plots of engine displacement and highway mileage by drive class.\nHere, we put the drv variable on the right hand side to indicate that we want a column for each drive class (as opposed to splitting by rows like we did above).\n\nqplot(displ, hwy, data = mpg, facets = . ~ drv)\n\n\n\n\nEngine displacement and highway mileage by drive class\n\n\n\n\nWhat if you wanted to add a smoother to each one of those panels? Simple, you literally just add the smoother as another geom.\n\nqplot(displ, hwy, data = mpg, facets = . ~ drv) + \n  geom_smooth(method = \"lm\")\n\n\n\n\nEngine displacement and highway mileage by drive class w/smoother\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe used a different type of smoother above.\nHere, we add a linear regression line (a type of smoother) to each group to see if there’s any difference.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s facet our palmerpenguins dataset example and explore different types of plots.\nBuilding off the code we previously wrote, perform the following tasks:\n\nFacet the plot based on species with the the three species along rows.\nAdd a linear regression line to each the types of species\n\n\n# try it yourself\n\nNext, make a histogram of the body_mass_g for each of the species colored by the three species.\n\n# try it yourself"
  },
  {
    "objectID": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#summary",
    "href": "posts/2022-09-13-ggplot2-plotting-system-part-1/index.html#summary",
    "title": "The ggplot2 plotting system: qplot()",
    "section": "Summary",
    "text": "Summary\nThe qplot() function in ggplot2 is the analog of plot() in base graphics but with many built-in features that the traditionaly plot() does not provide. The syntax is somewhere in between the base and lattice graphics system. The qplot() function is useful for quickly putting data on the page/screen, but for ultimate customization, it may make more sense to use some of the lower level functions that we discuss later in the next lesson."
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html",
    "href": "posts/2022-09-20-control-structures/index.html",
    "title": "Control Structures",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://rafalab.github.io/dsbook/programming-basics\nhttps://r4ds.had.co.nz/iteration\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-control-structures\nhttps://r4ds.had.co.nz/iteration"
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html#if-else",
    "href": "posts/2022-09-20-control-structures/index.html#if-else",
    "title": "Control Structures",
    "section": "if-else",
    "text": "if-else\nThe if-else combination is probably the most commonly used control structure in R (or perhaps any language). This structure allows you to test a condition and act on it depending on whether it’s true or false.\nFor starters, you can just use the if statement.\nif(<condition>) {\n        ## do something\n} \n## Continue with rest of code\nThe above code does nothing if the condition is false. If you have an action you want to execute when the condition is false, then you need an else clause.\nif(<condition>) {\n        ## do something\n} \nelse {\n        ## do something else\n}\nYou can have a series of tests by following the initial if with any number of else ifs.\nif(<condition1>) {\n        ## do something\n} else if(<condition2>)  {\n        ## do something different\n} else {\n        ## do something different\n}\nHere is an example of a valid if/else structure.\n\n## Generate a uniform random number\nx <- runif(1, 0, 10)  \n\nif(x > 3) {\n        y <- 10\n} else {\n        y <- 0\n}\n\nThe value of y is set depending on whether x > 3 or not. This expression can also be written a different, but equivalent, way in R.\n\ny <- if(x > 3) {\n        10\n} else { \n        0\n}\n\nNeither way of writing this expression is more correct than the other. Which one you use will depend on your preference and perhaps those of the team you may be working with.\nOf course, the else clause is not necessary. You could have a series of if clauses that always get executed if their respective conditions are true.\nif(<condition1>) {\n\n}\n\nif(<condition2>) {\n\n}\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s use the palmerpenguins dataset and write a if-else statement that\n\nRandomly samples a value from a standard normal distribution (Hint: check out the rnorm() function in base R).\nIf the value is larger than 0, use dplyr functions to keep only the Chinstrap penguins.\nOtherwise, keep only the Gentoo penguins.\nRe-run the code 10 times and look at output.\n\n\n# try it yourself\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins \n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g"
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html#for-loops",
    "href": "posts/2022-09-20-control-structures/index.html#for-loops",
    "title": "Control Structures",
    "section": "for Loops",
    "text": "for Loops\nFor loops are pretty much the only looping construct that you will need in R. While you may occasionally find a need for other types of loops, in my experience doing data analysis, I’ve found very few situations where a for loop was not sufficient.\nIn R, for loops take an iterator variable and assign it successive values from a sequence or vector.\nFor loops are most commonly used for iterating over the elements of an object (list, vector, etc.)\n\nfor(i in 1:10) {\n        print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nThis loop takes the i variable and in each iteration of the loop gives it values 1, 2, 3, …, 10, executes the code within the curly braces, and then the loop exits.\nThe following three loops all have the same behavior.\n\nx <- c(\"a\", \"b\", \"c\", \"d\")\n\nfor(i in 1:4) {\n        ## Print out each element of 'x'\n        print(x[i])  \n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\n\nThe seq_along() function is commonly used in conjunction with for loops in order to generate an integer sequence based on the length of an object (in this case, the object x).\n\n## Generate a sequence based on length of 'x'\nfor(i in seq_along(x)) {   \n        print(x[i])\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\n\nIt is not necessary to use an index-type variable.\n\nfor(letter in x) {\n        print(letter)\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\n\nFor one line loops, the curly braces are not strictly necessary.\n\nfor(i in 1:4) print(x[i])\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\n\nHowever, I like to use curly braces even for one-line loops, because that way if you decide to expand the loop to multiple lines, you won’t be burned because you forgot to add curly braces (and you will be burned by this)."
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html#nested-for-loops",
    "href": "posts/2022-09-20-control-structures/index.html#nested-for-loops",
    "title": "Control Structures",
    "section": "Nested for loops",
    "text": "Nested for loops\nfor loops can be nested inside of each other.\nx <- matrix(1:6, 2, 3)\n\nfor(i in seq_len(nrow(x))) {\n        for(j in seq_len(ncol(x))) {\n                print(x[i, j])\n        }   \n}\nNested loops are commonly needed for multidimensional or hierarchical data structures (e.g. matrices, lists). Be careful with nesting though. Nesting beyond 2 to 3 levels often makes it difficult to read/understand the code. If you find yourself in need of a large number of nested loops, you may want to break up the loops by using functions (discussed later)."
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html#while-loops",
    "href": "posts/2022-09-20-control-structures/index.html#while-loops",
    "title": "Control Structures",
    "section": "while Loops",
    "text": "while Loops\nWhile loops begin by testing a condition. If it is true, then they execute the loop body. Once the loop body is executed, the condition is tested again, and so forth, until the condition is false, after which the loop exits.\n\ncount <- 0\nwhile(count < 10) {\n        print(count)\n        count <- count + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n\nWhile loops can potentially result in infinite loops if not written properly. Use with care!\nSometimes there will be more than one condition in the test.\n\nz <- 5\nset.seed(1)\n\nwhile(z >= 3 && z <= 10) {\n        coin <- rbinom(1, 1, 0.5)\n        \n        if(coin == 1) {  ## random walk\n                z <- z + 1\n        } else {\n                z <- z - 1\n        } \n}\nprint(z)\n\n[1] 2\n\n\nConditions are always evaluated from left to right. For example, in the above code, if z were less than 3, the second test would not have been evaluated."
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html#repeat-loops",
    "href": "posts/2022-09-20-control-structures/index.html#repeat-loops",
    "title": "Control Structures",
    "section": "repeat Loops",
    "text": "repeat Loops\nrepeat initiates an infinite loop right from the start. These are not commonly used in statistical or data analysis applications but they do have their uses. The only way to exit a repeat loop is to call break.\nOne possible paradigm might be in an iterative algorithm where you may be searching for a solution and you don’t want to stop until you’re close enough to the solution. In this kind of situation, you often don’t know in advance how many iterations it’s going to take to get “close enough” to the solution.\n\nx0 <- 1\ntol <- 1e-8\n\nrepeat {\n        x1 <- computeEstimate()\n        \n        if(abs(x1 - x0) < tol) {  ## Close enough?\n                break\n        } else {\n                x0 <- x1\n        } \n}\n\nNote that the above code will not run if the computeEstimate() function is not defined (I just made it up for the purposes of this demonstration).\nThe loop above is a bit dangerous because there’s no guarantee it will stop. You could get in a situation where the values of x0 and x1 oscillate back and forth and never converge. Better to set a hard limit on the number of iterations by using a for loop and then report whether convergence was achieved or not."
  },
  {
    "objectID": "posts/2022-09-20-control-structures/index.html#next-break",
    "href": "posts/2022-09-20-control-structures/index.html#next-break",
    "title": "Control Structures",
    "section": "next, break",
    "text": "next, break\nnext is used to skip an iteration of a loop.\n\nfor(i in 1:100) {\n        if(i <= 20) {\n                ## Skip the first 20 iterations\n                next                 \n        }\n        ## Do something here\n}\n\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\n\nfor(i in 1:100) {\n      print(i)\n\n      if(i > 20) {\n              ## Stop loop after 20 iterations\n              break  \n      }     \n}"
  },
  {
    "objectID": "posts/2022-09-08-tidy-data-and-the-tidyverse/index.html",
    "href": "posts/2022-09-08-tidy-data-and-the-tidyverse/index.html",
    "title": "Tidy data and the Tidyverse",
    "section": "",
    "text": "“Tidy datasets are all alike, but every messy dataset is messy in its own way.” —- Hadley Wickham\n\n\nPre-lecture materials\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nTidy Data paper published in the Journal of Statistical Software\nhttps://r4ds.had.co.nz/tidy-data\ntidyr cheat sheet from RStudio\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-tidy-data-and-the-tidyverse\nhttps://r4ds.had.co.nz/tidy-data\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nDefine tidy data\nBe able to transform non-tidy data into tidy data\nBe able to transform wide data into long data\nBe able to separate character columns into multiple columns\nBe able to unite multiple character columns into one column\n\n\n\n\n\nTidy data\nAs we learned in the last lesson, one unifying concept of the tidyverse is the notion of tidy data.\nAs defined by Hadley Wickham in his 2014 paper published in the Journal of Statistical Software, a tidy dataset has the following properties:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\nArtwork by Allison Horst on tidy data\n\n\n[Source: Artwork by Allison Horst]\nThe purpose of defining tidy data is to highlight the fact that most data do not start out life as tidy.\nIn fact, much of the work of data analysis may involve simply making the data tidy (at least this has been our experience).\n\nOnce a dataset is tidy, it can be used as input into a variety of other functions that may transform, model, or visualize the data.\n\n\n\n\n\n\n\nExample\n\n\n\nAs a quick example, consider the following data illustrating religion and income survey data with the number of respondents with income range in column name.\nThis is in a classic table format:\n\nlibrary(tidyr)\nrelig_income\n\n# A tibble: 18 × 11\n   religion      `<$10k` $10-2…¹ $20-3…² $30-4…³ $40-5…⁴ $50-7…⁵ $75-1…⁶ $100-…⁷\n   <chr>           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Agnostic           27      34      60      81      76     137     122     109\n 2 Atheist            12      27      37      52      35      70      73      59\n 3 Buddhist           27      21      30      34      33      58      62      39\n 4 Catholic          418     617     732     670     638    1116     949     792\n 5 Don’t know/r…      15      14      15      11      10      35      21      17\n 6 Evangelical …     575     869    1064     982     881    1486     949     723\n 7 Hindu               1       9       7       9      11      34      47      48\n 8 Historically…     228     244     236     238     197     223     131      81\n 9 Jehovah's Wi…      20      27      24      24      21      30      15      11\n10 Jewish             19      19      25      25      30      95      69      87\n11 Mainline Prot     289     495     619     655     651    1107     939     753\n12 Mormon             29      40      48      51      56     112      85      49\n13 Muslim              6       7       9      10       9      23      16       8\n14 Orthodox           13      17      23      32      32      47      38      42\n15 Other Christ…       9       7      11      13      13      14      18      14\n16 Other Faiths       20      33      40      46      49      63      46      40\n17 Other World …       5       2       3       4       2       7       3       4\n18 Unaffiliated      217     299     374     365     341     528     407     321\n# … with 2 more variables: `>150k` <dbl>, `Don't know/refused` <dbl>, and\n#   abbreviated variable names ¹​`$10-20k`, ²​`$20-30k`, ³​`$30-40k`, ⁴​`$40-50k`,\n#   ⁵​`$50-75k`, ⁶​`$75-100k`, ⁷​`$100-150k`\n\n\n\n\nWhile this format is canonical and is useful for quickly observing the relationship between multiple variables, it is not tidy.\nThis format violates the tidy form because there are variables in the columns.\n\nIn this case the variables are religion, income bracket, and the number of respondents, which is the third variable, is presented inside the table.\n\nConverting this data to tidy format would give us\n\nlibrary(tidyverse)\n\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %>%\n  mutate(religion = factor(religion), income = factor(income))\n\n# A tibble: 180 × 3\n   religion income             respondents\n   <fct>    <fct>                    <dbl>\n 1 Agnostic <$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic >150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\n\nSome of these functions you have seen before, others might be new to you. Let’s talk about each one in the context of the tidyverse R packages.\n\n\nThe “Tidyverse”\nThere are a number of R packages that take advantage of the tidy data form and can be used to do interesting things with data. Many (but not all) of these packages are written by Hadley Wickham and the collection of packages is often referred to as the “tidyverse” because of their dependence on and presumption of tidy data.\n\n\n\n\n\n\nNote\n\n\n\nA subset of the “Tidyverse” packages include:\n\nggplot2: a plotting system based on the grammar of graphics\nmagrittr: defines the %>% operator for chaining functions together in a series of operations on data\ndplyr: a suite of (fast) functions for working with data frames\ntidyr: easily tidy data with pivot_wider() and pivot_longer() functions (also separate() and unite())\n\nA complete list can be found here (https://www.tidyverse.org/packages).\n\n\nWe will be using these packages quite a bit.\nThe “tidyverse” package can be used to install all of the packages in the tidyverse at once.\nFor example, instead of starting an R script with this:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\n\nYou can start with this:\n\nlibrary(tidyverse)\n\nIn the example above, let’s talk about what we did using the pivot_longer() function.\nWe will also talk about pivot_wider().\n\npivot_longer()\nThe tidyr package includes functions to transfer a data frame between long and wide.\n\nWide format data tends to have different attributes or variables describing an observation placed in separate columns.\nLong format data tends to have different attributes encoded as levels of a single variable, followed by another column that contains tha values of the observation at those different levels.\n\n\n\n\n\n\n\nExample\n\n\n\nIn the section above, we showed an example that used pivot_longer() to convert data into a tidy format.\nThe key problem with the tidyness of the data is that the income variables are not in their own columns, but rather are embedded in the structure of the columns.\nTo fix this, you can use the pivot_longer() function to gather values spread across several columns into a single column, here with the column names gathered into an income column.\nNote: when gathering, exclude any columns that you do not want “gathered” (religion in this case) by including the column names with a the minus sign in the pivot_longer() function.\nFor example:\n\n# Gather everything EXCEPT religion to tidy data\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\")\n\n# A tibble: 180 × 3\n   religion income             respondents\n   <chr>    <chr>                    <dbl>\n 1 Agnostic <$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic >150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\n\n\n\nEven if your data is in a tidy format, pivot_longer() is occasionally useful for pulling data together to take advantage of faceting, or plotting separate plots based on a grouping variable. We will talk more about that in a future lecture.\n\n\npivot_wider()\nThe pivot_wider() function is less commonly needed to tidy data. It can, however, be useful for creating summary tables.\n\n\n\n\n\n\nExample\n\n\n\nYou use the summarize() function in dplyr to summarize the total number of respondents per income category.\n\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %>%\n  mutate(religion = factor(religion), income = factor(income)) %>% \n  group_by(income) %>% \n  summarize(total_respondents = sum(respondents)) %>%\n  pivot_wider(names_from = \"income\", \n              values_from = \"total_respondents\") %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<$10k\n>150k\n$10-20k\n$100-150k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\nDon’t know/refused\n\n\n\n\n1930\n2608\n2781\n3197\n3357\n3302\n3085\n5185\n3990\n6121\n\n\n\n\n\n\n\nNotice in this example how pivot_wider() has been used at the very end of the code sequence to convert the summarized data into a shape that offers a better tabular presentation for a report.\n\n\n\n\n\n\nNote\n\n\n\nIn the pivot_wider() call, you first specify the name of the column to use for the new column names (income in this example) and then specify the column to use for the cell values (total_respondents here).\n\n\n\n\n\n\n\n\nExample of pivot_longer()\n\n\n\nLet’s try another dataset. This data contain an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.\n\nlibrary(gapminder)\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# … with 1,694 more rows\n\n\nIf we wanted to make lifeExp, pop and gdpPercap (all measurements that we observe) go from a wide table into a long table, what would we do?\n\n# try it yourself\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nOne more! Try using pivot_longer() to convert the the following data that contains made-up revenues for three companies by quarter for years 2006 to 2009.\nAfterward, use group_by() and summarize() to calculate the average revenue for each company across all years and all quarters.\nBonus: Calculate a mean revenue for each company AND each year (averaged across all 4 quarters).\n\ndf <- tibble(\n  \"company\" = rep(1:3, each=4), \n  \"year\"  = rep(2006:2009, 3),\n  \"Q1\"    = sample(x = 0:100, size = 12),\n  \"Q2\"    = sample(x = 0:100, size = 12),\n  \"Q3\"    = sample(x = 0:100, size = 12),\n  \"Q4\"    = sample(x = 0:100, size = 12),\n)\ndf\n\n# A tibble: 12 × 6\n   company  year    Q1    Q2    Q3    Q4\n     <int> <int> <int> <int> <int> <int>\n 1       1  2006    94    95    50    87\n 2       1  2007    73    16    86    82\n 3       1  2008    64    50    85    61\n 4       1  2009     6     4    87     1\n 5       2  2006    88    24    74    52\n 6       2  2007    32    98    52    94\n 7       2  2008     4    42    79     3\n 8       2  2009    23    97    71    76\n 9       3  2006     1    75    97    66\n10       3  2007    30    87    58     5\n11       3  2008    17    91    61    70\n12       3  2009    14    12     1    60\n\n\n\n# try it yourself \n\n\n\n\n\nseparate() and unite()\nThe same tidyr package also contains two useful functions:\n\nunite(): combine contents of two or more columns into a single column\nseparate(): separate contents of a column into two or more columns\n\nFirst, we combine the first three columns into one new column using unite().\n\ngapminder %>% \n  unite(col=\"country_continent_year\", \n        country:year, \n        sep=\"_\")\n\n# A tibble: 1,704 × 4\n   country_continent_year lifeExp      pop gdpPercap\n   <chr>                    <dbl>    <int>     <dbl>\n 1 Afghanistan_Asia_1952     28.8  8425333      779.\n 2 Afghanistan_Asia_1957     30.3  9240934      821.\n 3 Afghanistan_Asia_1962     32.0 10267083      853.\n 4 Afghanistan_Asia_1967     34.0 11537966      836.\n 5 Afghanistan_Asia_1972     36.1 13079460      740.\n 6 Afghanistan_Asia_1977     38.4 14880372      786.\n 7 Afghanistan_Asia_1982     39.9 12881816      978.\n 8 Afghanistan_Asia_1987     40.8 13867957      852.\n 9 Afghanistan_Asia_1992     41.7 16317921      649.\n10 Afghanistan_Asia_1997     41.8 22227415      635.\n# … with 1,694 more rows\n\n\nNext, we show how to separate the columns into three separate columns using separate() using the col, into and sep arguments.\n\ngapminder %>% \n  unite(col=\"country_continent_year\", \n        country:year, \n        sep=\"_\") %>% \n  separate(col=\"country_continent_year\", \n           into=c(\"country\", \"continent\", \"year\"), \n           sep=\"_\")\n\n# A tibble: 1,704 × 6\n   country     continent year  lifeExp      pop gdpPercap\n   <chr>       <chr>     <chr>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia      1952     28.8  8425333      779.\n 2 Afghanistan Asia      1957     30.3  9240934      821.\n 3 Afghanistan Asia      1962     32.0 10267083      853.\n 4 Afghanistan Asia      1967     34.0 11537966      836.\n 5 Afghanistan Asia      1972     36.1 13079460      740.\n 6 Afghanistan Asia      1977     38.4 14880372      786.\n 7 Afghanistan Asia      1982     39.9 12881816      978.\n 8 Afghanistan Asia      1987     40.8 13867957      852.\n 9 Afghanistan Asia      1992     41.7 16317921      649.\n10 Afghanistan Asia      1997     41.8 22227415      635.\n# … with 1,694 more rows\n\n\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nUsing prose, describe how the variables and observations are organised in a tidy dataset versus an non-tidy dataset.\nWhat do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets.\n\n\ntibble(x = c(\"a,b,c\", \"d,e,f,g\", \"h,i,j\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\n\nBoth unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\nCompare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite()?\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nTidy Data paper published in the Journal of Statistical Software\nhttps://r4ds.had.co.nz/tidy-data.html\ntidyr cheat sheet from RStudio"
  },
  {
    "objectID": "posts/2022-09-08-joining-data-in-r/index.html",
    "href": "posts/2022-09-08-joining-data-in-r/index.html",
    "title": "Joining data in R",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/relational-data\nhttps://rafalab.github.io/dsbook/joining-tables\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-joining-data-in-r-basics\nhttps://r4ds.had.co.nz/relational-data\nhttps://rafalab.github.io/dsbook/joining-tables"
  },
  {
    "objectID": "posts/2022-09-08-joining-data-in-r/index.html#keys",
    "href": "posts/2022-09-08-joining-data-in-r/index.html#keys",
    "title": "Joining data in R",
    "section": "Keys",
    "text": "Keys\nThe variables used to connect each pair of tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation.\n\n\n\n\n\n\nNote\n\n\n\nThere are two types of keys:\n\nA primary key uniquely identifies an observation in its own table.\nA foreign key uniquely identifies an observation in another table.\n\n\n\nLet’s consider an example to help us understand the difference between a primary key and foreign key."
  },
  {
    "objectID": "posts/2022-09-08-joining-data-in-r/index.html#example-of-keys",
    "href": "posts/2022-09-08-joining-data-in-r/index.html#example-of-keys",
    "title": "Joining data in R",
    "section": "Example of keys",
    "text": "Example of keys\nImagine you are conduct a study and collecting data on subjects and a health outcome.\nOften, subjects will make multiple visits (a so-called longitudinal study) and so we will record the outcome for each visit. Similarly, we may record other information about them, such as the kind of housing they live in.\n\nThe first table\nThis code creates a simple table with some made up data about some hypothetical subjects’ outcomes.\n\nlibrary(tidyverse)\n\noutcomes <- tibble(\n        id = rep(c(\"a\", \"b\", \"c\"), each = 3),\n        visit = rep(0:2, 3),\n        outcome = rnorm(3 * 3, 3)\n)\n\nprint(outcomes)\n\n# A tibble: 9 × 3\n  id    visit outcome\n  <chr> <int>   <dbl>\n1 a         0   3.74 \n2 a         1   4.36 \n3 a         2   3.23 \n4 b         0   3.22 \n5 b         1   0.290\n6 b         2   1.33 \n7 c         0   3.14 \n8 c         1   3.29 \n9 c         2   3.39 \n\n\nNote that subjects are labeled by a unique identifer in the id column.\n\n\nA second table\nHere is some code to create a second table (we will be joining the first and second tables shortly). This table contains some data about the hypothetical subjects’ housing situation by recording the type of house they live in.\n\nsubjects <- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\")\n)\n\nprint(subjects)\n\n# A tibble: 3 × 2\n  id    house   \n  <chr> <chr>   \n1 a     detached\n2 b     rowhouse\n3 c     rowhouse\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the primary key and foreign key?\n\nThe outcomes$id is a primary key because it uniquely identifies each subject in the outcomes table.\nThe subjects$id is a foreign key because it appears in the subjects table where it matches each subject to a unique id."
  },
  {
    "objectID": "posts/2022-09-08-joining-data-in-r/index.html#left-join",
    "href": "posts/2022-09-08-joining-data-in-r/index.html#left-join",
    "title": "Joining data in R",
    "section": "Left Join",
    "text": "Left Join\nRecall the outcomes and subjects datasets above.\n\noutcomes\n\n# A tibble: 9 × 3\n  id    visit outcome\n  <chr> <int>   <dbl>\n1 a         0   3.74 \n2 a         1   4.36 \n3 a         2   3.23 \n4 b         0   3.22 \n5 b         1   0.290\n6 b         2   1.33 \n7 c         0   3.14 \n8 c         1   3.29 \n9 c         2   3.39 \n\nsubjects\n\n# A tibble: 3 × 2\n  id    house   \n  <chr> <chr>   \n1 a     detached\n2 b     rowhouse\n3 c     rowhouse\n\n\nSuppose we want to create a table that combines the information about houses (subjects) with the information about the outcomes (outcomes).\nWe can use the left_join() function to merge the outcomes and subjects tables and produce the output above.\n\nleft_join(x = outcomes, y = subjects, by = \"id\")\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <int>   <dbl> <chr>   \n1 a         0   3.74  detached\n2 a         1   4.36  detached\n3 a         2   3.23  detached\n4 b         0   3.22  rowhouse\n5 b         1   0.290 rowhouse\n6 b         2   1.33  rowhouse\n7 c         0   3.14  rowhouse\n8 c         1   3.29  rowhouse\n9 c         2   3.39  rowhouse\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe by argument indicates the column (or columns) that the two tables have in common.\n\n\n\nLeft Join with Incomplete Data\nIn the previous examples, the subjects table didn’t have a visit column. But suppose it did? Maybe people move around during the study. We could image a table like this one.\n\nsubjects <- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        visit = c(0, 1, 0),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\"),\n)\n\nprint(subjects)\n\n# A tibble: 3 × 3\n  id    visit house   \n  <chr> <dbl> <chr>   \n1 a         0 detached\n2 b         1 rowhouse\n3 c         0 rowhouse\n\n\nWhen we left joint the tables now we get:\n\nleft_join(outcomes, subjects, by = c(\"id\", \"visit\"))\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 a         0   3.74  detached\n2 a         1   4.36  <NA>    \n3 a         2   3.23  <NA>    \n4 b         0   3.22  <NA>    \n5 b         1   0.290 rowhouse\n6 b         2   1.33  <NA>    \n7 c         0   3.14  rowhouse\n8 c         1   3.29  <NA>    \n9 c         2   3.39  <NA>    \n\n\n\n\n\n\n\n\nNote\n\n\n\nTwo things to point out here:\n\nIf we do not have information about a subject’s housing in a given visit, the left_join() function automatically inserts an NA value to indicate that it is missing.\nWe can “join” on multiple variable (e.g. here we joined on the id and the visit columns).\n\n\n\nWe may even have a situation where we are missing housing data for a subject completely. The following table has no information about subject a.\n\nsubjects <- tibble(\n        id = c(\"b\", \"c\"),\n        visit = c(1, 0),\n        house = c(\"rowhouse\", \"rowhouse\"),\n)\n\nsubjects\n\n# A tibble: 2 × 3\n  id    visit house   \n  <chr> <dbl> <chr>   \n1 b         1 rowhouse\n2 c         0 rowhouse\n\n\nBut we can still join the tables together and the house values for subject a will all be NA.\n\nleft_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 a         0   3.74  <NA>    \n2 a         1   4.36  <NA>    \n3 a         2   3.23  <NA>    \n4 b         0   3.22  <NA>    \n5 b         1   0.290 rowhouse\n6 b         2   1.33  <NA>    \n7 c         0   3.14  rowhouse\n8 c         1   3.29  <NA>    \n9 c         2   3.39  <NA>    \n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe bottom line for left_join() is that it always retains the values in the “left” argument (in this case the outcomes table).\n\nIf there are no corresponding values in the “right” argument, NA values will be filled in."
  },
  {
    "objectID": "posts/2022-09-08-joining-data-in-r/index.html#inner-join",
    "href": "posts/2022-09-08-joining-data-in-r/index.html#inner-join",
    "title": "Joining data in R",
    "section": "Inner Join",
    "text": "Inner Join\nThe inner_join() function only retains the rows of both tables that have corresponding values. Here we can see the difference.\n\ninner_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n# A tibble: 2 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 b         1   0.290 rowhouse\n2 c         0   3.14  rowhouse"
  },
  {
    "objectID": "posts/2022-09-08-joining-data-in-r/index.html#right-join",
    "href": "posts/2022-09-08-joining-data-in-r/index.html#right-join",
    "title": "Joining data in R",
    "section": "Right Join",
    "text": "Right Join\nThe right_join() function is like the left_join() function except that it gives priority to the “right” hand argument.\n\nright_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n# A tibble: 2 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 b         1   0.290 rowhouse\n2 c         0   3.14  rowhouse"
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/data-visualisation\nhttp://vita.had.co.nz/papers/layered-grammar.pdf\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-2"
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#basic-components-of-a-ggplot2-plot",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#basic-components-of-a-ggplot2-plot",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "Basic components of a ggplot2 plot",
    "text": "Basic components of a ggplot2 plot\n\n\n\n\n\n\nKey components\n\n\n\nA ggplot2 plot consists of a number of key components.\n\nA data frame: stores all of the data that will be displayed on the plot\naesthetic mappings: describe how data are mapped to color, size, shape, location\ngeoms: geometric objects like points, lines, shapes\nfacets: describes how conditional/panel plots should be constructed\nstats: statistical transformations like binning, quantiles, smoothing\nscales: what scale an aesthetic map uses (example: left-handed = red, right-handed = blue)\ncoordinate system: describes the system in which the locations of the geoms will be drawn\n\n\n\nIt is essential to organize your data into a data frame before you start with ggplot2 (and all the appropriate metadata so that your data frame is self-describing and your plots will be self-documenting).\nWhen building plots in ggplot2 (rather than using qplot()), the “artist’s palette” model may be the closest analogy.\nEssentially, you start with some raw data, and then you gradually add bits and pieces to it to create a plot.\n\n\n\n\n\n\nNote\n\n\n\nPlots are built up in layers, with the typically ordering being\n\nPlot the data\nOverlay a summary\nAdd metadata and annotation\n\n\n\nFor quick exploratory plots you may not get past step 1."
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#example-bmi-pm2.5-asthma",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#example-bmi-pm2.5-asthma",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "Example: BMI, PM2.5, Asthma",
    "text": "Example: BMI, PM2.5, Asthma\nTo demonstrate the various pieces of ggplot2 we will use a running example from the Mouse Allergen and Asthma Cohort Study (MAACS). Here, the question we are interested in is\n\n“Are overweight individuals, as measured by body mass index (BMI), more susceptible than normal weight individuals to the harmful effects of PM2.5 on asthma symptoms?”\n\nThere is a suggestion that overweight individuals may be more susceptible to the negative effects of inhaling PM2.5.\nThis would suggest that increases in PM2.5 exposure in the home of an overweight child would be more deleterious to his/her asthma symptoms than they would be in the home of a normal weight child.\nWe want to see if we can see that difference in the data from MAACS.\n\n\n\n\n\n\nNote\n\n\n\nBecause the individual-level data for this study are protected by various U.S. privacy laws, we cannot make those data available.\nFor the purposes of this lesson, we have simulated data that share many of the same features of the original data, but do not contain any of the actual measurements or values contained in the original dataset.\n\n\n\n\n\n\n\n\nExample\n\n\n\nWe can look at the data quickly by reading it in as a tibble with read_csv() in the tidyverse package.\n\nlibrary(tidyverse)\nlibrary(here)\nmaacs <- read_csv(here(\"data\", \"bmi_pm25_no2_sim.csv\"),\n                  col_types = \"nnci\")\nmaacs\n\n# A tibble: 517 × 4\n   logpm25 logno2_new bmicat        NocturnalSympt\n     <dbl>      <dbl> <chr>                  <int>\n 1   1.25       1.18  normal weight              1\n 2   1.12       1.55  overweight                 0\n 3   1.93       1.43  normal weight              0\n 4   1.37       1.77  overweight                 2\n 5   0.775      0.765 normal weight              0\n 6   1.49       1.11  normal weight              0\n 7   2.16       1.43  normal weight              0\n 8   1.65       1.40  normal weight              0\n 9   1.55       1.81  normal weight              0\n10   2.04       1.35  overweight                 3\n# … with 507 more rows\n\n\n\n\nThe outcome we will look at here (NocturnalSymp) is the number of days in the past 2 weeks where the child experienced asthma symptoms (e.g. coughing, wheezing) while sleeping.\nThe other key variables are:\n\nlogpm25: average level of PM2.5 over the course of 7 days (micrograms per cubic meter) on the log scale\nlogno2_new: exhaled nitric oxide on the log scale\nbmicat: categorical variable with BMI status"
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#first-plot-with-point-layer",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#first-plot-with-point-layer",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "First plot with point layer",
    "text": "First plot with point layer\nTo make a scatter plot, we need add at least one geom, such as points.\nHere, we add the geom_point() function to create a traditional scatter plot.\n\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\ng + geom_point()\n\n\n\n\nScatterplot of PM2.5 and days with nocturnal symptoms\n\n\n\n\nHow does ggplot know what points to plot? In this case, it can grab them from the data frame maacs that served as the input into the ggplot() function."
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#adding-more-layers",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#adding-more-layers",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "Adding more layers",
    "text": "Adding more layers\n\nsmooth\nBecause the data appear rather noisy, it might be better if we added a smoother on top of the points to see if there is a trend in the data with PM2.5.\n\ng + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nScatterplot with smoother\n\n\n\n\nThe default smoother is a loess smoother, which is flexible and nonparametric but might be too flexible for our purposes. Perhaps we’d prefer a simple linear regression line to highlight any first order trends. We can do this by specifying method = \"lm\" to geom_smooth().\n\ng + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\nScatterplot with linear regression line\n\n\n\n\nHere, we can see there appears to be a slight increasing trend, suggesting that higher levels of PM2.5 are associated with increased days with nocturnal symptoms.\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s use the ggplot() function with our palmerpenguins dataset example and make a scatter plot with flipper_length_mm on the x-axis, bill_length_mm on the y-axis, colored by species, and a smoother by adding a linear regression.\n\n# try it yourself\n\nlibrary(palmerpenguins)\npenguins \n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\n\n\n\nfacets\nBecause our primary question involves comparing overweight individuals to normal weight individuals, we can stratify the scatter plot of PM2.5 and nocturnal symptoms by the BMI category (bmicat) variable, which indicates whether an individual is overweight or now.\nTo visualize this we can add a facet_grid(), which takes a formula argument.\n\n\n\n\n\n\nExample\n\n\n\nWe want one row and two columns, one column for each weight category. So we specify bmicat on the right hand side of the forumla passed to facet_grid().\n\ng + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  facet_grid(. ~ bmicat) \n\n\n\n\nScatterplot of PM2.5 and nocturnal symptoms by BMI category\n\n\n\n\n\n\nNow it seems clear that the relationship between PM2.5 and nocturnal symptoms is relatively flat among normal weight individuals, while the relationship is increasing among overweight individuals.\nThis plot suggests that overweight individuals may be more susceptible to the effects of PM2.5."
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#customizing-the-smooth",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#customizing-the-smooth",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "Customizing the smooth",
    "text": "Customizing the smooth\nWe can also customize aspects of the geoms.\nFor example, we can customize the smoother that we overlay on the points with geom_smooth().\nHere we change the line type and increase the size from the default. We also remove the shaded standard error from the line.\n\ng + \n  geom_point(aes(color = bmicat), \n             size = 2, \n             alpha = 1/2) + \n  geom_smooth(size = 4, \n              linetype = 3, \n              method = \"lm\", \n              se = FALSE)\n\n\n\n\nCustomizing a smoother"
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#changing-the-theme",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#changing-the-theme",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "Changing the theme",
    "text": "Changing the theme\nThe default theme for ggplot2 uses the gray background with white grid lines.\nIf you don’t find this suitable, you can use the black and white theme by using the theme_bw() function.\nThe theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\n\n\n\n\n\n\nNote\n\n\n\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\n\ntheme_gray(): The default theme (gray background)\ntheme_bw(): More stark/plain\n\n\n\n\ng + \n  geom_point(aes(color = bmicat)) + \n  theme_bw(base_family = \"Times\")\n\n\n\n\nModifying the theme for a plot\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s take our palmerpenguins scatterplot from above and change out the theme to use theme_dark().\n\n# try it yourself\n\nlibrary(palmerpenguins)\npenguins \n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g"
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#modifying-labels",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#modifying-labels",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "Modifying labels",
    "text": "Modifying labels\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of annotations you can add to a plot, including different kinds of labels.\n\nxlab() for x-axis labels\nylab() for y-axis labels\nggtitle() for specifying plot titles\n\nlabs() function is generic and can be used to modify multiple types of labels at once\n\n\nHere is an example of modifying the title and the x and y labels to make the plot a bit more informative.\n\ng + \n  geom_point(aes(color = bmicat)) + \n  labs(title = \"MAACS Cohort\") + \n  labs(x = expression(\"log \" * PM[2.5]), \n       y = \"Nocturnal Symptoms\")\n\n\n\n\nModifying plot labels"
  },
  {
    "objectID": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#a-quick-aside-about-axis-limits",
    "href": "posts/2022-09-15-ggplot2-plotting-system-part-2/index.html#a-quick-aside-about-axis-limits",
    "title": "The ggplot2 plotting system: ggplot()",
    "section": "A quick aside about axis limits",
    "text": "A quick aside about axis limits\nOne quick quirk about ggplot2 that caught me up when I first started using the package can be displayed in the following example.\nIf you make a lot of time series plots, you often want to restrict the range of the y-axis while still plotting all the data.\nIn the base graphics system you can do that as follows.\n\ntestdat <- data.frame(x = 1:100, \n                      y = rnorm(100))\ntestdat[50,2] <- 100  ## Outlier!\nplot(testdat$x, \n     testdat$y,\n     type = \"l\", \n     ylim = c(-3,3))\n\n\n\n\nTime series plot with base graphics\n\n\n\n\nHere, we have restricted the y-axis range to be between -3 and 3, even though there is a clear outlier in the data.\n\n\n\n\n\n\nExample\n\n\n\nWith ggplot2 the default settings will give you this.\n\ng <- ggplot(testdat, aes(x = x, y = y))\ng + geom_line()\n\n\n\n\nTime series plot with default settings\n\n\n\n\nOne might think that modifying the ylim() attribute would give you the same thing as the base plot, but it doesn’t (?????)\n\ng + \n  geom_line() + \n  ylim(-3, 3)\n\n\n\n\nTime series plot with modified ylim\n\n\n\n\n\n\nEffectively, what this does is subset the data so that only observations between -3 and 3 are included, then plot the data.\nTo plot the data without subsetting it first and still get the restricted range, you have to do the following.\n\ng + \n  geom_line() + \n  coord_cartesian(ylim = c(-3, 3))\n\n\n\n\nTime series plot with restricted y-axis range\n\n\n\n\nAnd now you know!"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html",
    "href": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html",
    "title": "Introduction to R and RStudio!",
    "section": "",
    "text": "There are only two kinds of languages: the ones people complain about and the ones nobody uses. —Bjarne Stroustrup"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html#rtistry",
    "href": "posts/2022-08-30-introduction-to-r-and-rstudio/index.html#rtistry",
    "title": "Introduction to R and RStudio!",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Water Colours’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html",
    "title": "R Nuts and Bolts",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://rafalab.github.io/dsbook/r-basics\nhttps://r4ds.had.co.nz/vectors.html?q=typeof#vectors\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-r-nuts-and-bolts"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#entering-input",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#entering-input",
    "title": "R Nuts and Bolts",
    "section": "Entering Input",
    "text": "Entering Input\nAt the R prompt we type expressions. The <- symbol is the assignment operator.\n\nx <- 1\nprint(x)\n\n[1] 1\n\nx\n\n[1] 1\n\nmsg <- \"hello\"\n\nThe grammar of the language determines whether an expression is complete or not.\n\nx <-  ## Incomplete expression\n\nError: <text>:2:0: unexpected end of input\n1: x <-  ## Incomplete expression\n   ^\n\n\nThe # character indicates a comment.\nAnything to the right of the # (including the # itself) is ignored. This is the only comment character in R.\nUnlike some other languages, R does not support multi-line comments or comment blocks."
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#evaluation",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#evaluation",
    "title": "R Nuts and Bolts",
    "section": "Evaluation",
    "text": "Evaluation\nWhen a complete expression is entered at the prompt, it is evaluated and the result of the evaluated expression is returned.\nThe result may be auto-printed.\n\nx <- 5  ## nothing printed\nx       ## auto-printing occurs\n\n[1] 5\n\nprint(x)  ## explicit printing\n\n[1] 5\n\n\nThe [1] shown in the output indicates that x is a vector and 5 is its first element.\nTypically with interactive work, we do not explicitly print objects with the print() function; it is much easier to just auto-print them by typing the name of the object and hitting return/enter.\nHowever, when writing scripts, functions, or longer programs, there is sometimes a need to explicitly print objects because auto-printing does not work in those settings.\nWhen an R vector is printed you will notice that an index for the vector is printed in square brackets [] on the side. For example, see this integer sequence of length 20.\n\n\n\n\nx <- 11:30\nx\n\n [1] 11 12 13 14 15 16 17 18 19 20 21 22\n[13] 23 24 25 26 27 28 29 30\n\n\n\n\n\nThe numbers in the square brackets are not part of the vector itself, they are merely part of the printed output.\n\n\n\n\n\n\nNote\n\n\n\nWith R, it’s important that one understand that there is a difference between the actual R object and the manner in which that R object is printed to the console.\nOften, the printed output may have additional bells and whistles to make the output more friendly to the users. However, these bells and whistles are not inherently part of the object.\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\nThe : operator is used to create integer sequences.\n\n5:0\n\n[1] 5 4 3 2 1 0\n\n-15:15\n\n [1] -15 -14 -13 -12 -11 -10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3\n[20]   4   5   6   7   8   9  10  11  12  13  14  15"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#r-objects",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#r-objects",
    "title": "R Nuts and Bolts",
    "section": "R Objects",
    "text": "R Objects\nThe most basic type of R object is a vector.\n\nVectors\nThere is really only one rule about vectors in R, which is that\n\nA vector can only contain objects of the same class\n\nTo understand what we mean here, we need to dig a little deeper. We will come back this in just a minute.\n\nTypes of vectors\nThere are two types of vectors in R:\n\nAtomic vectors:\n\nlogical: FALSE, TRUE, and NA\ninteger (and doubles): these are known collectively as numeric vectors (or real numbers)\ncomplex: complex numbers\ncharacter: the most complex type of atomic vector, because each element of a character vector is a string, and a string can contain an arbitrary amount of data\nraw: used to store fixed-length sequences of bytes. These are not commonly used directly in data analysis and I won’t cover them here.\n\nLists, which are sometimes called recursive vectors because lists can contain other lists.\n\n\n[Source: R 4 Data Science]\n\n\n\n\n\n\nNote\n\n\n\nThere’s one other related object: NULL.\n\nNULL is often used to represent the absence of a vector (as opposed to NA which is used to represent the absence of a value in a vector).\nNULL typically behaves like a vector of length 0.\n\n\n\n\n\nCreate an empty vector\nEmpty vectors can be created with the vector() function.\n\nvector(mode = \"numeric\", length = 4)\n\n[1] 0 0 0 0\n\nvector(mode = \"logical\", length = 4)\n\n[1] FALSE FALSE FALSE FALSE\n\nvector(mode = \"character\", length = 4)\n\n[1] \"\" \"\" \"\" \"\"\n\n\n\n\nCreating a non-empty vector\nThe c() function can be used to create vectors of objects by concatenating things together.\n\nx <- c(0.5, 0.6)       ## numeric\nx <- c(TRUE, FALSE)    ## logical\nx <- c(T, F)           ## logical\nx <- c(\"a\", \"b\", \"c\")  ## character\nx <- 9:29              ## integer\nx <- c(1+0i, 2+4i)     ## complex\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above example, T and F are short-hand ways to specify TRUE and FALSE.\nHowever, in general, one should try to use the explicit TRUE and FALSE values when indicating logical values.\nThe T and F values are primarily there for when you’re feeling lazy.\n\n\n\n\nLists\nSo, I know I said there is one rule about vectors:\n\nA vector can only contain objects of the same class\n\nBut of course, like any good rule, there is an exception, which is a list (which we will get to in greater details a bit later).\nFor now, just know a list is represented as a vector but can contain objects of different classes. Indeed, that’s usually why we use them.\n\n\n\n\n\n\nNote\n\n\n\nThe main difference between atomic vectors and lists is that atomic vectors are homogeneous, while lists can be heterogeneous.\n\n\n\n\n\nNumerics\nInteger and double vectors are known collectively as numeric vectors.\nIn R, numbers are doubles by default.\nTo make an integer, place an L after the number:\n\ntypeof(4)\n\n[1] \"double\"\n\ntypeof(4L)\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between integers and doubles is not usually important, but there are two important differences that you should be aware of:\n\nDoubles are approximations!\nDoubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory. This means that you should consider all doubles to be approximations.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s explore this. What is square of the square root of two? i.e. \\((\\sqrt{2})^2\\)\n\nx <- sqrt(2) ^ 2\nx\n\n[1] 2\n\n\nTry subtracting 2 from x? What happened?\n\n## try it here\n\n\n\n\n\nNumbers\nNumbers in R are generally treated as numeric objects (i.e. double precision real numbers).\nThis means that even if you see a number like “1” or “2” in R, which you might think of as integers, they are likely represented behind the scenes as numeric objects (so something like “1.00” or “2.00”).\nThis isn’t important most of the time…except when it is!\nIf you explicitly want an integer, you need to specify the L suffix. So entering 1 in R gives you a numeric object; entering 1L explicitly gives you an integer object.\n\n\n\n\n\n\nNote\n\n\n\nThere is also a special number Inf which represents infinity. This allows us to represent entities like 1 / 0. This way, Inf can be used in ordinary calculations; e.g. 1 / Inf is 0.\nThe value NaN represents an undefined value (“not a number”); e.g. 0 / 0; NaN can also be thought of as a missing value (more on that later)\n\n\n\n\nAttributes\nR objects can have attributes, which are like metadata for the object.\nThese metadata can be very useful in that they help to describe the object.\nFor example, column names on a data frame help to tell us what data are contained in each of the columns. Some examples of R object attributes are\n\nnames, dimnames\ndimensions (e.g. matrices, arrays)\nclass (e.g. integer, numeric)\nlength\nother user-defined attributes/metadata\n\nAttributes of an object (if any) can be accessed using the attributes() function. Not all R objects contain attributes, in which case the attributes() function returns NULL.\nHowever, every vector has two key properties:\n\nIts type, which you can determine with typeof().\n\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\ntypeof(letters)\n\n[1] \"character\"\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ntypeof(1:10)\n\n[1] \"integer\"\n\n\n\nIts length, which you can determine with length().\n\n\nx <- list(\"a\", \"b\", 1:10)\nx\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] \"b\"\n\n[[3]]\n [1]  1  2  3  4  5  6  7  8  9 10\n\nlength(x)\n\n[1] 3\n\ntypeof(x)\n\n[1] \"list\"\n\nattributes(x)\n\nNULL"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#mixing-objects",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#mixing-objects",
    "title": "R Nuts and Bolts",
    "section": "Mixing Objects",
    "text": "Mixing Objects\nThere are occasions when different classes of R objects get mixed together.\nSometimes this happens by accident but it can also happen on purpose.\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s use typeof() to ask what happens when we mix different classes of R objects together.\n\ny <- c(1.7, \"a\")\ny <- c(TRUE, 2)\ny <- c(\"a\", TRUE)\n\n\n## try it here\n\n\n\nWhy is this happening?\nIn each case above, we are mixing objects of two different classes in a vector.\nBut remember that the only rule about vectors says this is not allowed?\nWhen different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class.\nIn the example above, we see the effect of implicit coercion.\nWhat R tries to do is find a way to represent all of the objects in the vector in a reasonable fashion. Sometimes this does exactly what you want and…sometimes not.\nFor example, combining a numeric object with a character object will create a character vector, because numbers can usually be easily represented as strings."
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#explicit-coercion",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#explicit-coercion",
    "title": "R Nuts and Bolts",
    "section": "Explicit Coercion",
    "text": "Explicit Coercion\nObjects can be explicitly coerced from one class to another using the as.*() functions, if available.\n\nx <- 0:6\nclass(x)\n\n[1] \"integer\"\n\nas.numeric(x)\n\n[1] 0 1 2 3 4 5 6\n\nas.logical(x)\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nas.character(x)\n\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n\nSometimes, R can’t figure out how to coerce an object and this can result in NAs being produced.\n\nx <- c(\"a\", \"b\", \"c\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA\n\nas.logical(x)\n\n[1] NA NA NA\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s try to convert the x vector above to integers.\n\n## try it here \n\n\n\nWhen nonsensical coercion takes place, you will usually get a warning from R."
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#matrices",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#matrices",
    "title": "R Nuts and Bolts",
    "section": "Matrices",
    "text": "Matrices\nMatrices are vectors with a dimension attribute.\n\nThe dimension attribute is itself an integer vector of length 2 (number of rows, number of columns)\n\n\nm <- matrix(nrow = 2, ncol = 3) \nm\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\ndim(m)\n\n[1] 2 3\n\nattributes(m)\n\n$dim\n[1] 2 3\n\n\nMatrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns.\n\nm <- matrix(1:6, nrow = 2, ncol = 3) \nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s try to use attributes() function to look at the attributes of the m object\n\n## try it here \n\n\n\nMatrices can also be created directly from vectors by adding a dimension attribute.\n\nm <- 1:10 \nm\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ndim(m) <- c(2, 5)\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n\nMatrices can be created by column-binding or row-binding with the cbind() and rbind() functions.\n\nx <- 1:3\ny <- 10:12\ncbind(x, y)\n\n     x  y\n[1,] 1 10\n[2,] 2 11\n[3,] 3 12\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s try to use rbind() to row bind x and y above.\n\n## try it here"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#lists-1",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#lists-1",
    "title": "R Nuts and Bolts",
    "section": "Lists",
    "text": "Lists\nLists are a special type of vector that can contain elements of different classes. Lists are a very important data type in R and you should get to know them well.\n\n\n\n\n\n\nPro-tip\n\n\n\nLists, in combination with the various “apply” functions discussed later, make for a powerful combination.\n\n\nLists can be explicitly created using the list() function, which takes an arbitrary number of arguments.\n\nx <- list(1, \"a\", TRUE, 1 + 4i) \nx\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nWe can also create an empty list of a prespecified length with the vector() function\n\nx <- vector(\"list\", length = 5)\nx\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#factors",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#factors",
    "title": "R Nuts and Bolts",
    "section": "Factors",
    "text": "Factors\nFactors are used to represent categorical data and can be unordered or ordered. One can think of a factor as an integer vector where each integer has a label.\n\n\n\n\n\n\nPro-tip\n\n\n\nFactors are important in statistical modeling and are treated specially by modelling functions like lm() and glm().\n\n\nUsing factors with labels is better than using integers because factors are self-describing.\n\n\n\n\n\n\nPro-tip\n\n\n\nHaving a variable that has values “Yes” and “No” or “Smoker” and “Non-Smoker” is better than a variable that has values 1 and 2.\n\n\nFactor objects can be created with the factor() function.\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \nx\n\n[1] yes yes no  yes no \nLevels: no yes\n\ntable(x) \n\nx\n no yes \n  2   3 \n\n## See the underlying representation of factor\nunclass(x)  \n\n[1] 2 2 1 2 1\nattr(,\"levels\")\n[1] \"no\"  \"yes\"\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s try to use attributes() function to look at the attributes of the x object\n\n## try it here \n\n\n\nOften factors will be automatically created for you when you read in a dataset using a function like read.table().\n\nThose functions often default to creating factors when they encounter data that look like characters or strings.\n\nThe order of the levels of a factor can be set using the levels argument to factor(). This can be important in linear modeling because the first level is used as the baseline level.\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"))\nx  ## Levels are put in alphabetical order\n\n[1] yes yes no  yes no \nLevels: no yes\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"),\n            levels = c(\"yes\", \"no\"))\nx\n\n[1] yes yes no  yes no \nLevels: yes no"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#missing-values",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#missing-values",
    "title": "R Nuts and Bolts",
    "section": "Missing Values",
    "text": "Missing Values\nMissing values are denoted by NA or NaN for undefined mathematical operations.\n\nis.na() is used to test objects if they are NA\nis.nan() is used to test for NaN\nNA values have a class also, so there are integer NA, character NA, etc.\nA NaN value is also NA but the converse is not true\n\n\n## Create a vector with NAs in it\nx <- c(1, 2, NA, 10, 3)  \n## Return a logical vector indicating which elements are NA\nis.na(x)    \n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n## Return a logical vector indicating which elements are NaN\nis.nan(x)   \n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\n## Now create a vector with both NA and NaN values\nx <- c(1, 2, NaN, NA, 4)\nis.na(x)\n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\nis.nan(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#data-frames",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#data-frames",
    "title": "R Nuts and Bolts",
    "section": "Data Frames",
    "text": "Data Frames\nData frames are used to store tabular data in R. They are an important type of object in R and are used in a variety of statistical modeling applications. Hadley Wickham’s package dplyr has an optimized set of functions designed to work efficiently with data frames.\nData frames are represented as a special type of list where every element of the list has to have the same length.\n\nEach element of the list can be thought of as a column\nThe length of each element of the list is the number of rows\n\nUnlike matrices, data frames can store different classes of objects in each column. Matrices must have every element be the same class (e.g. all integers or all numeric).\nIn addition to column names, indicating the names of the variables or predictors, data frames have a special attribute called row.names which indicate information about each row of the data frame.\nData frames are usually created by reading in a dataset using the read.table() or read.csv(). However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists.\n\nx <- data.frame(foo = 1:4, bar = c(T, T, F, F)) \nx\n\n  foo   bar\n1   1  TRUE\n2   2  TRUE\n3   3 FALSE\n4   4 FALSE\n\nnrow(x)\n\n[1] 4\n\nncol(x)\n\n[1] 2\n\nattributes(x)\n\n$names\n[1] \"foo\" \"bar\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2 3 4\n\n\nData frames can be converted to a matrix by calling data.matrix(). While it might seem that the as.matrix() function should be used to coerce a data frame to a matrix, almost always, what you want is the result of data.matrix().\n\ndata.matrix(x)\n\n     foo bar\n[1,]   1   1\n[2,]   2   1\n[3,]   3   0\n[4,]   4   0\n\nattributes(data.matrix(x))\n\n$dim\n[1] 4 2\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"foo\" \"bar\"\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLet’s use the palmerpenguins dataset.\n\nWhat attributes does penguins have?\nTry using typeof() with penguins.\nWhat are the levels in the species column in the penguins dataset?\nCreate a logical vector for all the penguins measured from 2008.\nCreate a matrix with just the columns bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g\n\n\n# try it yourself\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins \n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g"
  },
  {
    "objectID": "posts/2022-09-20-r-nuts-and-bolts/index.html#names",
    "href": "posts/2022-09-20-r-nuts-and-bolts/index.html#names",
    "title": "R Nuts and Bolts",
    "section": "Names",
    "text": "Names\nR objects can have names, which is very useful for writing readable code and self-describing objects.\nHere is an example of assigning names to an integer vector.\n\nx <- 1:3\nnames(x)\n\nNULL\n\nnames(x) <- c(\"New York\", \"Seattle\", \"Los Angeles\") \nx\n\n   New York     Seattle Los Angeles \n          1           2           3 \n\nnames(x)\n\n[1] \"New York\"    \"Seattle\"     \"Los Angeles\"\n\nattributes(x)\n\n$names\n[1] \"New York\"    \"Seattle\"     \"Los Angeles\"\n\n\nLists can also have names, which is often very useful.\n\nx <- list(\"Los Angeles\" = 1, Boston = 2, London = 3) \nx\n\n$`Los Angeles`\n[1] 1\n\n$Boston\n[1] 2\n\n$London\n[1] 3\n\nnames(x)\n\n[1] \"Los Angeles\" \"Boston\"      \"London\"     \n\n\nMatrices can have both column and row names.\n\nm <- matrix(1:4, nrow = 2, ncol = 2)\ndimnames(m) <- list(c(\"a\", \"b\"), c(\"c\", \"d\")) \nm\n\n  c d\na 1 3\nb 2 4\n\n\nColumn names and row names can be set separately using the colnames() and rownames() functions.\n\ncolnames(m) <- c(\"h\", \"f\")\nrownames(m) <- c(\"x\", \"z\")\nm\n\n  h f\nx 1 3\nz 2 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor data frames, there is a separate function for setting the row names, the row.names() function.\nAlso, data frames do not have column names, they just have names (like lists).\nSo to set the column names of a data frame just use the names() function. Yes, I know its confusing.\nHere’s a quick summary:\n\n\n\nObject\nSet column names\nSet row names\n\n\n\n\ndata frame\nnames()\nrow.names()\n\n\nmatrix\ncolnames()\nrownames()"
  },
  {
    "objectID": "posts/2022-09-01-reproducible-research/index.html",
    "href": "posts/2022-09-01-reproducible-research/index.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "[Link to Claerbout and Karrenbach (1992) article]\n\nPre-lecture materials\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://ropensci.github.io/reproducibility-guide/sections/introduction\nhttps://rdpeng.github.io/Biostat776\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\n\n\n\n\nIntroduction\nThis lecture will be about reproducible reporting, and I want to take the opportunity to cover some basic concepts and ideas that are related to reproducible reporting, just in case you have not heard about it or don not know what it is.\nBefore we get to reproducibility, we need to cover a little background with respect to how science works (even if you are not a scientist, this is important).\nThe ultimate standard in strengthening scientific evidence is replication. Assume you claim that X causes Y, or that Vitamin C improves disease. The goal of replication is to have independent people to do independent things with different data, different methods, and different laboratories and see if they get the same result.\nThere is a sense that if a relationship in nature is truly there, then it should be robust to having different people discover it in different ways. Replication is particularly important in areas where findings can have big policy impacts or can influence regulatory types of decisions.\n\nWhat is wrong with replication?\nThere is really nothing wrong with it. This is what science has been doing for a long time, through hundreds of years. And there is nothing wrong with it today.\nBut the problem is that it is becoming more and more challenging to do replication or to replicate other studies.\nHere are some reasons:\n\nOften times studies are much larger and more costly than previously. If you want to do ten versions of the same study, you need ten times as much money and there is not as much money around as there used to be.\nSometimes it is difficult to replicate a study because if the original study took 20 years to complete, it is difficult to wait around another 20 years for replication.\nSome studies are just plain unique, such as studying the impact of a massive earthquake in a very specific location and time. If you are looking at a unique situation in time or a unique population, you cannot readily replicate that situation.\n\nThere are a lot of good reasons why you cannot replicate a study. If you cannot replicate a study, is the alternative just to do nothing (?? 😱), just let that study stand by itself?\nThe idea behind a reproducible reporting is to create a kind of minimum standard (or a middle ground) where we will not be replicating a study, but maybe we can do something in between. What can we do that’s in between the gold standard and doing nothing?\nThat is where reproducibility comes in. That’s how we can kind of bridge the gap between replication and nothing.\nIn non-research settings, often full replication is not even the point. Often the goal is to preserve something to the point where anybody in an organization can repeat what you did (for example, after you leave the organization).\n\nIn this case, reproducibility is key to maintaining the history of a project and making sure that every step along the way is clear.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nReplication, whereby scientific questions are examined and verified independently by different scientists, is the gold standard for scientific validity.\nReplication can be difficult and often there are no resources to independently replicate a study.\nReproducibility, whereby data and code are re-analyzed by independent scientists to obtain the same results of the original investigator, is a reasonable minimum standard when replication is not possible.\n\n\n\n\n\n\nReproducibility to the Rescue\nLet’s first define reproducibility. The basic idea is that you need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you have run, and come to the same findings that you found.\nWhat reproducible reporting is about is a validation of the data analysis (not the original question itself). Because you are not collecting independent data using independent methods, it is a little bit more difficult to validate the scientific question itself. But if you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nIn this way, you can at least have confidence that you can reproduce the analysis.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\n\nhttps://www.science.org/toc/science/334/6060\n\nOther journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\n\nhttps://jasa-acs.github.io/repro-guide\n\n\nWhy does this matter?\nHere is an example. In 2012, a feature on the TV show 60 minutes looked at a major incident at Duke University where many results involving a promising cancer test were found to be not reproducible. This led to a number of studies and clinical trials having to be stopped, followed by an investigation which is still ongoing.\n\n\n\n\n[Source on YouTube]\n\n\nTypes of reproducibility\nWhat are the different kinds of reproducible research? Enabling reproducibility can be complicated, but by separating out some of the levels and degrees of reproducibility the problem can become more manageable because we can focus our efforts on what best suits our specific scientific domain. Victoria Stodden (2014), a prominent scholar on this topic, has identified some useful distinctions in reproducible research:\n\nComputational reproducibility: when detailed information is provided about code, software, hardware and implementation details.\nEmpirical reproducibility: when detailed information is provided about non-computational empirical scientific experiments and observations. In practice this is enabled by making data freely available, as well as details of how the data was collected.\nStatistical reproducibility: when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. This mostly relates to pre-registration of study design to prevent p-value hacking and other manipulations.\n\n[Source]\n\n\nElements of computational reproducibility\nWhat do we need for computational reproducibility? There are a variety of ways to talk about this, but one basic definition that we hae come up with is that there are four things that are required to make results reproducible:\n\nAnalytic data. The data that were used for the analysis that was presented should be available for others to access. This is different from the raw data because very often in a data analysis the raw data are not all used for the analysis, but rather some subset is used. It may be interesting to see the raw data but impractical to actually have it. Analytic data is key to examining the data analysis.\nAnalytic code. The analytic code is the code that was applied to the analytic data to produce the key results. This may be preprocessing code, regression modeling code, or really any other code used to produce the results from the analytic data.\nDocumentation. Documentation of that code and the data is very important.\nDistribution. Finally, there needs to be some standard means of distribution, so all this data in the code is easily accessible.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nReproducible reporting is about is a validation of the data analysis\nThere are multiple types of reproducibility\nThere are four elements to computational reproducibility\n\n\n\n\n\n\n“X” to “Computational X”\nWhat is driving this need for a “reproducibility middle ground” between replication and doing nothing?\nFor starters, there are a lot of new technologies on the scene and in many different fields of study including, biology, chemistry and environmental science. These technologies allow us to collect data at a much higher throughput so we end up with these very complex and very high dimensional data sets.\nThese datasets can be collected almost instantaneously compared to even just ten years ago—the technology has allowed us to create huge data sets at essentially the touch of a button. Furthermore, we the computing power to take existing (already huge) databases and merge them into even bigger and bigger databases. Finally, the massive increase in computing power has allowed us to implement more sophisticated and complex analysis routines.\nThe analyses themselves, the models that we fit and the algorithms that we run, are much much more complicated than they used to be. Having a basic understanding of these algorithms is difficult, even for a sophisticated person, and it is almost impossible to describe these algorithms with words alone.\nUnderstanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used.\nThe bottom line with all these different trends is that for every field “X”, there is now “Computational X”. There’s computational biology, computational astronomy—whatever it is you want, there is a computational version of it.\n\nExample: machine learning in the life sciences\nOne example of an area where reproducibility is important comes from research that I have conducted in the area of machine learning in the life sciences.\nIn the above article, computational reproducibility is not throught of as a binary property, but rather it is on a sliding scale that reflects the time needed to reproduce. Published works fall somewhere on this scale, which is bookended by ‘forever’, for a completely irreproducible work, and ‘zero’, for a work where one can automatically repeat the entire analysis with a single keystroke.\nAs in many cases it is difficult to impose a single standard that divides work into ‘reproducible’ and ‘irreproducible’. Therefore, instead a menu is proposed of three standards with varying degrees of rigor for computational reproducibility:\n\nBronze standard. The authors make the data, models and code used in the analysis publicly available. The bronze standard is the minimal standard for reproducibility. Without data, models and code, it is not possible to reproduce a work.\nSilver standard. In addition to meeting the bronze standard: (1) the dependencies of the analysis can be downloaded and installed in a single command; (2) key details for reproducing the work are documented, including the order in which to run the analysis scripts, the operating system used and system resource requirements; and (3) all random components in the analysis are set to be deterministic. The silver standard is a midway point between minimal availability and full automation. Works that meet this standard will take much less time to reproduce than ones only meeting the bronze standard.\nGold standard. The work meets the silver standard, and the authors make the analysis reproducible with a single command. The gold standard for reproducibility is full automation. When a work meets this standard, it will take little to no effort for a scientist to reproduce it.\n\n\n\n\nThe Data Science Pipeline\nThe basic issue is when you read a description of a data analysis, such as in an article or a technical report, for the most part, what you get is the report and nothing else.\nOf course, everyone knows that behind the scenes there’s a lot that went into this article and that is what I call the data science pipeline.\n\n\n\nThe Data Science Pipeline\n\n\nIn this pipeline, there are two “actors”: the author of the report/article and the reader.\n\nOn the left side, the author is going from left to right along this pipeline.\nOn the right side, the reader is going from right to left.\n\nIf you are the reader, you read the article, and you may want to know more about what happened e.g.\n\nWhere are the data?\nWhat methods were used here?\n\nThe basic idea behind computational reproducibility is to focus on the elements in the blue box: the analytic data and the computational results. With computational reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”.\n\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience.\n\nPublishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely).\nResources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\n\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together.\n\nReaders may not have the same computational resources that the original authors did.\nIf the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\n\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing.\n\nIn practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized.\nThere are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\n\n\n\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat is the difference between replication and reproducible?\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?"
  },
  {
    "objectID": "posts/2022-09-13-plotting-systems/index.html",
    "href": "posts/2022-09-13-plotting-systems/index.html",
    "title": "Plotting Systems",
    "section": "",
    "text": "The data may not contain the answer. And, if you torture the data long enough, it will tell you anything. —John W. Tukey"
  },
  {
    "objectID": "posts/2022-09-13-plotting-systems/index.html#the-base-plotting-system",
    "href": "posts/2022-09-13-plotting-systems/index.html#the-base-plotting-system",
    "title": "Plotting Systems",
    "section": "The Base Plotting System",
    "text": "The Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model.\nThe idea is you start with blank canvas and build up from there.\nIn more R-specific terms, you typically start with plot() function (or similar plot creating function) to initiate a plot and then annotate the plot with various annotation functions (text, lines, points, axis)\nThe base plotting system is often the most convenient plotting system to use because it mirrors how we sometimes think of building plots and analyzing data.\nIf we do not have a completely well-formed idea of how we want to look at some data, often we will start by “throwing some data on the page” and then slowly add more information to it as our thought process evolves.\n\n\n\n\n\n\nExample\n\n\n\nWe might look at a simple scatterplot and then decide to add a linear regression line or a smoother to it to highlight the trends.\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nScatterplot with loess curve\n\n\n\n\n\n\nIn the code above:\n\nThe plot() function creates the initial plot and draws the points (circles) on the canvas.\nThe lines function is used to annotate or add to the plot (in this case it adds a loess smoother to the scatterplot).\n\nNext, we use the plot() function to draw the points on the scatterplot and then use the main argument to add a main title to the plot.\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone, main = \"my plot\")\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nScatterplot with loess curve\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOne downside with constructing base plots is that you cannot go backwards once the plot has started.\nIt is possible that you could start down the road of constructing a plot and realize later (when it is too late) that you do not have enough room to add a y-axis label or something like that\n\n\nIf you have specific plot in mind, there is then a need to plan in advance to make sure, for example, that you have set your margins to be the right size to fit all of the annotations that you may want to include.\nWhile the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you.\n\n\n\n\n\n\nNote\n\n\n\nAnother downside of the base plotting system is that it is difficult to describe or translate a plot to others because there is no clear graphical language or grammar that can be used to communicate what you have done.\nThe only real way to describe what you have done in a base plot is to just list the series of commands/functions that you have executed, which is not a particularly compact way of communicating things.\nThis is one problem that the ggplot2 package attempts to address.\n\n\n\n\n\n\n\n\nExample\n\n\n\nAnother typical base plot is constructed with the following code.\n\ndata(cars)\n\n## Create the plot / draw canvas\nwith(cars, plot(speed, dist))\n\n## Add annotation\ntitle(\"Speed vs. Stopping distance\")\n\n\n\n\nBase plot with title\n\n\n\n\n\n\nWe will go into more detail on what these functions do in later lessons."
  },
  {
    "objectID": "posts/2022-09-13-plotting-systems/index.html#the-lattice-system",
    "href": "posts/2022-09-13-plotting-systems/index.html#the-lattice-system",
    "title": "Plotting Systems",
    "section": "The Lattice System",
    "text": "The Lattice System\nThe lattice plotting system is implemented in the lattice R package which comes with every installation of R (although it is not loaded by default).\nTo use the lattice plotting functions, you must first load the lattice package with the library function.\n\nlibrary(lattice)\n\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot().\nThere is no real distinction between functions that create or initiate plots and functions that annotate plots because it all happens at once.\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z.\n\ne.g. these types of plots are useful for looking at multi-dimensional data and often allow you to squeeze a lot of information into a single window or page.\n\nAnother aspect of lattice that makes it different from base plotting is that things like margins and spacing are set automatically.\nThis is possible because entire plot is specified at once via a single function call, so all of the available information needed to figure out the spacing and margins is already there.\n\n\n\n\n\n\nExample\n\n\n\nHere is a lattice plot that looks at the relationship between life expectancy and income and how that relationship varies by region in the United States.\n\nstate <- data.frame(state.x77, region = state.region)\nxyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))\n\n\n\n\nLattice plot\n\n\n\n\n\n\nYou can see that the entire plot was generated by the call to xyplot() and all of the data for the plot were stored in the state data frame.\nThe plot itself contains four panels—one for each region—and within each panel is a scatterplot of life expectancy and income.\nThe notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a condition, like “region”).\n\n\n\n\n\n\nNote\n\n\n\nDownsides with the lattice system\n\nIt can sometimes be very awkward to specify an entire plot in a single function call (you end up with functions with many many arguments).\nAnnotation in panels in plots is not especially intuitive and can be difficult to explain. In particular, the use of custom panel functions and subscripts can be difficult to wield and requires intense preparation.\nOnce a plot is created, you cannot “add” to the plot (but of course you can just make it again with modifications)."
  },
  {
    "objectID": "posts/2022-09-13-plotting-systems/index.html#the-ggplot2-system",
    "href": "posts/2022-09-13-plotting-systems/index.html#the-ggplot2-system",
    "title": "Plotting Systems",
    "section": "The ggplot2 System",
    "text": "The ggplot2 System\nThe ggplot2 plotting system attempts to split the difference between base and lattice in a number of ways.\n\n\n\n\n\n\nNote\n\n\n\nTaking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot.\n\n\nThe ggplot2 system is implemented in the ggplot2 package (part of the tidyverse package), which is available from CRAN (it does not come with R).\nYou can install it from CRAN via\n\ninstall.packages(\"ggplot2\")\n\nand then load it into R via the library() function.\n\nlibrary(ggplot2)\n\nSuperficially, the ggplot2 functions are similar to lattice, but the system is generally easier and more intuitive to use.\nThe defaults used in ggplot2 make many choices for you, but you can still customize plots to your heart’s desire.\n\n\n\n\n\n\nExample\n\n\n\nA typical plot with the ggplot2 package looks as follows.\n\nlibrary(tidyverse)\ndata(mpg)\nmpg %>%\n  ggplot(aes(displ, hwy)) + \n  geom_point()\n\n\n\n\nggplot2 plot\n\n\n\n\n\n\nThere are additional functions in ggplot2 that allow you to make arbitrarily sophisticated plots.\nWe will discuss more about this in the next lecture."
  },
  {
    "objectID": "posts/2022-09-01-reference-management/index.html",
    "href": "posts/2022-09-01-reference-management/index.html",
    "title": "Reference management",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nAuthoring in R Markdown from RStudio\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nBibliography from R Markdown Cookbook\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html"
  },
  {
    "objectID": "posts/2022-09-01-reference-management/index.html#rtistry",
    "href": "posts/2022-09-01-reference-management/index.html#rtistry",
    "title": "Reference management",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n[Add here.]"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html",
    "title": "Reading and Writing data",
    "section": "",
    "text": "[Source]"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#txt-or-csv",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#txt-or-csv",
    "title": "Reading and Writing data",
    "section": "txt or csv",
    "text": "txt or csv\nThere are a few primary functions reading data from base R.\n\nread.table(), read.csv(): for reading tabular data\nreadLines(): for reading lines of a text file\n\nThere are analogous functions for writing data to files\n\nwrite.table(): for writing tabular data to text files (i.e. CSV) or connections\nwriteLines(): for writing character data line-by-line to a file or connection\n\nLet’s try reading some data into R with the read.csv() function.\n\ndf <- read.csv(here(\"data\", \"team_standings.csv\"))\ndf\n\n   Standing         Team\n1         1        Spain\n2         2  Netherlands\n3         3      Germany\n4         4      Uruguay\n5         5    Argentina\n6         6       Brazil\n7         7        Ghana\n8         8     Paraguay\n9         9        Japan\n10       10        Chile\n11       11     Portugal\n12       12          USA\n13       13      England\n14       14       Mexico\n15       15  South Korea\n16       16     Slovakia\n17       17  Ivory Coast\n18       18     Slovenia\n19       19  Switzerland\n20       20 South Africa\n21       21    Australia\n22       22  New Zealand\n23       23       Serbia\n24       24      Denmark\n25       25       Greece\n26       26        Italy\n27       27      Nigeria\n28       28      Algeria\n29       29       France\n30       30     Honduras\n31       31     Cameroon\n32       32  North Korea\n\n\nWe can use the $ symbol to pick out a specific column:\n\ndf$Team\n\n [1] \"Spain\"        \"Netherlands\"  \"Germany\"      \"Uruguay\"      \"Argentina\"   \n [6] \"Brazil\"       \"Ghana\"        \"Paraguay\"     \"Japan\"        \"Chile\"       \n[11] \"Portugal\"     \"USA\"          \"England\"      \"Mexico\"       \"South Korea\" \n[16] \"Slovakia\"     \"Ivory Coast\"  \"Slovenia\"     \"Switzerland\"  \"South Africa\"\n[21] \"Australia\"    \"New Zealand\"  \"Serbia\"       \"Denmark\"      \"Greece\"      \n[26] \"Italy\"        \"Nigeria\"      \"Algeria\"      \"France\"       \"Honduras\"    \n[31] \"Cameroon\"     \"North Korea\" \n\n\nWe can also ask for the full paths for specific files\n\nhere(\"data\", \"team_standings.csv\")\n\n[1] \"/Users/stephaniehicks/Documents/github/teaching/jhustatcomputing2022/data/team_standings.csv\"\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat happens when you use readLines() function with the team_standings.csv data?\nHow would you only read in the first 5 lines?"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#r-code",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#r-code",
    "title": "Reading and Writing data",
    "section": "R code",
    "text": "R code\nSometimes, someone will give you a file that ends in a .R.\nThis is what’s called an R script file. It may contain code someone has written (maybe even you!), for example, a function that you can use with your data. In this case, you want the function available for you to use.\nTo use the function, you have to first, read in the function from R script file into R.\nYou can check to see if the function already is loaded in R by looking at the Environment tab.\nThe function you want to use is\n\nsource(): for reading in R code files\n\nFor example, it might be something like this:\n\nsource(here::here('functions.R'))"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#r-objects",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#r-objects",
    "title": "Reading and Writing data",
    "section": "R objects",
    "text": "R objects\nAlternatively, you might be interested in reading and writing R objects.\nWriting data in e.g. .txt, .csv or Excel file formats is good if you want to open these files with other analysis software, such as Excel. However, these formats do not preserve data structures, such as column data types (numeric, character or factor). In order to do that, the data should be written out in a R data format.\nThere are several types R data file formats to be aware of:\n\n.RData: Stores multiple R objects\n.Rda: This is short for .RData and is equivalent.\n.Rds: Stores a single R object\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is saving data in as a R object useful?\nSaving data into R data formats can typically reduce considerably the size of large files by compression.\n\n\nNext, we will learn how to read and save\n\nA single R object\nMultiple R objects\nYour entire work space in a specified file\n\n\nReading in data from files\n\nload(): for reading in single or multiple R objects (opposite of save()) with a .Rda or .RData file format (objects must be same name)\nreadRDS(): for reading in a single object with a .Rds file format (can rename objects)\nunserialize(): for reading single R objects in binary form\n\n\n\nWriting data to files\n\nsave(): for saving an arbitrary number of R objects in binary format (possibly compressed) to a file.\nsaveRDS(): for saving a single object\nserialize(): for converting an R object into a binary format for outputting to a connection (or file).\nsave.image(): short for ‘save my current workspace’; while this sounds nice, it’s not terribly useful for reproducibility (hence not suggested); it’s also what happens when you try to quit R and it asks if you want to save your work space.\n\n\n\n\n\n\nSave data into R data file formats: RDS and RDATA\n\n\n\n\n[Source]\n\n\nExample\nLet’s try an example. Let’s save a vector of length 5 into the two file formats.\n\nx <- 1:5\nsave(x, file=here(\"data\", \"x.Rda\"))\nsaveRDS(x, file=here(\"data\", \"x.Rds\"))\nlist.files(path=here(\"data\"))\n\n[1] \"2016-07-19.csv.bz2\" \"chicago.rds\"        \"chocolate.RDS\"     \n[4] \"team_standings.csv\" \"x.Rda\"              \"x.Rds\"             \n\n\nHere we assign the imported data to an object using readRDS()\n\nnew_x1 <- readRDS(here(\"data\", \"x.Rds\"))\nnew_x1\n\n[1] 1 2 3 4 5\n\n\nHere we assign the imported data to an object using load()\n\nnew_x2 <- load(here(\"data\", \"x.Rda\"))\nnew_x2\n\n[1] \"x\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nload() simply returns the name of the objects loaded. Not the values.\n\n\nLet’s clean up our space.\n\nfile.remove(here(\"data\", \"x.Rda\"))\n\n[1] TRUE\n\nfile.remove(here(\"data\", \"x.Rds\"))\n\n[1] TRUE\n\nrm(x)\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think this code will do?\nHint: change eval=TRUE to see result\n\nx <- 1:5\ny <- x^2\nsave(x,y, file=here(\"data\", \"x.Rda\"))\nnew_x2 <- load(here(\"data\", \"x.Rda\"))\n\nWhen you are done:\n\nfile.remove(here(\"data\", \"x.Rda\"))"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#other-data-types",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#other-data-types",
    "title": "Reading and Writing data",
    "section": "Other data types",
    "text": "Other data types\nNow, there are of course, many R packages that have been developed to read in all kinds of other datasets, and you may need to resort to one of these packages if you are working in a specific area.\nFor example, check out\n\nDBI for relational databases\nhaven for SPSS, Stata, and SAS data\nhttr for web APIs\nreadxl for .xls and .xlsx sheets\ngooglesheets4 for Google Sheets\ngoogledrive for Google Drive files\nrvest for web scraping\njsonlite for JSON\nxml2 for XML."
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#reading-data-files-with-read.table",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#reading-data-files-with-read.table",
    "title": "Reading and Writing data",
    "section": "Reading data files with read.table()",
    "text": "Reading data files with read.table()\n\n\nFor details on reading data with read.table(), click here.\n\nThe read.table() function is one of the most commonly used functions for reading data. The help file for read.table() is worth reading in its entirety if only because the function gets used a lot (run ?read.table in R).\nI know, I know, everyone always says to read the help file, but this one is actually worth reading.\nThe read.table() function has a few important arguments:\n\nfile, the name of a file, or a connection\nheader, logical indicating if the file has a header line\nsep, a string indicating how the columns are separated\ncolClasses, a character vector indicating the class of each column in the dataset\nnrows, the number of rows in the dataset. By default read.table() reads an entire file.\ncomment.char, a character string indicating the comment character. This defaults to \"#\". If there are no commented lines in your file, it’s worth setting this to be the empty string \"\".\nskip, the number of lines to skip from the beginning\nstringsAsFactors, should character variables be coded as factors? This defaults to FALSE. However, back in the “old days”, it defaulted to TRUE. The reason for this was because, if you had data that were stored as strings, it was because those strings represented levels of a categorical variable. Now, we have lots of data that is text data and they do not always represent categorical variables. So you may want to set this to be FALSE in those cases. If you always want this to be FALSE, you can set a global option via options(stringsAsFactors = FALSE).\n\nI’ve never seen so much heat generated on discussion forums about an R function argument than the stringsAsFactors argument. Seriously.\nFor small to moderately sized datasets, you can usually call read.table() without specifying any other arguments\n\ndata <- read.table(\"foo.txt\")\n\n\n\n\n\n\n\nNote\n\n\n\nfoo.txt is not a real dataset here. It is only used as an example for how to use read.table()\n\n\nIn this case, R will automatically:\n\nskip lines that begin with a #\nfigure out how many rows there are (and how much memory needs to be allocated)\nfigure what type of variable is in each column of the table.\n\nTelling R all these things directly makes R run faster and more efficiently.\n\n\n\n\n\n\nNote\n\n\n\nThe read.csv() function is identical to read.table() except that some of the defaults are set differently (like the sep argument)."
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#reading-in-larger-datasets-with-read.table",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#reading-in-larger-datasets-with-read.table",
    "title": "Reading and Writing data",
    "section": "Reading in larger datasets with read.table()",
    "text": "Reading in larger datasets with read.table()\n\n\nFor details on reading larger datasets with read.table(), click here.\n\nWith much larger datasets, there are a few things that you can do that will make your life easier and will prevent R from choking.\n\nRead the help page for read.table(), which contains many hints\nMake a rough calculation of the memory required to store your dataset (see the next section for an example of how to do this). If the dataset is larger than the amount of RAM on your computer, you can probably stop right here.\nSet comment.char = \"\" if there are no commented lines in your file.\nUse the colClasses argument. Specifying this option instead of using the default can make read.table() run MUCH faster, often twice as fast. In order to use this option, you have to know the class of each column in your data frame. If all of the columns are “numeric”, for example, then you can just set colClasses = \"numeric\". A quick an dirty way to figure out the classes of each column is the following:\n\n\ninitial <- read.table(\"datatable.txt\", nrows = 100)\nclasses <- sapply(initial, class)\ntabAll <- read.table(\"datatable.txt\", colClasses = classes)\n\nNote: datatable.txt is not a real dataset here. It is only used as an example for how to use read.table().\n\nSet nrows. This does not make R run faster but it helps with memory usage. A mild overestimate is okay. You can use the Unix tool wc to calculate the number of lines in a file.\n\nIn general, when using R with larger datasets, it’s also useful to know a few things about your system.\n\nHow much memory is available on your system?\nWhat other applications are in use? Can you close any of them?\nAre there other users logged into the same system?\nWhat operating system ar you using? Some operating systems can limit the amount of memory a single process can access"
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#advantages",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#advantages",
    "title": "Reading and Writing data",
    "section": "Advantages",
    "text": "Advantages\nThe advantage of the read_csv() function is perhaps better understood from an historical perspective.\n\nR’s built in read.csv() function similarly reads CSV files, but the read_csv() function in readr builds on that by removing some of the quirks and “gotchas” of read.csv() as well as dramatically optimizing the speed with which it can read data into R.\nThe read_csv() function also adds some nice user-oriented features like a progress meter and a compact method for specifying column types."
  },
  {
    "objectID": "posts/2022-09-06-reading-and-writing-data/index.html#example-1",
    "href": "posts/2022-09-06-reading-and-writing-data/index.html#example-1",
    "title": "Reading and Writing data",
    "section": "Example",
    "text": "Example\nA typical call to read_csv() will look as follows.\n\nlibrary(readr)\nteams <- read_csv(here(\"data\", \"team_standings.csv\"))\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Team\ndbl (1): Standing\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nteams\n\n# A tibble: 32 × 2\n   Standing Team       \n      <dbl> <chr>      \n 1        1 Spain      \n 2        2 Netherlands\n 3        3 Germany    \n 4        4 Uruguay    \n 5        5 Argentina  \n 6        6 Brazil     \n 7        7 Ghana      \n 8        8 Paraguay   \n 9        9 Japan      \n10       10 Chile      \n# … with 22 more rows\n\n\nBy default, read_csv() will open a CSV file and read it in line-by-line. Similar to read.table(), you can tell the function to skip lines or which lines are comments:\n\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2)\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n\nAlternatively, you can use the comment argument:\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\")\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n\nIt will also (by default), read in the first few rows of the table in order to figure out the type of each column (i.e. integer, character, etc.). From the read_csv() help page:\n\nIf ‘NULL’, all column types will be imputed from the first 1000 rows on the input. This is convenient (and fast), but not robust. If the imputation fails, you’ll need to supply the correct types yourself.\n\nYou can specify the type of each column with the col_types argument.\n\n\n\n\n\n\nNote\n\n\n\nIn general, it is a good idea to specify the column types explicitly.\nThis rules out any possible guessing errors on the part of read_csv().\nAlso, specifying the column types explicitly provides a useful safety check in case anything about the dataset should change without you knowing about it.\n\n\nHere is an example of how to specify the column types explicitly:\n\nteams <- read_csv(here(\"data\", \"team_standings.csv\"), \n                  col_types = \"cc\")\n\nNote that the col_types argument accepts a compact representation. Here \"cc\" indicates that the first column is character and the second column is character (there are only two columns). Using the col_types argument is useful because often it is not easy to automatically figure out the type of a column by looking at a few rows (especially if a column has many missing values).\n\n\n\n\n\n\nNote\n\n\n\nThe read_csv() function will also read compressed files automatically.\nThere is no need to decompress the file first or use the gzfile connection function.\n\n\nThe following call reads a gzip-compressed CSV file containing download logs from the RStudio CRAN mirror.\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 n_max = 10)\n\nRows: 10 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): r_version, r_arch, r_os, package, version, country\ndbl  (2): size, ip_id\ndate (1): date\ntime (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that the warnings indicate that read_csv() may have had some difficulty identifying the type of each column. This can be solved by using the col_types argument.\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 col_types = \"ccicccccci\", \n                 n_max = 10)\nlogs\n\n# A tibble: 10 × 10\n   date       time       size r_ver…¹ r_arch r_os  package version country ip_id\n   <chr>      <chr>     <int> <chr>   <chr>  <chr> <chr>   <chr>   <chr>   <int>\n 1 2016-07-19 22:00:00 1.89e6 3.3.0   x86_64 ming… data.t… 1.9.6   US          1\n 2 2016-07-19 22:00:05 4.54e4 3.3.1   x86_64 ming… assert… 0.1     US          2\n 3 2016-07-19 22:00:03 1.43e7 3.3.1   x86_64 ming… stringi 1.1.1   DE          3\n 4 2016-07-19 22:00:05 1.89e6 3.3.1   x86_64 ming… data.t… 1.9.6   US          4\n 5 2016-07-19 22:00:06 3.90e5 3.3.1   x86_64 ming… foreach 1.4.3   US          4\n 6 2016-07-19 22:00:08 4.88e4 3.3.1   x86_64 linu… tree    1.0-37  CO          5\n 7 2016-07-19 22:00:12 5.25e2 3.3.1   x86_64 darw… surviv… 2.39-5  US          6\n 8 2016-07-19 22:00:08 3.23e6 3.3.1   x86_64 ming… Rcpp    0.12.5  US          2\n 9 2016-07-19 22:00:09 5.56e5 3.3.1   x86_64 ming… tibble  1.1     US          2\n10 2016-07-19 22:00:10 1.52e5 3.3.1   x86_64 ming… magrit… 1.5     US          2\n# … with abbreviated variable name ¹​r_version\n\n\nYou can specify the column type in a more detailed fashion by using the various col_*() functions.\nFor example, in the log data above, the first column is actually a date, so it might make more sense to read it in as a Date object.\nIf we wanted to just read in that first column, we could do\n\nlogdates <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                     col_types = cols_only(date = col_date()),\n                     n_max = 10)\nlogdates\n\n# A tibble: 10 × 1\n   date      \n   <date>    \n 1 2016-07-19\n 2 2016-07-19\n 3 2016-07-19\n 4 2016-07-19\n 5 2016-07-19\n 6 2016-07-19\n 7 2016-07-19\n 8 2016-07-19\n 9 2016-07-19\n10 2016-07-19\n\n\nNow the date column is stored as a Date object which can be used for relevant date-related computations (for example, see the lubridate package).\n\n\n\n\n\n\nNote\n\n\n\nThe read_csv() function has a progress option that defaults to TRUE.\nThis options provides a nice progress meter while the CSV file is being read.\nHowever, if you are using read_csv() in a function, or perhaps embedding it in a loop, it is probably best to set progress = FALSE."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome! I am very excited to have you in our one-term (i.e. half a semester) course on Statistical Computing course number (140.776) offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health.\nThis course is designed for ScM and PhD students at Johns Hopkins Bloomberg School of Public Health. I am pretty flexible about permitting outside students, but I want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works.\nThis class is not designed to teach the theoretical aspects of statistical or computational methods, but rather the goal is to help with the practical issues related to setting up a statistical computing environment for data analyses, developing high-quality R packages, conducting reproducible data analyses, best practices for data visualization and writing code, and creating websites for personal or project use."
  },
  {
    "objectID": "posts/welcome/index.html#disability-support-service",
    "href": "posts/welcome/index.html#disability-support-service",
    "title": "Welcome!",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with Student Disability Service (SDS). It is the responsibility of the student to register for accommodations with SDS. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. Once you are f a student in your class has approved accommodations you will receive formal notification and the student will be encouraged to reach out. If you have questions about requesting accommodations, please contact BSPH.dss@jhu.edu."
  },
  {
    "objectID": "posts/welcome/index.html#previous-versions-of-the-class",
    "href": "posts/welcome/index.html#previous-versions-of-the-class",
    "title": "Welcome!",
    "section": "Previous versions of the class",
    "text": "Previous versions of the class\n\nhttps://www.stephaniehicks.com/jhustatcomputing2021\nhttps://rdpeng.github.io/Biostat776"
  },
  {
    "objectID": "posts/welcome/index.html#typos-and-corrections",
    "href": "posts/welcome/index.html#typos-and-corrections",
    "title": "Welcome!",
    "section": "Typos and corrections",
    "text": "Typos and corrections\nFeel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/stephaniehicks/jhustatcomputing2022. You will have the thanks of your grateful instructor!"
  },
  {
    "objectID": "posts/welcome/index.html#rtistry",
    "href": "posts/welcome/index.html#rtistry",
    "title": "Welcome!",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Unboxing’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-gitgithub/index.html",
    "href": "posts/2022-08-30-introduction-to-gitgithub/index.html",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nHappy Git with R from Jenny Bryan\nChapter on git and GitHub in dsbook from Rafael Irizarry\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse"
  },
  {
    "objectID": "posts/2022-08-30-introduction-to-gitgithub/index.html#rtistry",
    "href": "posts/2022-08-30-introduction-to-gitgithub/index.html#rtistry",
    "title": "Introduction to git/GitHub",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Flametree’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html",
    "href": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html",
    "title": "Managing data frames with the Tidyverse",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/tibbles\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-managing-data-frames-with-the-tidyverse\nhttps://jhudatascience.org/tidyversecourse/get-data.html#tibbles"
  },
  {
    "objectID": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html#tibbles",
    "href": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html#tibbles",
    "title": "Managing data frames with the Tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nAnother type of data structure that we need to discuss is called the tibble! It’s best to think of tibbles as an updated and stylish version of the data.frame.\nTibbles are what tidyverse packages work with most seamlessly. Now, that does not mean tidyverse packages require tibbles.\nIn fact, they still work with data.frames, but the more you work with tidyverse and tidyverse-adjacent packages, the more you will see the advantages of using tibbles.\nBefore we go any further, tibbles are data frames, but they have some new bells and whistles to make your life easier.\n\nHow tibbles differ from data.frame\nThere are a number of differences between tibbles and data.frames.\n\n\n\n\n\n\nNote\n\n\n\nTo see a full vignette about tibbles and how they differ from data.frame, you will want to execute vignette(\"tibble\") and read through that vignette.\n\n\nWe will summarize some of the most important points here:\n\nInput type remains unchanged - data.frame is notorious for treating strings as factors; this will not happen with tibbles\nVariable names remain unchanged - In base R, creating data.frames will remove spaces from names, converting them to periods or add “x” before numeric column names. Creating tibbles will not change variable (column) names.\nThere are no row.names() for a tibble - Tidy data requires that variables be stored in a consistent way, removing the need for row names.\nTibbles print first ten rows and columns that fit on one screen - Printing a tibble to screen will never print the entire huge data frame out. By default, it just shows what fits to your screen."
  },
  {
    "objectID": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html#creating-a-tibble",
    "href": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html#creating-a-tibble",
    "title": "Managing data frames with the Tidyverse",
    "section": "Creating a tibble",
    "text": "Creating a tibble\nThe tibble package is part of the tidyverse and can thus be loaded in (once installed) using:\n\nlibrary(tidyverse)\n\n\nas_tibble()\nSince many packages use the historical data.frame from base R, you will often find yourself in the situation that you have a data.frame and want to convert that data.frame to a tibble.\nTo do so, the as_tibble() function is exactly what you are looking for.\nFor the example, here we use a dataset (chicago.rds) containing air pollution and temperature data for the city of Chicago in the U.S.\nThe dataset is available in the /data repository. You can load the data into R using the readRDS() function.\n\nlibrary(here)\n\nhere() starts at /Users/stephaniehicks/Documents/github/teaching/jhustatcomputing2022\n\nchicago <- readRDS(here(\"data\", \"chicago.rds\"))\n\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\n\ndim(chicago)\n\n[1] 6940    8\n\nstr(chicago)\n\n'data.frame':   6940 obs. of  8 variables:\n $ city      : chr  \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num  31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num  31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date, format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num  NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num  34 NA 34.2 47 NA ...\n $ o3tmean2  : num  4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num  20 23.2 23.8 30.4 30.3 ...\n\n\nWe see this data structure is a data.frame with 6940 observations and 8 variables.\nTo convert this data.frame to a tibble you would use the following:\n\nstr(as_tibble(chicago))\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\nTibbles, by default, only print the first ten rows to screen.\nIf you were to print the data.frame chicago to screen, all 6940 rows would be displayed. When working with large data.frames, this default behavior can be incredibly frustrating.\nUsing tibbles removes this frustration because of the default settings for tibble printing.\n\n\nAdditionally, you will note that the type of the variable is printed for each variable in the tibble. This helpful feature is another added bonus of tibbles relative to data.frame.\n\nWant to see more of the tibble?\nIf you do want to see more rows from the tibble, there are a few options!\n\nThe View() function in RStudio is incredibly helpful. The input to this function is the data.frame or tibble you would like to see.\n\nSpecifically, View(chicago) would provide you, the viewer, with a scrollable view (in a new tab) of the complete dataset.\n\nUse the fact that print() enables you to specify how many rows and columns you would like to display.\n\nHere, we again display the chicago data.frame as a tibble but specify that we would only like to see 5 rows. The width = Inf argument specifies that we would like to see all the possible columns. Here, there are only 8, but for larger datasets, this can be helpful to specify.\n\nas_tibble(chicago) %>% \n  print(n = 5, width = Inf)\n\n# A tibble: 6,940 × 8\n  city   tmpd  dptp date       pm25tmean2 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>      <dbl>    <dbl>     <dbl>\n1 chic   31.5  31.5 1987-01-01         NA       34       4.25      20.0\n2 chic   33    29.9 1987-01-02         NA       NA       3.30      23.2\n3 chic   33    27.4 1987-01-03         NA       34.2     3.33      23.8\n4 chic   29    28.6 1987-01-04         NA       47       4.38      30.4\n5 chic   32    28.9 1987-01-05         NA       NA       4.75      30.3\n# … with 6,935 more rows\n\n\n\n\n\ntibble()\nAlternatively, you can create a tibble on the fly by using tibble() and specifying the information you would like stored in each column.\n\n\n\n\n\n\nNote\n\n\n\nIf you provide a single value, this value will be repeated across all rows of the tibble. This is referred to as “recycling inputs of length 1.”\nIn the example here, we see that the column c will contain the value ‘1’ across all rows.\n\ntibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n# A tibble: 5 × 4\n      a     b     c     z\n  <int> <int> <dbl> <dbl>\n1     1     6     1    50\n2     2     7     1    82\n3     3     8     1   122\n4     4     9     1   170\n5     5    10     1   226\n\n\n\n\nThe tibble() function allows you to quickly generate tibbles and even allows you to reference columns within the tibble you are creating, as seen in column z of the example above.\n\n\n\n\n\n\nNote\n\n\n\nTibbles can have column names that are not allowed in data.frame.\nIn the example below, we see that to utilize a nontraditional variable name, you surround the column name with backticks.\nNote that to refer to such columns in other tidyverse packages, you willl continue to use backticks surrounding the variable name.\n\ntibble(\n  `two words` = 1:5,\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\n\n# A tibble: 5 × 3\n  `two words` `12`    `:)` \n        <int> <chr>   <chr>\n1           1 numeric smile\n2           2 numeric smile\n3           3 numeric smile\n4           4 numeric smile\n5           5 numeric smile"
  },
  {
    "objectID": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html#subsetting-tibbles",
    "href": "posts/2022-09-06-managing-data-frames-with-tidyverse/index.html#subsetting-tibbles",
    "title": "Managing data frames with the Tidyverse",
    "section": "Subsetting tibbles",
    "text": "Subsetting tibbles\nSubsetting tibbles also differs slightly from how subsetting occurs with data.frame.\nWhen it comes to tibbles,\n\n[[ can subset by name or position\n$ only subsets by name\n\nFor example:\n\ndf <- tibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n# Extract by name using $ or [[]]\ndf$z\n\n[1]  50  82 122 170 226\n\ndf[[\"z\"]]\n\n[1]  50  82 122 170 226\n\n# Extract by position requires [[]]\ndf[[4]]\n\n[1]  50  82 122 170 226\n\n\nHaving now discussed tibbles, which are the type of object most tidyverse and tidyverse-adjacent packages work best with, we now know the goal.\nIn many cases, tibbles are ultimately what we want to work with in R.\nHowever, data are stored in many different formats outside of R. We will spend the rest of this lesson discussing wrangling functions that work either a data.frame or tibble."
  },
  {
    "objectID": "posts/2022-09-01-literate-programming/index.html",
    "href": "posts/2022-09-01-literate-programming/index.html",
    "title": "Literate Statistical Programming",
    "section": "",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to define literate programming\nRecognize differences between available tools to for literate programming\nKnow how to efficiently work within RStudio for efficient literate programming\nCreate a R Markdown document\n\n\n\n\n\nIntroduction\nOne basic idea to make writing reproducible reports easier is what’s known as literate statistical programming (or sometimes called literate statistical practice). This comes from the idea of literate programming in the area of writing computer programs.\nThe idea is to think of a report or a publication as a stream of text and code.\n\nThe text is readable by people and the code is readable by computers.\nThe analysis is described in a series of text and code chunks.\nEach kind of code chunk will do something like load some data or compute some results.\nEach text chunk will relay something in a human readable language.\n\nThere might also be presentation code that formats tables and figures and there’s article text that explains what’s going on around all this code. This stream of text and code is a literate statistical program or a literate statistical analysis.\n\nWeaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways.\nLiterate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents”, or in other words, machine readable code.\nThe basic idea behind literate programming in order to generate the different kinds of output you might need, you only need a single source document—you can weave and tangle to get the rest.\nIn order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable).\n\n\nSweave\nOne of the original literate programming systems in R that was designed to do this was called Sweave. Sweave enables users to combine R code with a documentation program called LaTeX.\nSweave files ends a .Rnw and have R code weaved through the document:\n<<plot1, height=4, width=5, eval=FALSE>>=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file, Sweave will process the file, executing the R chunks and replacing them with output as appropriate before creating the PDF document.\nIt was originally developed by Fritz Leisch, who is a core member of R, and the code base is still maintained by R Core. The Sweave system comes with any installation of R.\nThere are many limitations to the original Sweave system.\n\nOne of the limitations is that it is focused primarily on LaTeX, which is not a documentation language that many people are familiar with.\nTherefore, it can be difficult to learn this type of markup language if you’re not already in a field that uses it regularly.\nSweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\n\nInstead, folks have moved towards using something called knitr, which offers everything Sweave does, plus it extends it further.\n\nWith Sweave, additional tools are required for advanced operations, whereas knitr supports more internally. We’ll discuss knitr below.\n\n\n\nrmarkdown\nAnother choice for literate programming is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md.. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\n\n\n\n\n\nTip\n\n\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\n\nhttps://bookdown.org/yihui/rmarkdown\n\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\n\nBecause this is lecture is built in a .qmd file (which is very similar to a .Rmd file), let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\n\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\n\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio).\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nmarkdown (R package). markdown is an R package which converts .md files into HTML. It is no longer recommended for use has been surpassed by rmarkdown (discussed below).\nR Markdown (markup language). R Markdown is an extension of the markdown syntax. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the R Markdown Quick Tour for more:\n\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\n\n\n\n\n\nArtwork by Allison Horst on RMarkdown\n\n\n\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr.\n\nThe knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them.\nknitr still uses R as its programming language, but it allows you to mix other programming languages in.\nYou can also use a variety of documentation languages now, such as LaTeX, markdown and HTML.\nknitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\n\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\n\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\n\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\n\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\n\n\n\nConverting a Rmd file to many outputs using knitr and pandoc\n\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]\n\n\n\nCreate and Knit Your First R Markdown Document\n\n\nWhen creating your first R Markdown document, in RStudio you can\n\nGo to File > New File > R Markdown…\nFeel free to edit the Title\nMake sure to select “Default Output Format” to be HTML\nClick “OK”. RStudio creates the R Markdown document and places some boilerplate text in there just so you can see how things are setup.\nClick the “Knit” button (or go to File > Knit Document) to make sure you can create the HTML output\n\nIf you successfully knit your first R Markdown document, then congratulations!\n\n\n\n\n\nMission accomplished!\n\n\n\n\n\n\nWebsites and Books in R Markdown\nNow that you are on the road to using R Markdown documents, it is important to know about other wonderful things you do with these documents. For example, let’s say you have multiple .Rmd documents that you want to put together into a website, blog, book, etc.\nThere are primarily two ways to build multiple .Rmd documents together:\n\nblogdown for building websites\nbookdown for authoring books\n\nIn this section, we briefly introduce both packages, but it’s worth mentioning that the rmarkdown package also has a built-in site generator to build websites.\n\nblogdown\n\n\n\n\n\nblogdown logo\n\n\n\n\n[Source]\nThe blogdown R package is built on top of R Markdown, supports multi-page HTML output to write a blog post or a general page in an Rmd document, or a plain Markdown document.\n\nThese source documents (e.g. .Rmd or .md) are built into a static website (i.e. a bunch of static HTML files, images and CSS files).\nUsing this folder of files, it is very easy to publish it to any web server as a website.\nAlso, it is easy to maintain because it is only a single folder.\n\n\n\n\n\n\n\nTip\n\n\n\nFor example, my personal website was built in blogdown:\n\nhttps://www.stephaniehicks.com\n\nOther really great examples can be found here:\n\nhttps://awesome-blogdown.com\n\n\n\nOther advantages include the content likely being reproducible, easier to maintain, and easy to convert pages to e.g. PDF or other formats in the future if you do not want to convert to HTML files.\nBecause it is based on the Markdown syntax, it is easy to write technical documents, including math equations, insert figures or tables with captions, cross-reference with figure or table numbers, add citations, and present theorems or proofs.\nHere’s a video you can watch of someone making a blogdown website.\n\n\n\n\n[Source on YouTube]\n\n\nbookdown\n\n\n\n\n\nbook logo\n\n\n\n\n[Source]\nSimilar to blogdown, the bookdown R package is built on top of R Markdown, but also offers features like multi-page HTML output, numbering and cross-referencing figures/tables/sections/equations, inserting parts/appendices, and imported the GitBook style (https://www.gitbook.com) to create elegant and appealing HTML book pages. Share\n\n\n\n\n\n\nTip\n\n\n\nFor example, the previous version of this course was built in bookdown:\n\nhttps://rdpeng.github.io/Biostat776\n\nAnother example is the Tidyverse Skills for Data Science book that the JHU Data Science Lab wrote. The github repo that contains all the .Rmd files can be found here.\n\nhttps://jhudatascience.org/tidyversecourse\nhttps://github.com/jhudsl/tidyversecourse\n\n\n\nNote: Even though the word “book” is in “bookdown”, this package is not only for books. It really can be anything that consists of multiple .Rmd documents meant to be read in a linear sequence such as course dissertation/thesis, handouts, study notes, a software manual, a thesis, or even a diary.\n\nhttps://bookdown.org/yihui/rmarkdown/basics-examples.html#examples-books\n\n\n\ndistill\nThere is another great way to build blogs or websites using the distill for R Markdown.\n\nhttps://rstudio.github.io/distill\n\nDistill for R Markdown combines the technical authoring features of the Distill web framework (optimized for scientific and technical communication) with R Markdown, enabling a fully reproducible workflow based on literate programming (Knuth 1984).\nDistill articles include:\n\nReader-friendly typography that adapts well to mobile devices.\nFeatures essential to technical writing like LaTeX math, citations, and footnotes.\nFlexible figure layout options (e.g. displaying figures at a larger width than the article text).\nAttractively rendered tables with optional support for pagination.\nSupport for a wide variety of diagramming tools for illustrating concepts. The ability to incorporate JavaScript and D3-based interactive visualizations.\nA variety of ways to publish articles, including support for publishing sets of articles as a Distill website or as a Distill blog.\n\nThe course website from last year was built in Distill for R Markdown:\n\nWebsite: https://stephaniehicks.com/jhustatcomputing2021\nGithub: https://github.com/stephaniehicks/jhustatcomputing2021\n\nSome other cool things about distill is the use of footnotes and asides.\nFor example 1. The number of the footnote will be automatically generated.\nYou can also optionally include notes in the gutter of the article (immediately to the right of the article text). To do this use the aside tag.\n\nThis content will appear in the gutter of the article.\n\nYou can also include figures in the gutter. Just enclose the code chunk which generates the figure in an aside tag\n\n\n\nTips and tricks in R Markdown in RStudio\nHere are shortcuts and tips on efficiently using RStudio to improve how you write code.\n\nRun code\nIf you want to run a code chunk:\ncommand + Enter on Mac\nCtrl + Enter on Windows\n\n\nInsert a comment in R and R Markdown\nTo insert a comment:\ncommand + Shift + C on Mac\nCtrl + Shift + C on Windows\nThis shortcut can be used both for:\n\nR code when you want to comment your code. It will add a # at the beginning of the line\nfor text in R Markdown. It will add <!-- and --> around the text\n\nNote that if you want to comment more than one line, select all the lines you want to comment then use the shortcut. If you want to uncomment a comment, apply the same shortcut.\n\n\nKnit a R Markdown document\nYou can knit R Markdown documents by using this shortcut:\ncommand + Shift + K on Mac\nCtrl + Shift + K on Windows\n\n\nCode snippets\nCode snippets is usually a few characters long and is used as a shortcut to insert a common piece of code. You simply type a few characters then press Tab and it will complete your code with a larger code. Tab is then used again to navigate through the code where customization is required. For instance, if you type fun then press Tab, it will auto-complete the code with the required code to create a function:\nname <- function(variables) {\n  \n}\nPressing Tab again will jump through the placeholders for you to edit it. So you can first edit the name of the function, then the variables and finally the code inside the function (try by yourself!).\nThere are many code snippets by default in RStudio. Here are the code snippets I use most often:\n\nlib to call library()\n\n\nlibrary(package)\n\n\nmat to create a matrix\n\n\nmatrix(data, nrow = rows, ncol = cols)\n\n\nif, el, and ei to create conditional expressions such as if() {}, else {} and else if () {}\n\n\nif (condition) {\n  \n}\n\nelse {\n  \n}\n\nelse if (condition) {\n  \n}\n\n\nfun to create a function\n\n\nname <- function(variables) {\n  \n}\n\n\nfor to create for loops\n\n\nfor (variable in vector) {\n  \n}\n\n\nts to insert a comment with the current date and time (useful if you have very long code and share it with others so they see when it has been edited)\n\n\n# Tue Jan 21 20:20:14 2020 ------------------------------\n\nYou can see all default code snippets and add yours by clicking on Tools > Global Options… > Code (left sidebar) > Edit Snippets…\n\n\nOrdered list in R Markdown\nIn R Markdown, when creating an ordered list such as this one:\n\nItem 1\nItem 2\nItem 3\n\nInstead of bothering with the numbers and typing\n1. Item 1\n2. Item 2\n3. Item 3\nyou can simply type\n1. Item 1\n1. Item 2\n1. Item 3\nfor the exact same result (try it yourself or check the code of this article!). This way you do not need to bother which number is next when creating a new item.\nTo go even further, any numeric will actually render the same result as long as the first item is the number you want to start from. For example, you could type:\n1. Item 1\n7. Item 2\n3. Item 3\nwhich renders\n\nItem 1\nItem 2\nItem 3\n\nHowever, I suggest always using the number you want to start from for all items because if you move one item at the top, the list will start with this new number. For instance, if we move 7. Item 2 from the previous list at the top, the list becomes:\n7. Item 2\n1. Item 1\n3. Item 3\nwhich incorrectly renders\n\nItem 2\nItem 1\nItem 3\n\n\n\nNew code chunk in R Markdown\nWhen editing R Markdown documents, you will need to insert a new R code chunk many times. The following shortcuts will make your life easier:\ncommand + option + I on Mac (or command + alt + I depending on your keyboard)\nCtrl + ALT + I on Windows\n\n\nReformat code\nA clear and readable code is always easier and faster to read (and look more professional when sharing it to collaborators). To automatically apply the most common coding guidelines such as white spaces, indents, etc., use:\ncmd + Shift + A on Mac\nCtrl + Shift + A on Windows\nSo for example the following code which does not respect the guidelines (and which is not easy to read):\n1+1\n  for(i in 1:10){if(!i%%2){next}\nprint(i)\n }\nbecomes much more neat and readable:\n1 + 1\nfor (i in 1:10) {\n  if (!i %% 2) {\n    next\n  }\n  print(i)\n}\n\n\nRStudio addins\nRStudio addins are extensions which provide a simple mechanism for executing advanced R functions from within RStudio. In simpler words, when executing an addin (by clicking a button in the Addins menu), the corresponding code is executed without you having to write the code. RStudio addins have the advantage that they allow you to execute complex and advanced code much more easily than if you would have to write it yourself.\n\n\n\n\n\n\nTip\n\n\n\nFor more information about RStudio addins, check out:\n\nhttps://rstudio.github.io/rstudioaddins\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown\n\n\n\n\n\nOthers\nSimilar to many other programs, you can also use:\n\ncommand + Shift + N on Mac and Ctrl + Shift + N on Windows to open a new R Script\ncommand + S on Mac and Ctrl + S on Windows to save your current script or R Markdown document\n\nCheck out Tools –> Keyboard Shortcuts Help to see a long list of these shortcuts.\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\nQuestions\n\nWhat is literate programming?\nWhat was the first literate statistical programming tool to weave together a statistical language (R) with a markup language (LaTeX)?\nWhat is knitr and how is different than other literate statistical programming tools?\nWhere can you find a list of other commands that help make your code writing more efficient in RStudio?\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nRMarkdown Tips and Tricks by Indrajeet Patil\nhttps://bookdown.org/yihui/rmarkdown\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\nFootnotes\n\n\nThis will become a hover-able footnote↩︎"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all to clone the repo and navigate within RStudio.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\n\nModule 1\n\nStatistical and computational tools for scientific and reproducible research\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nAug 30\n👋 Course introduction [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n👩‍💻 Introduction to R and RStudio [html] [Qmd]\n\n\n\n\n\n\n🐙 Introduction to git/GitHub [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nSept 1\n🔬 Reproducible Research [html] [Qmd]\n\n\n\n\n\n\n👓 Literate programming [html] [Qmd]\n\n\n\n\n\n\n🆒 Reference management [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nData analysis in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\nSept 6\n👀 Reading and writing data [html] [Qmd]\n🌴 Project 1 [html] [Qmd]\n\n\n\n\n\n✂️ Managing data frames with Tidyverse [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nSept 8\n😻 Tidy data and the Tidyverse [html] [Qmd]\n🍂 Project 0 due\n\n\n\n\n\n🤝 Joining data in R: Basics [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData visualizations R\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\nSept 13\n📊 Plotting systems in R [html] [Qmd]\n\n\n\n\n\n\n📊 The ggplot2 plotting system: qplot() [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nSept 15\n📊 The ggplot2 plotting system: ggplot() [html] [Qmd]\n🌴 Project 2 [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nSept 16\n\n🍂 Project 1 due\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nNuts and bolts of R\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\nSept 20\n🔩 R Nuts and Bolts [html] [Qmd]\n\n\n\n\n\n\n🔩 Control structures in R [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nSept 22\n🔩 Functions in R\n\n\n\n\n\n\n🔩 Loop functions\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nSept 27\n🐛 Debugging code in R\n\n\n\n\n\n\n🐛 Error handling code in R\n\n\n\n\n\n\n\n\n\n\n\n\nSept 29\n🔩 Best practices for project-oriented workflows in R\n\n\n\n\n\nSept 30\n\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nSpecial data types in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\nOct 4\n📆 Working with dates and times\n🌴 Project 3\n\n\n\n\n\n\n\n\n\n\n\nOct 6\n✨ Regular expressions\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\nOct 11\n🐱 Working with factors\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13\n📆 Working with text data and sentiment analysis\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nBest practices for working with data and other languages\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\nOct 18\n☁️ Best practices for storing data\n\n\n\n\n\n\n☁️ Best practices for ethical data analysis\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20\n🐍 Leveraging Python within R\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21\n\n🍂 Project 3 due"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "module 4\n\n\nweek 4\n\n\nR\n\n\nprogramming\n\n\n\n\nIntroduction to control the flow of execution of a series of R expressions\n\n\n\n\n\n\nSep 20, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 4\n\n\nweek 4\n\n\nR\n\n\nprogramming\n\n\n\n\nIntroduction to data types and objects in R\n\n\n\n\n\n\nSep 20, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 3\n\n\nR\n\n\nprogramming\n\n\nggplot2\n\n\ndata viz\n\n\n\n\nAn overview of the ggplot2 plotting system in R with ggplot()\n\n\n\n\n\n\nSep 15, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 3\n\n\nR\n\n\nprogramming\n\n\nggplot2\n\n\ndata viz\n\n\n\n\nAn overview of the ggplot2 plotting system in R with qplot()\n\n\n\n\n\n\nSep 13, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 3\n\n\nR\n\n\nprogramming\n\n\nggplot2\n\n\ndata viz\n\n\n\n\nOverview of three plotting systems in R\n\n\n\n\n\n\nSep 13, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 2\n\n\nR\n\n\nprogramming\n\n\ntidyr\n\n\nhere\n\n\ntidyverse\n\n\n\n\nIntroduction to tidy data and how to convert between wide and long data with the tidyr R package\n\n\n\n\n\n\nSep 8, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 2\n\n\nR\n\n\nprogramming\n\n\ndplyr\n\n\nhere\n\n\ntidyverse\n\n\n\n\nIntroduction to relational data and join functions in the dplyr R package\n\n\n\n\n\n\nSep 8, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 2\n\n\nR\n\n\nprogramming\n\n\nreadr\n\n\nhere\n\n\ntidyverse\n\n\n\n\nHow to get data in and out of R using relative paths\n\n\n\n\n\n\nSep 6, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 2\n\n\nR\n\n\nprogramming\n\n\ndplyr\n\n\nhere\n\n\ntibble\n\n\ntidyverse\n\n\n\n\nAn introduction to data frames in R and the managing them with the dplyr R package\n\n\n\n\n\n\nSep 6, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR\n\n\nreproducibility\n\n\n\n\nIntroduction to reproducible research covering some basic concepts and ideas that are related to reproducible reporting\n\n\n\n\n\n\nSep 1, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR Markdown\n\n\nprogramming\n\n\n\n\nHow to use citations and incorporate references from a bibliography in R Markdown.\n\n\n\n\n\n\nSep 1, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR Markdown\n\n\nprogramming\n\n\n\n\nIntroduction to literate statistical programming tools including R Markdown\n\n\n\n\n\n\nSep 1, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nR\n\n\nprogramming\n\n\nRStudio\n\n\n\n\nLet’s dig into the R programming language and the RStudio integrated developer environment\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncourse-admin\n\n\nmodule 1\n\n\nweek 1\n\n\n\n\nOverview course information for BSPH Biostatistics 140.776 in Fall 2022\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nprogramming\n\n\nversion control\n\n\ngit\n\n\nGitHub\n\n\n\n\nVersion control is a game changer; or how I learned to love git/GitHub\n\n\n\n\n\n\nAug 30, 2022\n\n\nStephanie Hicks\n\n\n\n\n\n\nNo matching items"
  }
]